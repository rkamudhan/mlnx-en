From: Yishai Hadas <yishaih@mellanox.com>
Subject: [PATCH] BACKPORT: 2.6.18 patch

Change-Id: Ica72fddd2832f6e4a0b2a8207e56711c42dc3796
Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/net/ethernet/mellanox/mlx5/core/Makefile   |    4 +-
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      |   38 +++
 drivers/net/ethernet/mellanox/mlx5/core/cq.c       |   10 +-
 drivers/net/ethernet/mellanox/mlx5/core/debugfs.c  |    2 +
 drivers/net/ethernet/mellanox/mlx5/core/en.h       |   53 ++++-
 .../net/ethernet/mellanox/mlx5/core/en_ethtool.c   |   79 ++++++-
 .../ethernet/mellanox/mlx5/core/en_flow_table.c    |   17 ++-
 drivers/net/ethernet/mellanox/mlx5/core/en_main.c  |  258 +++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c    |   42 +++-
 drivers/net/ethernet/mellanox/mlx5/core/en_tx.c    |   72 +++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c  |   30 +++
 drivers/net/ethernet/mellanox/mlx5/core/eq.c       |   15 +-
 drivers/net/ethernet/mellanox/mlx5/core/fs_core.h  |   14 +
 drivers/net/ethernet/mellanox/mlx5/core/health.c   |    2 +
 drivers/net/ethernet/mellanox/mlx5/core/main.c     |  102 ++++++++-
 .../net/ethernet/mellanox/mlx5/core/mlx5_core.h    |    2 +
 .../net/ethernet/mellanox/mlx5/core/pagealloc.c    |    4 +
 drivers/net/ethernet/mellanox/mlx5/core/qp.c       |   16 ++
 drivers/net/ethernet/mellanox/mlx5/core/sriov.c    |    2 +
 drivers/net/ethernet/mellanox/mlx5/core/uar.c      |    4 +
 include/linux/mlx5/driver.h                        |   11 +
 include/rdma/ib_verbs.h                            |   10 +
 22 files changed, 757 insertions(+), 30 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@ -6,6 +6,4 @@ mlx5_core-y :=	main.o cmd.o debugfs.o fw
 		health.o mcg.o cq.o srq.o alloc.o qp.o port.o mr.o pd.o   \
 		mad.o wq.o vport.o transobj.o en_main.o \
 		en_flow_table.o en_ethtool.o en_tx.o en_rx.o en_txrx.o \
-		sriov.o params.o en_debugfs.o en_selftest.o en_sysfs.o en_ecn.o \
-		en_dcb_nl.o fs_cmd.o fs_tree.o fs_debugfs.o en_flow_table.o \
-		en_eswitch.o
+		sriov.o params.o en_selftest.o fs_cmd.o fs_tree.o en_flow_table.o
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -201,7 +201,11 @@ static void poll_timeout(struct mlx5_cmd
 			ent->ret = 0;
 			return;
 		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 36)
 		usleep_range(5000, 10000);
+#else
+		msleep(5);
+#endif
 	} while (time_before(jiffies, poll_end));
 
 	ent->ret = -ETIMEDOUT;
@@ -255,6 +259,7 @@ enum {
 	MLX5_DRIVER_STATUS_ABORTED = 0xfe,
 };
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 static int mlx5_internal_err_ret_value(struct mlx5_core_dev *dev, u16 op, u32 *synd, u8 *status)
 {
 	*synd = 0;
@@ -399,6 +404,7 @@ static int mlx5_internal_err_ret_value(s
 		return -EINVAL;
 	}
 }
+#endif
 
 const char *mlx5_command_str(int command)
 {
@@ -1003,6 +1009,7 @@ out:
 	return err;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 static ssize_t dbg_write(struct file *filp, const char __user *buf,
 			 size_t count, loff_t *pos)
 {
@@ -1033,6 +1040,7 @@ static const struct file_operations fops
 	.open	= simple_open,
 	.write	= dbg_write,
 };
+#endif
 
 static int mlx5_copy_to_msg(struct mlx5_cmd_msg *to, void *from, int size)
 {
@@ -1189,6 +1197,7 @@ static void mlx5_free_cmd_msg(struct mlx
 	kfree(msg);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 static ssize_t data_write(struct file *filp, const char __user *buf,
 			  size_t count, loff_t *pos)
 {
@@ -1320,6 +1329,7 @@ static const struct file_operations olfo
 	.write	= outlen_write,
 	.read	= outlen_read,
 };
+#endif
 
 static void set_wqname(struct mlx5_core_dev *dev)
 {
@@ -1329,6 +1339,7 @@ static void set_wqname(struct mlx5_core_
 		 dev_name(&dev->pdev->dev));
 }
 
+#ifndef HAVE_NO_DEBUGFS
 static void clean_debug_files(struct mlx5_core_dev *dev)
 {
 	struct mlx5_cmd_debug *dbg = &dev->cmd.dbg;
@@ -1384,6 +1395,7 @@ err_dbg:
 	clean_debug_files(dev);
 	return err;
 }
+#endif
 
 void mlx5_cmd_use_events(struct mlx5_core_dev *dev)
 {
@@ -1578,10 +1590,12 @@ static struct mlx5_cmd_msg *alloc_msg(st
 	return msg;
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 static u16 opcode_from_in(struct mlx5_inbox_hdr *in)
 {
 	return be16_to_cpu(in->opcode);
 }
+#endif
 
 static int is_manage_pages(struct mlx5_inbox_hdr *in)
 {
@@ -1599,6 +1613,7 @@ static int cmd_exec(struct mlx5_core_dev
 	u8 status = 0;
 	u32 drv_synd;
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (pci_channel_offline(dev->pdev) ||
 	    dev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR) {
 		err = mlx5_internal_err_ret_value(dev, opcode_from_in(in), &drv_synd, &status);
@@ -1606,6 +1621,7 @@ static int cmd_exec(struct mlx5_core_dev
 		*get_status_ptr(out) = status;
 		return err;
 	}
+#endif
 
 	pages_queue = is_manage_pages(in);
 	gfp = callback ? GFP_ATOMIC : GFP_KERNEL;
@@ -1872,16 +1888,20 @@ int mlx5_cmd_init(struct mlx5_core_dev *
 		goto err_cache;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 	err = create_debugfs_files(dev);
 	if (err) {
 		err = -ENOMEM;
 		goto err_wq;
 	}
+#endif
 
 	return 0;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 err_wq:
 	destroy_workqueue(cmd->wq);
+#endif
 
 err_cache:
 	destroy_msg_cache(dev);
@@ -1900,7 +1920,9 @@ void mlx5_cmd_cleanup(struct mlx5_core_d
 {
 	struct mlx5_cmd *cmd = &dev->cmd;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 	clean_debug_files(dev);
+#endif
 	destroy_workqueue(cmd->wq);
 	destroy_msg_cache(dev);
 	free_cmd_page(dev, cmd);
@@ -2295,11 +2317,19 @@ err_put:
 	device_remove_file(class_dev, &dev_attr_real_miss);
 	for (; i >= 0; i--) {
 		ch = &cache->ch[i];
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 		kobject_put(&ch->kobj);
+#else
+		kobject_unregister(&ch->kobj);
+#endif
 	}
 
 err_rm:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	kobject_put(cache->ko);
+#else
+	kobject_unregister(cache->ko);
+#endif
 	return err;
 }
 
@@ -2313,10 +2343,18 @@ static void cmd_sysfs_cleanup(struct mlx
 	for (i = MLX5_NUM_COMMAND_CACHES - 1; i >= 0; i--) {
 		ch = &dev->cmd.cache.ch[i];
 		if (ch->dev)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 			kobject_put(&ch->kobj);
+#else
+			kobject_unregister(&ch->kobj);
+#endif
 	}
 	if (dev->cmd.cache.ko) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 		kobject_put(dev->cmd.cache.ko);
+#else
+		kobject_unregister(dev->cmd.cache.ko);
+#endif
 		dev->cmd.cache.ko = NULL;
 	}
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/cq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cq.c
@@ -105,10 +105,12 @@ int mlx5_core_create_cq(struct mlx5_core
 		goto err_cmd;
 
 	cq->pid = current->pid;
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_debug_cq_add(dev, cq);
 	if (err)
 		mlx5_core_dbg(dev, "failed adding CP 0x%x to debug file system\n",
 			      cq->cqn);
+#endif
 
 	return 0;
 
@@ -155,7 +157,9 @@ int mlx5_core_destroy_cq(struct mlx5_cor
 		return mlx5_cmd_status_to_err(&out.hdr);
 
 
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_debug_cq_remove(dev, cq);
+#endif
 	return 0;
 }
 EXPORT_SYMBOL(mlx5_core_destroy_cq);
@@ -223,17 +227,21 @@ int mlx5_core_modify_cq_moderation(struc
 int mlx5_init_cq_table(struct mlx5_core_dev *dev)
 {
 	struct mlx5_cq_table *table = &dev->priv.cq_table;
-	int err;
+	int err = 0;
 
 	memset(table, 0, sizeof(*table));
 	spin_lock_init(&table->lock);
 	INIT_RADIX_TREE(&table->tree, GFP_ATOMIC);
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_cq_debugfs_init(dev);
+#endif
 
 	return err;
 }
 
 void mlx5_cleanup_cq_table(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_cq_debugfs_cleanup(dev);
+#endif
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#ifndef HAVE_NO_DEBUGFS
 #include <linux/module.h>
 #include <linux/debugfs.h>
 #include <linux/mlx5/qp.h>
@@ -717,3 +718,4 @@ void mlx5_debug_cq_remove(struct mlx5_co
 	if (cq->dbg)
 		rem_res_tree(cq->dbg);
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -77,10 +77,19 @@
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
 #define MLX5E_PARAMS_DEFAULT_RX_HASH_LOG_TBL_SZ         0x7
 
-#define MLX5E_TX_CQ_POLL_BUDGET        128
+#ifndef HAVE_OLD_NAPI
+#define MLX5E_TX_CQ_POLL_BUDGET                128
+#else
+#define MLX5E_TX_CQ_POLL_BUDGET                512
+#endif
+
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
 
+#ifndef HAVE_SK_BUFF_XMIT_MORE
+#define MLX5E_XMIT_MORE                        0xa
+#endif
+
 #define MLX5E_INDICATE_WQE_ERR	       0xffff
 #define MLX5E_MSG_LEVEL                NETIF_MSG_LINK
 
@@ -423,7 +432,11 @@ struct mlx5e_cq {
 	unsigned long              flags;
 
 	/* data path - accessed per napi poll */
+#ifndef HAVE_OLD_NAPI
 	struct napi_struct        *napi;
+#else
+	struct net_device         *poll_dev; /* for napi */
+#endif
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_channel      *channel;
 
@@ -518,6 +531,9 @@ struct mlx5e_sq {
 	u16                        prev_cc;
 	u8                         bf_budget;
 	struct mlx5e_sq_stats      stats;
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	spinlock_t                 queue_lock;
+#endif
 
 	struct mlx5e_cq            cq;
 
@@ -560,7 +576,11 @@ struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
 	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
+#ifndef HAVE_OLD_NAPI
 	struct napi_struct         napi;
+#else
+	struct net_device         *poll_dev; /* for napi */
+#endif
 	struct device             *pdev;
 	struct net_device         *netdev;
 	__be32                     mkey_be;
@@ -671,6 +691,7 @@ struct mlx5e_flow_tables {
 	struct mlx5e_flow_table		main;
 };
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 struct mlx5e_ecn_rp_attributes {
 	struct mlx5_core_dev	*mdev;
 	/* ATTRIBUTES */
@@ -720,6 +741,7 @@ struct mlx5e_ecn_enable_ctx {
 
 	struct kobj_attribute	enable;
 };
+#endif
 
 #define MLX5E_NIC_DEFAULT_PRIO	0
 
@@ -727,6 +749,11 @@ struct mlx5e_ecn_enable_ctx {
 #define MLX5E_PRIV_FLAG_HWLRO (1<<0)
 #endif
 
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+#define MLX5E_TX_INDIR_SIZE        256
+#define MLX5E_TX_INDIR_MASK        (MLX5E_TX_INDIR_SIZE -1)
+#endif
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	int                        default_vlan_prio;
@@ -735,6 +762,9 @@ struct mlx5e_priv {
 #if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
 	struct vlan_group          *vlan_grp;
 #endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	u8                         rx_csum_offload;
+#endif
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -748,6 +778,9 @@ struct mlx5e_priv {
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	u32                        rqtn;
 	u32                        tirn[MLX5E_NUM_TT];
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	u8                         tx_indir_table[MLX5E_TX_INDIR_SIZE];
+#endif
 
 	struct mlx5e_flow_tables   fts;
 	struct mlx5e_eth_addr_db   eth_addr;
@@ -776,11 +809,16 @@ struct mlx5e_priv {
 	struct dentry *dfs_root;
 	u32 msg_level;
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	struct kobject *ecn_root_kobj;
 
 	struct mlx5e_ecn_ctx ecn_ctx[MLX5E_CONG_PROTOCOL_NUM];
 	struct mlx5e_ecn_enable_ctx ecn_enable_ctx[MLX5E_CONG_PROTOCOL_NUM][8];
+#endif
 	int			   internal_error;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	u32			cached_ptys[MLX5_ST_SZ_DW(ptys_reg)];
+#endif
 };
 
 #define MLX5E_NET_IP_ALIGN 2
@@ -811,7 +849,11 @@ netdev_tx_t mlx5e_xmit(struct sk_buff *s
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
+#ifndef HAVE_OLD_NAPI
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
+#else
+int mlx5e_napi_poll(struct net_device *poll_dev, int *budget);
+#endif
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq);
 struct sk_buff *mlx5e_poll_default_rx_cq(struct mlx5_cqe64 *cqe,
 					 struct mlx5e_rq *rq,
@@ -823,7 +865,12 @@ struct sk_buff *mlx5e_poll_striding_rx_c
 					  u16 *ret_bytes_recv,
 					  struct mlx5e_rx_wqe **ret_wqe,
 					  __be16 *ret_wqe_id_be);
+#ifndef HAVE_OLD_NAPI
 bool mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+#else
+int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int *budget);
+#endif
+
 bool is_poll_striding_wqe(struct mlx5e_rq *rq);
 void free_rq_res(struct mlx5e_rq *rq);
 void free_striding_rq_res(struct mlx5e_rq *rq);
@@ -892,6 +939,7 @@ static inline void mlx5e_tx_notify_hw(st
 	 * doorbell */
 	wmb();
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (bf_sz) {
 		__iowrite64_copy(sq->uar_bf_map + ofst, &wqe->ctrl, bf_sz);
 
@@ -901,6 +949,9 @@ static inline void mlx5e_tx_notify_hw(st
 	} else {
 		mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
 	}
+#else
+	mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
+#endif
 
 	sq->bf_offset ^= sq->bf_buf_size;
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#include <linux/ethtool.h>
 #include "en.h"
 
 static const char mlx5e_test_names[][ETH_GSTRING_LEN] = {
@@ -192,10 +193,15 @@ static const struct {
 	},
 };
 
+#ifdef HAVE_SSET_COUNT
 static int mlx5e_get_sset_count(struct net_device *dev, int sset)
+#else
+static int mlx5e_get_stats_count(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
+#ifdef HAVE_SSET_COUNT
 	switch (sset) {
 	case ETH_SS_STATS:
 		return NUM_VPORT_COUNTERS + NUM_PPORT_COUNTERS +
@@ -212,7 +218,20 @@ static int mlx5e_get_sset_count(struct n
 	default:
 		return -EOPNOTSUPP;
 	}
+#else
+	return NUM_VPORT_COUNTERS +
+	       priv->params.num_channels * NUM_RQ_STATS +
+	       priv->params.num_channels * priv->params.num_tc *
+					   NUM_SQ_STATS;
+#endif
+}
+
+#ifndef HAVE_SSET_COUNT
+int mlx5e_self_test_count(struct net_device *dev)
+{
+	return MLX5E_NUM_SELF_TEST;
 }
+#endif
 
 static void mlx5e_get_strings(struct net_device *dev,
 			      uint32_t stringset, uint8_t *data)
@@ -238,12 +257,12 @@ static void mlx5e_get_strings(struct net
 		for (i = 0; i < NUM_VPORT_COUNTERS; i++)
 			strcpy(data + (idx++) * ETH_GSTRING_LEN,
 			       vport_strings[i]);
-
+#ifdef HAVE_SSET_COUNT
 		/* PPORT counters */
 		for (i = 0; i < NUM_PPORT_COUNTERS; i++)
 			strcpy(data + (idx++) * ETH_GSTRING_LEN,
 			       pport_strings[i]);
-
+#endif
 		sprintf(data + (idx++) * ETH_GSTRING_LEN,
 			"q_counter_%s", qcounter_stats_strings[0]);
 
@@ -280,10 +299,10 @@ static void mlx5e_get_ethtool_stats(stru
 
 	for (i = 0; i < NUM_VPORT_COUNTERS; i++)
 		data[idx++] = ((u64 *)&priv->stats.vport)[i];
-
+#ifdef HAVE_SSET_COUNT
 	for (i = 0; i < NUM_PPORT_COUNTERS; i++)
 		data[idx++] = be64_to_cpu(((__be64 *)&priv->stats.pport)[i]);
-
+#endif
 	data[idx++] = (priv->counter_set_id != -1) ? priv->stats.out_of_buffer
 						     : 0;
 
@@ -599,25 +618,41 @@ static u8 get_connector_port(u32 eth_pro
 	return PORT_OTHER;
 }
 
+#ifdef HAVE_LP_ADV
 static void get_lp_advertising(u32 eth_proto_lp, u32 *lp_advertising)
 {
 
 	*lp_advertising = ptys2ethtool_adver_link(eth_proto_lp);
 }
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+static void mlx5e_get_cached_port_ptys(struct mlx5e_priv *priv,
+				       u32 *ptys, int ptys_size)
+{
+	memcpy(ptys, priv->cached_ptys, ptys_size);
+}
+#endif
 
 static int mlx5e_get_settings(struct net_device *netdev,
 			      struct ethtool_cmd *cmd)
 {
 	struct mlx5e_priv *priv    = netdev_priv(netdev);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	struct mlx5_core_dev *mdev = priv->mdev;
+#endif
 	u32 out[MLX5_ST_SZ_DW(ptys_reg)];
 	u32 eth_proto_cap;
 	u32 eth_proto_admin;
 	u32 eth_proto_lp;
 	u32 eth_proto_oper;
-	int err;
+	int err = 0;
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	mlx5e_get_cached_port_ptys(priv, out, sizeof(out));
+#else
 	err = mlx5_query_port_ptys(mdev, out, sizeof(out), MLX5_PTYS_EN);
+#endif
 
 	if (err) {
 		netdev_err(netdev, "%s: query port ptys failed: %d\n",
@@ -640,7 +675,9 @@ static int mlx5e_get_settings(struct net
 	eth_proto_oper = eth_proto_oper ? eth_proto_oper : eth_proto_cap;
 
 	cmd->port = get_connector_port(eth_proto_oper);
+#ifdef HAVE_LP_ADV
 	get_lp_advertising(eth_proto_lp, &cmd->lp_advertising);
+#endif
 
 	cmd->transceiver = XCVR_INTERNAL;
 
@@ -981,15 +1018,39 @@ static int mlx5e_set_tso(struct net_devi
        return 0;
 }
 #endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+static u32 mlx5e_get_rx_csum(struct net_device *dev)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	return priv->rx_csum_offload;
+}
+
+static int mlx5e_set_rx_csum(struct net_device *dev, u32 data)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	priv->rx_csum_offload = data;
+	return 0;
+}
+#endif
 #endif
 
 const struct ethtool_ops mlx5e_ethtool_ops = {
 	.get_drvinfo       = mlx5e_get_drvinfo,
 	.get_link          = ethtool_op_get_link,
 	.get_strings       = mlx5e_get_strings,
+#ifdef HAVE_SSET_COUNT
 	.get_sset_count    = mlx5e_get_sset_count,
+#else
+	.get_stats_count   = mlx5e_get_stats_count,
+#endif
 	.get_ethtool_stats = mlx5e_get_ethtool_stats,
 	.self_test         = mlx5e_self_test,
+#ifndef HAVE_SSET_COUNT
+	.self_test_count   = mlx5e_self_test_count,
+#endif
 	.get_msglevel      = mlx5e_get_msglevel,
 	.set_msglevel      = mlx5e_set_msglevel,
 	.get_ringparam     = mlx5e_get_ringparam,
@@ -1019,6 +1080,14 @@ const struct ethtool_ops mlx5e_ethtool_o
 	.get_tso	   = mlx5e_get_tso,
 	.set_tso	   = mlx5e_set_tso,
 #endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	.get_rx_csum       = mlx5e_get_rx_csum,
+	.set_rx_csum       = mlx5e_set_rx_csum,
+	.get_tx_csum       = ethtool_op_get_tx_csum,
+	.set_tx_csum       = ethtool_op_set_tx_csum,
+	.get_sg            = ethtool_op_get_sg,
+	.set_sg            = ethtool_op_set_sg,
+#endif
 #endif
 };
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_flow_table.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_flow_table.c
@@ -733,16 +733,23 @@ static void mlx5e_execute_action(struct
 static void mlx5e_sync_netdev_addr(struct mlx5e_priv *priv)
 {
 	struct net_device *netdev = priv->netdev;
+#ifdef TBD
 	struct netdev_hw_addr *ha;
 #ifndef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	struct dev_mc_list *mclist;
 #endif
+#endif
 
+#ifdef HAVE_ADDR_LOCK
 	netif_addr_lock_bh(netdev);
+#else
+	netif_tx_lock_bh(netdev);
+#endif
 
 	mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_uc,
 				   priv->netdev->dev_addr);
 
+#ifdef TBD
 	netdev_for_each_uc_addr(ha, netdev)
 		mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_uc, ha->addr);
 
@@ -754,10 +761,16 @@ static void mlx5e_sync_netdev_addr(struc
 		mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_mc,
 					   mclist->dmi_addr);
 #endif
+#endif
 
+#ifdef HAVE_ADDR_LOCK
 	netif_addr_unlock_bh(netdev);
+#else
+	netif_tx_unlock_bh(netdev);
+#endif
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 static void mlx5e_vport_context_update_addr_list(struct mlx5e_priv *priv,
 						 int list_type)
 {
@@ -834,6 +847,7 @@ static void mlx5e_vport_context_update(s
 				      ea->allmulti_enabled,
 				      ea->promisc_enabled);
 }
+#endif
 
 static void mlx5e_apply_netdev_addr(struct mlx5e_priv *priv)
 {
@@ -907,8 +921,9 @@ void mlx5e_set_rx_mode_core(struct mlx5e
 	ea->promisc_enabled   = promisc_enabled;
 	ea->allmulti_enabled  = allmulti_enabled;
 	ea->broadcast_enabled = broadcast_enabled;
-
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	mlx5e_vport_context_update(priv);
+#endif
 }
 
 void mlx5e_set_rx_mode_work(struct work_struct *work)
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -32,6 +32,10 @@
 
 #include "en.h"
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+extern int num_channels;
+#endif
+
 struct mlx5e_rq_param {
 	u32                        rqc[MLX5_ST_SZ_DW(rqc)];
 	struct mlx5_wq_param       wq;
@@ -285,6 +289,21 @@ free_out:
 	kvfree(out);
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+static void mlx5e_cache_port_ptys(struct mlx5e_priv *priv)
+{
+	struct mlx5_core_dev *mdev = priv->mdev;
+	int err;
+
+	err = mlx5_query_port_ptys(mdev, priv->cached_ptys,
+				   sizeof(priv->cached_ptys),
+				   MLX5_PTYS_EN);
+	if (err)
+		netdev_err(netdev, "%s: query port ptys failed: %d\n",
+			   __func__, err);
+}
+#endif
+
 static void mlx5e_update_stats_work(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -293,6 +312,9 @@ static void mlx5e_update_stats_work(stru
 	mutex_lock(&priv->state_lock);
 	if (test_bit(MLX5E_STATE_OPENED, &priv->state) && !priv->internal_error) {
 		mlx5e_update_stats(priv);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+		mlx5e_cache_port_ptys(priv);
+#endif
 		schedule_delayed_work(dwork,
 				      msecs_to_jiffies(
 					      MLX5E_UPDATE_STATS_INTERVAL));
@@ -316,8 +338,7 @@ static void __mlx5e_async_event(struct m
 		break;
 	case MLX5_DEV_EVENT_SYS_ERROR:
 		priv->internal_error = 1;
-		cancel_work_sync(&priv->set_rx_mode_work);
-		cancel_work_sync(&priv->update_carrier_work);
+		msleep(1000);
 		/* this is used to serialize the marking of internal error
 		 * state and the restart of update stats work
 		 */
@@ -671,7 +692,12 @@ static void mlx5e_close_rq(struct mlx5e_
 	int i;
 
 	clear_bit(MLX5E_RQ_STATE_POST_WQES_ENABLE, &rq->state);
+#ifndef HAVE_OLD_NAPI
 	napi_synchronize(&rq->channel->napi); /* prevent mlx5e_post_rx_wqes */
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &rq->channel->poll_dev->state))
+		msleep(1);
+#endif
 
 	mlx5e_modify_rq(rq, MLX5_RQC_STATE_RDY, MLX5_RQC_STATE_ERR);
 	if (!priv->internal_error) {
@@ -683,7 +709,12 @@ static void mlx5e_close_rq(struct mlx5e_
 	}
 
 	/* avoid destroying rq before mlx5e_poll_rx_cq() is done with it */
+#ifndef HAVE_OLD_NAPI
 	napi_synchronize(&rq->channel->napi);
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &rq->channel->poll_dev->state))
+		msleep(1);
+#endif
 
 	mlx5e_disable_rq(rq);
 	mlx5e_destroy_rq(rq);
@@ -742,16 +773,24 @@ static int mlx5e_create_sq(struct mlx5e_
 	sq->uar_map     = sq->uar.map;
 	sq->uar_bf_map  = sq->uar.bf_map;
 	sq->bf_buf_size = (1 << MLX5_CAP_GEN(mdev, log_bf_reg_size)) / 2;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	sq->max_inline  = sq->bf_buf_size -
 			  sizeof(struct mlx5e_tx_wqe) +
 			  2 /*sizeof(mlx5e_tx_wqe.inline_hdr_start)*/;
-
+#else
+	sq->max_inline = 0;
+#endif
 	err = mlx5e_alloc_sq_db(sq, cpu_to_node(c->cpu));
 	if (err)
 		goto err_sq_wq_destroy;
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	spin_lock_init(&sq->queue_lock);
+#endif
 
 	txq_ix = c->ix + tc * priv->params.num_channels;
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
 	sq->txq = netdev_get_tx_queue(priv->netdev, txq_ix);
+#endif
 	priv->txq_to_sq_map[txq_ix] = sq;
 
 	sq->pdev      = c->pdev;
@@ -885,8 +924,10 @@ static int mlx5e_open_sq(struct mlx5e_ch
 		goto err_disable_sq;
 
 	set_bit(MLX5E_SQ_STATE_WAKE_TXQ_ENABLE, &sq->state);
+#ifdef TBD
 	netdev_tx_reset_queue(sq->txq);
 	netif_tx_start_queue(sq->txq);
+#endif
 
 	return 0;
 
@@ -899,20 +940,29 @@ err_destroy_sq:
 }
 
 /* TODO: make this function general, i.e move to netdevice.h */
+#ifdef TBD
 static inline void netif_tx_disable_queue(struct netdev_queue *txq)
 {
 	__netif_tx_lock_bh(txq);
 	netif_tx_stop_queue(txq);
 	__netif_tx_unlock_bh(txq);
 }
+#endif
 
 static void mlx5e_close_sq(struct mlx5e_priv *priv, struct mlx5e_sq *sq)
 {
 	int i;
 
 	clear_bit(MLX5E_SQ_STATE_WAKE_TXQ_ENABLE, &sq->state);
+#ifndef HAVE_OLD_NAPI
 	napi_synchronize(&sq->channel->napi); /* prevent netif_tx_wake_queue */
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &sq->channel->poll_dev->state))
+		msleep(1);
+#endif
+#ifdef TBD
 	netif_tx_disable_queue(sq->txq);
+#endif
 
 	/* ensure hw is notified of all pending wqes */
 	if (mlx5e_sq_has_room_for(sq, 1))
@@ -928,7 +978,12 @@ static void mlx5e_close_sq(struct mlx5e_
 	}
 
 	/* avoid destroying sq before mlx5e_poll_tx_cq() is done with it */
+#ifndef HAVE_OLD_NAPI
 	napi_synchronize(&sq->channel->napi);
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &sq->channel->poll_dev->state))
+		msleep(1);
+#endif
 
 	mlx5e_disable_sq(sq);
 	mlx5e_destroy_sq(sq);
@@ -957,7 +1012,11 @@ static int mlx5e_create_cq(struct mlx5e_
 
 	mlx5_vector2eqn(mdev, param->eq_ix, &eqn_not_used, &irqn);
 
+#ifndef HAVE_OLD_NAPI
 	cq->napi        = &c->napi;
+#else
+	cq->poll_dev	= c->poll_dev;
+#endif
 
 	mcq->cqe_sz     = 64;
 	mcq->set_ci_db  = cq->wq_ctrl.db.db;
@@ -1087,6 +1146,7 @@ static void mlx5e_close_cq(struct mlx5e_
 	mlx5e_destroy_cq(cq);
 }
 
+#ifndef HAVE_OLD_NUMA
 static int mlx5e_get_cpu(struct mlx5e_priv *priv, int ix)
 {
 #ifdef CONFIG_CPUMASK_OFFSTACK
@@ -1097,6 +1157,7 @@ static int mlx5e_get_cpu(struct mlx5e_pr
 	return 0;                                     
 #endif
 }
+#endif
 
 static void mlx5e_build_tc_to_txq_map(struct mlx5e_priv *priv, int ix)
 {
@@ -1172,8 +1233,16 @@ static int mlx5e_open_channel(struct mlx
 			      struct mlx5e_channel_param *cparam,
 			      struct mlx5e_channel **cp)
 {
+#ifndef HAVE_OLD_NAPI
 	struct net_device *netdev = priv->netdev;
+#else
+	char name[IFNAMSIZ];
+#endif
+#ifndef HAVE_OLD_NUMA
 	int cpu = mlx5e_get_cpu(priv, ix);
+#else
+	int cpu = numa_node_id();
+#endif
 	struct mlx5e_channel *c;
 	int err;
 
@@ -1191,6 +1260,7 @@ static int mlx5e_open_channel(struct mlx
 
 	mlx5e_build_tc_to_txq_map(priv, ix);
 
+#ifndef HAVE_OLD_NAPI
 	netif_napi_add(netdev, &c->napi, mlx5e_napi_poll, 64);
 
 	err = mlx5e_open_tx_cqs(c, cparam);
@@ -1205,6 +1275,34 @@ static int mlx5e_open_channel(struct mlx
 		goto err_close_tx_cqs;
 
 	napi_enable(&c->napi);
+#else
+
+	snprintf(name, IFNAMSIZ, "mlx5e-%u", c->rq.rqn);
+	c->poll_dev = alloc_netdev(0, name, ether_setup);
+	if (!c->poll_dev)
+		return -ENOMEM;
+
+	c->poll_dev->priv = c;
+	c->poll_dev->weight = 64; /* TBD check this value */
+	c->poll_dev->poll = &mlx5e_napi_poll;
+	set_bit(__LINK_STATE_START, &c->poll_dev->state);
+
+	err = mlx5e_open_tx_cqs(c, cparam);
+	if (err) {
+		printk("%s[%d]: mlx5e_open_tx_cqs failed, %d\n",
+			   __func__, ix, err);
+		return err;
+	}
+	err = mlx5e_open_cq(c, &cparam->rx_cq, &c->rq.cq,
+			    priv->params.rx_cq_moderation_usec,
+			    priv->params.rx_cq_moderation_pkts,
+			    MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
+	if (err) {
+		printk("%s[%d]: mlx5e_open_cq rx failed, %d\n",
+			   __func__, ix, err);
+		goto err_close_tx_cqs;
+	}
+#endif
 
 	err = mlx5e_open_sqs(c, cparam);
 	if (err)
@@ -1214,6 +1312,7 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_sqs;
 
+#ifdef HAVE_XPS
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)) || \
      defined(CONFIG_COMPAT_IS_NETIF_SET_XPS_QUEUE_NOT_CONST_CPUMASK)
 	netif_set_xps_queue(netdev, (struct cpumask *)get_cpu_mask(c->cpu), ix);
@@ -1223,6 +1322,7 @@ static int mlx5e_open_channel(struct mlx
 #if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	c->irq_desc = irq_to_desc(priv->mdev->priv.msix_arr[c->rq.cq.mcq.irqn].vector);
 #endif
+#endif
 	*cp = c;
 
 	return 0;
@@ -1231,14 +1331,23 @@ err_close_sqs:
 	mlx5e_close_sqs(c);
 
 err_disable_napi:
+#ifndef HAVE_OLD_NAPI
 	napi_disable(&c->napi);
+#endif
 	mlx5e_close_cq(&c->rq.cq);
 
 err_close_tx_cqs:
 	mlx5e_close_tx_cqs(c);
 
+#ifndef HAVE_OLD_NAPI
 err_napi_del:
 	netif_napi_del(&c->napi);
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &c->poll_dev->state))
+		msleep(1);
+	free_netdev(c->poll_dev);
+	c->poll_dev = NULL;
+#endif
 	kfree(c);
 
 	return err;
@@ -1248,13 +1357,26 @@ static void mlx5e_close_channel(struct m
 {
 	mlx5e_close_rq(&c->rq);
 	mlx5e_close_sqs(c);
+#ifndef HAVE_OLD_NAPI
 	napi_disable(&c->napi);
 	mlx5e_close_cq(&c->rq.cq);
 	mlx5e_close_tx_cqs(c);
 	netif_napi_del(&c->napi);
+#else
+	while (test_bit(__LINK_STATE_RX_SCHED, &c->poll_dev->state))
+		msleep(1);
+	free_netdev(c->poll_dev);
+	c->poll_dev = NULL;
+	mlx5e_close_cq(&c->rq.cq);
+	mlx5e_close_tx_cqs(c);
+#endif
 	kfree(c);
 }
 
+#ifdef HAVE_OLD_NUMA
+#define dev_to_node(pdev) numa_node_id()
+#endif
+
 static void mlx5e_build_rq_param(struct mlx5e_priv *priv,
 				 struct mlx5e_rq_param *param)
 {
@@ -1480,6 +1602,7 @@ static void mlx5e_close_tises(struct mlx
 		mlx5e_close_tis(priv, tc);
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 static int mlx5e_bits_invert(unsigned long a, int size)
 {
 	int i;
@@ -1490,6 +1613,7 @@ static int mlx5e_bits_invert(unsigned lo
 
 	return inv;
 }
+#endif
 
 static int mlx5e_open_rqt(struct mlx5e_priv *priv)
 {
@@ -1515,10 +1639,17 @@ static int mlx5e_open_rqt(struct mlx5e_p
 	for (i = 0; i < sz; i++) {
 		int ix = i;
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 		if (priv->params.rss_hash_xor)
 			ix = mlx5e_bits_invert(i, log_tbl_sz);
+#endif
 
+#ifndef HAVE_OLD_NAPI
 		ix = ix % priv->params.num_channels;
+#else
+		ix = (priv->params.num_channels == 1) ? 0 :
+			(ix % (priv->params.num_channels / 2));
+#endif
 		MLX5_SET(rqtc, rqtc, rq_num[i], priv->channel[ix]->rq.rqn);
 	}
 
@@ -1536,6 +1667,9 @@ static void mlx5e_close_rqt(struct mlx5e
 static void mlx5e_build_tir_ctx(struct mlx5e_priv *priv, u32 *tirc, int tt)
 {
 	void *hfso = MLX5_ADDR_OF(tirc, tirc, rx_hash_field_selector_outer);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	__be32 *hkey;
+#endif
 
 	MLX5_SET(tirc, tirc, transport_domain, priv->tdn);
 
@@ -1582,6 +1716,22 @@ static void mlx5e_build_tir_ctx(struct m
 			 MLX5_TIRC_DISP_TYPE_INDIRECT);
 		MLX5_SET(tirc, tirc, indirect_table,
 			 priv->rqtn);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+		MLX5_SET(tirc, tirc, rx_hash_fn,
+			 MLX5_TIRC_RX_HASH_FN_HASH_TOEPLITZ);
+		MLX5_SET(tirc, tirc, rx_hash_symmetric, 1);
+		hkey = (__be32 *)MLX5_ADDR_OF(tirc, tirc, rx_hash_toeplitz_key);
+		hkey[0] = cpu_to_be32(0xD181C62C);
+		hkey[1] = cpu_to_be32(0xF7F4DB5B);
+		hkey[2] = cpu_to_be32(0x1983A2FC);
+		hkey[3] = cpu_to_be32(0x943E1ADB);
+		hkey[4] = cpu_to_be32(0xD9389E6B);
+		hkey[5] = cpu_to_be32(0xD1039C2C);
+		hkey[6] = cpu_to_be32(0xA74499AD);
+		hkey[7] = cpu_to_be32(0x593D56D9);
+		hkey[8] = cpu_to_be32(0xF3253C06);
+		hkey[9] = cpu_to_be32(0x2ADC1FFC);
+#else
 		if (priv->params.rss_hash_xor) {
 			MLX5_SET(tirc, tirc, rx_hash_fn,
 				 MLX5_TIRC_RX_HASH_FN_HASH_INVERTED_XOR8);
@@ -1597,6 +1747,7 @@ static void mlx5e_build_tir_ctx(struct m
 
 			netdev_rss_key_fill(rss_key, len);
 		}
+#endif
 		break;
 	}
 
@@ -1786,15 +1937,19 @@ static int mlx5e_set_dev_port_mtu(struct
 int mlx5e_open_locked(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
 	int num_txqs;
+#endif
 	int err;
 
 	mlx5e_netdev_set_tcs(netdev);
-
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
 	num_txqs = priv->params.num_channels * priv->params.num_tc;
+#ifdef HAVE_NETIF_SET_REAL_NUM_TX_QUEUES
 	netif_set_real_num_tx_queues(netdev, num_txqs);
 	netif_set_real_num_rx_queues(netdev, priv->params.num_channels);
-
+#endif
+#endif
 	err = mlx5e_set_dev_port_mtu(netdev);
 	if (err)
 		return err;
@@ -1846,20 +2001,27 @@ int mlx5e_open_locked(struct net_device
 
 	set_bit(MLX5E_STATE_OPENED, &priv->state);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	mlx5e_create_debugfs(priv);
+#endif
 	mlx5e_update_carrier(priv);
 	mlx5e_set_rx_mode_core(priv);
 
 	schedule_delayed_work(&priv->update_stats_work, 0);
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	err = mlx5e_sysfs_create(netdev);
 	if (err)
 		goto err_rollback;
+#endif
 
 	return 0;
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 err_rollback:
 	mlx5e_destroy_debugfs(priv);
 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
+#endif
 
 err_close_flow_table:
 	mlx5e_close_flow_table(priv);
@@ -1898,6 +2060,9 @@ static int mlx5e_open(struct net_device
 int mlx5e_close_locked(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	int tc, i;
+#endif
 
 	/* May already be CLOSED in case a previous configuration operation
 	 * (e.g RX/TX queue size change) that involves close&open failed at the
@@ -1908,13 +2073,26 @@ int mlx5e_close_locked(struct net_device
 		netdev_err(netdev, "Device is already closed\n");
 		return 0;
 	}
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	mlx5e_sysfs_remove(netdev);
+#endif
 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
 
 	mlx5e_set_rx_mode_core(priv);
 	mlx5e_del_all_vlan_rules(priv);
 	netif_carrier_off(priv->netdev);
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	for (i = 0; i < priv->params.num_channels; i++) {
+		for (tc = 0; tc < priv->params.num_tc; tc++) {
+			spin_lock(&priv->channel[i]->sq[tc].queue_lock);
+			spin_unlock(&priv->channel[i]->sq[tc].queue_lock);
+		}
+	}
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	mlx5e_destroy_debugfs(priv);
+#endif
 	mlx5e_close_flow_table(priv);
 	mlx5e_close_tirs(priv);
 	mlx5e_close_rqt(priv);
@@ -2036,6 +2214,7 @@ static struct net_device_stats *mlx5e_ge
 	return stats;
 }
 
+#ifdef HAVE_NETDEV_OPS
 static void mlx5e_set_rx_mode(struct net_device *dev)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2043,6 +2222,7 @@ static void mlx5e_set_rx_mode(struct net
 	if (!priv->internal_error)
 		schedule_work(&priv->set_rx_mode_work);
 }
+#endif
 
 static int mlx5e_set_mac(struct net_device *netdev, void *addr)
 {
@@ -2052,9 +2232,17 @@ static int mlx5e_set_mac(struct net_devi
 	if (!is_valid_ether_addr(saddr->sa_data))
 		return -EADDRNOTAVAIL;
 
+#ifdef HAVE_ADDR_LOCK
 	netif_addr_lock_bh(netdev);
+#else
+	netif_tx_lock_bh(netdev);
+#endif
 	ether_addr_copy(netdev->dev_addr, saddr->sa_data);
+#ifdef HAVE_ADDR_LOCK
 	netif_addr_unlock_bh(netdev);
+#else
+	netif_tx_unlock_bh(netdev);
+#endif
 
 	if (!priv->internal_error)
 		schedule_work(&priv->set_rx_mode_work);
@@ -2062,6 +2250,7 @@ static int mlx5e_set_mac(struct net_devi
 	return 0;
 }
 
+#ifdef HAVE_FEATURES
 #if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_set_features(struct net_device *netdev,
 #ifdef HAVE_NET_DEVICE_OPS_EXT
@@ -2102,6 +2291,8 @@ out:
 	return err;
 }
 #endif
+#endif
+
 
 static int mlx5e_change_mtu(struct net_device *netdev, int new_mtu)
 {
@@ -2214,6 +2405,7 @@ static int mlx5e_get_vf_config(struct ne
 }
 #endif
 
+#ifdef HAVE_NETDEV_OPS
 static struct net_device_ops mlx5e_netdev_ops = {
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
@@ -2250,6 +2442,7 @@ static struct net_device_ops mlx5e_netde
 	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
 #endif
 };
+#endif
 
 #ifdef HAVE_NET_DEVICE_OPS_EXT
 static const struct net_device_ops_ext mlx5_netdev_ops_ext = {
@@ -2267,6 +2460,7 @@ static const struct net_device_ops_ext m
 
 static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
 {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (MLX5_CAP_GEN(mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
 		return -ENOTSUPP;
 	/* TODO: cehck if more caps are needed */
@@ -2284,6 +2478,7 @@ static int mlx5e_check_required_hca_cap(
 			       "Not creating net device, some required device capabilities are missing\n");
 		return -ENOTSUPP;
 	}
+#endif
 	return 0;
 }
 
@@ -2292,6 +2487,9 @@ static void mlx5e_build_netdev_priv(stru
 				    int num_comp_vectors)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	int i;
+#endif
 
 	/* TODO: consider link speed for setting the following:
 	 *       log_sq_size
@@ -2322,8 +2520,14 @@ static void mlx5e_build_netdev_priv(stru
 		MLX5E_PARAMS_DEFAULT_RX_HASH_LOG_TBL_SZ;
 	priv->params.num_tc                = 1;
 	priv->params.default_vlan_prio     = 0;
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	for (i = 0; i < MLX5E_TX_INDIR_SIZE; i++)
+		priv->tx_indir_table[i] = i % num_comp_vectors;
+#endif
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	priv->params.rss_hash_xor = true;
+#endif
 
 	if (MLX5_CAP_GEN(mdev, striding_rq)) {
 		/* TODO ethtoo for these params */
@@ -2366,7 +2570,7 @@ static void mlx5e_build_netdev(struct ne
 	struct mlx5_core_dev *mdev = priv->mdev;
 
 	SET_NETDEV_DEV(netdev, &mdev->pdev->dev);
-
+#ifdef HAVE_NETDEV_OPS
 	netdev->netdev_ops        = &mlx5e_netdev_ops;
 	netdev->watchdog_timeo    = 15 * HZ;
 #ifdef HAVE_ETHTOOL_OPS_EXT
@@ -2379,6 +2583,22 @@ static void mlx5e_build_netdev(struct ne
 #ifdef HAVE_IEEE_DCBNL_ETS
 	netdev->dcbnl_ops	  = &mlx5e_dcbnl_ops;
 #endif
+#else /*HAVE_NETDEV_OPS*/
+	netdev->open		= mlx5e_open;
+	netdev->stop		= mlx5e_close;
+	netdev->hard_start_xmit	= mlx5e_xmit;
+	netdev->get_stats	= mlx5e_get_stats;
+	netdev->set_mac_address	= mlx5e_set_mac;
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	netdev->vlan_rx_register = mlx5e_vlan_register;
+#endif
+	netdev->vlan_rx_add_vid	= mlx5e_vlan_rx_add_vid;
+	netdev->vlan_rx_kill_vid= mlx5e_vlan_rx_kill_vid;
+	netdev->change_mtu	= mlx5e_change_mtu;
+	SET_ETHTOOL_OPS(netdev, &mlx5e_ethtool_ops);
+#endif
+
+#ifdef HAVE_VLAN_HW_FEATURES
 	netdev->vlan_features    |= NETIF_F_SG;
 	netdev->vlan_features    |= NETIF_F_IP_CSUM;
 	netdev->vlan_features    |= NETIF_F_IPV6_CSUM;
@@ -2416,6 +2636,17 @@ static void mlx5e_build_netdev(struct ne
 #ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	netdev->priv_flags       |= IFF_UNICAST_FLT;
 #endif
+#else
+	netdev->features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			    NETIF_F_TSO | NETIF_F_TSO6 |
+			    NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_FILTER;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	priv->rx_csum_offload = 1;
+#endif
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	netdev->features         |= NETIF_F_LLTX;
+#endif
+#endif
 
 #ifdef HAVE_NET_DEVICE_OPS_EXT
 	set_netdev_ops_ext(netdev, &mlx5_netdev_ops_ext);
@@ -2453,12 +2684,21 @@ static void *mlx5e_create_netdev(struct
 {
 	struct net_device *netdev;
 	struct mlx5e_priv *priv;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	int ncv = (num_channels > 0) ?
+		  min(min(mdev->priv.eq_table.num_comp_vectors, num_channels),
+		      MLX5E_MAX_NUM_CHANNELS) :
+		  min(mdev->priv.eq_table.num_comp_vectors,
+		      MLX5E_MAX_NUM_CHANNELS);
+#else
 	int ncv = min_t(int, mdev->priv.eq_table.num_comp_vectors,
 			MLX5E_MAX_NUM_CHANNELS);
+#endif
 	int err;
 
 	if (mlx5e_check_required_hca_cap(mdev))
 		return NULL;
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
 #ifdef HAVE_NEW_TX_RING_SCHEME
 	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
 				    ncv * MLX5E_MAX_NUM_TC,
@@ -2466,6 +2706,9 @@ static void *mlx5e_create_netdev(struct
 #else
 	netdev = alloc_etherdev_mq(sizeof(struct mlx5e_priv), ncv);
 #endif
+#else
+	netdev = alloc_etherdev(sizeof(struct mlx5e_priv));
+#endif
 	if (!netdev) {
 		mlx5_core_err(mdev, "alloc_etherdev_mqs() failed\n");
 		return NULL;
@@ -2549,8 +2792,7 @@ static void mlx5e_destroy_netdev(struct
 	mlx5_core_dealloc_pd(priv->mdev, priv->pdn);
 	mlx5_unmap_free_uar(priv->mdev, &priv->cq_uar);
 	mlx5e_disable_async_events(priv);
-	cancel_work_sync(&priv->set_rx_mode_work);
-	cancel_work_sync(&priv->update_carrier_work);
+	msleep(1000);
 	/* this is used to serialize the marking of internal error
 	 * state and the restart of update stats work
 	 */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -102,7 +102,11 @@ inline int mlx5e_alloc_rx_wqe(struct mlx
 				  rq->wqe_sz,
 				  DMA_FROM_DEVICE);
 
+#ifdef HAVE_DMA_MAP
 	if (unlikely(dma_mapping_error(rq->pdev, dma_addr)))
+#else
+	if (unlikely(dma_mapping_error(dma_addr)))
+#endif
 		goto err_free_skb;
 
 	skb_reserve(skb, MLX5E_NET_IP_ALIGN);
@@ -144,7 +148,11 @@ inline int mlx5e_alloc_striding_rx_wqe(s
 
 	dma = dma_map_page(rq->pdev, page, 0, PAGE_SIZE << rq->page_order,
 			   PCI_DMA_FROMDEVICE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 	if (dma_mapping_error(rq->pdev, dma)) {
+#else
+	if (dma_mapping_error(dma)) {
+#endif
 		ret = -ENOMEM;
 		goto err_put_page;
 	}
@@ -253,8 +261,10 @@ static inline void mlx5e_skb_set_hash(st
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
 #else
+#ifdef HAVE_SKB_RXHASH
 	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
 #endif
+#endif
 }
 #endif
 
@@ -288,7 +298,15 @@ static inline void mlx5e_handle_csum(str
 				     struct mlx5e_rq *rq,
 				     struct sk_buff *skb)
 {
+#ifdef HAVE_RXCSUM
 	if (unlikely(!(netdev->features & NETIF_F_RXCSUM)))
+#else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	if (unlikely(!(((struct mlx5e_priv *)netdev_priv(netdev))->rx_csum_offload)))
+#else
+	if (unlikely(!(netdev->features & NETIF_F_IP_CSUM)))
+#endif
+#endif
 		goto csum_none;
 
 	if (likely(cqe->hds_ip_ext & CQE_L4_OK)) {
@@ -434,7 +452,12 @@ static inline void send_skb(struct mlx5e
 #endif
 		else
 #endif
+#ifdef HAVE_GRO
 			napi_gro_receive(cq->napi, skb);
+#else
+			netif_receive_skb(skb);
+#endif
+
 }
 
 struct sk_buff *mlx5e_poll_striding_rx_cq(struct mlx5_cqe64 *cqe,
@@ -481,7 +504,11 @@ struct sk_buff *mlx5e_poll_striding_rx_c
 	return skb;
 }
 
+#ifndef HAVE_OLD_NAPI
 bool mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
+#else
+int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int *budget)
+#endif
 {
 	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
 	struct net_device *netdev = rq->netdev;
@@ -496,8 +523,11 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 	if (!test_and_clear_bit(MLX5E_CQ_HAS_CQES, &cq->flags))
 		return false;
 	cqe = mlx5e_get_cqe(cq);
-
+#ifndef HAVE_OLD_NAPI
 	for (i = 0; i < budget; i++) {
+#else
+	for (i = 0; i < *budget; i++) {
+#endif
 		struct sk_buff *skb;
 		u16 bytes_recv = 0;
 		__be16 wqe_id_be;
@@ -534,6 +564,12 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 #endif
 		cqe = mlx5e_get_cqe(cq);
 
+		/* Use SKB's control buffer to mark xmit_more for the packet
+		* Do it only there are more CQEs and skip every 16th packet
+		*/
+		if (cqe && (i & 0xf))
+			skb->cb[47] = MLX5E_XMIT_MORE;
+
 #if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
                 send_skb(cq, rq, skb, prev_cqe);
 #else
@@ -552,7 +588,11 @@ wq_ll_pop:
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
 
+#ifndef HAVE_OLD_NAPI
 	if (i == budget) {
+#else
+	if (i == *budget) {
+#endif
 		set_bit(MLX5E_CQ_HAS_CQES, &cq->flags);
 		return true;
 	}
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -96,6 +96,8 @@ static inline void mlx5e_dma_get(struct
 	*size = sq->dma_fifo[i & sq->dma_fifo_mask].size;
 }
 
+#ifdef HAVE_NDO_SELECT_QUEUE
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
 #ifndef HAVE_SELECT_QUEUE_FALLBACK_T
 #define fallback(dev, skb) __netdev_pick_tx(dev, skb)
 #endif     
@@ -124,6 +126,8 @@ u16 mlx5e_select_queue(struct net_device
 #endif
 	return priv->tc_to_txq_map[channel_ix][tc];
 }
+#endif
+#endif
 
 static inline u16 mlx5e_get_inline_hdr_size(struct mlx5e_sq *sq,
 					    struct sk_buff *skb, bool bf)
@@ -152,6 +156,11 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 	u16 ihs;
 	int i;
 
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	if (unlikely(!mlx5e_sq_has_room_for(sq, MLX5_SEND_WQE_MAX_WQEBBS)))
+		return NETDEV_TX_BUSY;
+#endif
+
 	memset(wqe, 0, sizeof(*wqe));
 
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
@@ -203,7 +212,11 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 	if (headlen) {
 		dma_addr = dma_map_single(sq->pdev, skb->data, headlen,
 					  DMA_TO_DEVICE);
+#ifdef HAVE_DMA_MAP
 		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
+#else
+		if (unlikely(dma_mapping_error(dma_addr)))
+#endif
 			goto dma_unmap_wqe_err;
 
 		dseg->addr       = cpu_to_be64(dma_addr);
@@ -222,7 +235,11 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 
 		dma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,
 					    DMA_TO_DEVICE);
+#ifdef HAVE_DMA_MAP
 		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
+#else
+		if (unlikely(dma_mapping_error(dma_addr)))
+#endif
 			goto dma_unmap_wqe_err;
 
 		dseg->addr       = cpu_to_be64(dma_addr);
@@ -246,14 +263,21 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 							MLX5_SEND_WQEBB_NUM_DS);
 	sq->pc += MLX5E_TX_SKB_CB(skb)->num_wqebbs;
 
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
+#ifdef CONFIG_BQL
 	netdev_tx_sent_queue(sq->txq, MLX5E_TX_SKB_CB(skb)->num_bytes);
-
+#endif
 	if (unlikely(!mlx5e_sq_has_room_for(sq, MLX5E_SQ_STOP_ROOM))) {
 		netif_tx_stop_queue(sq->txq);
 		sq->stats.stopped++;
 	}
+#endif
+
 #ifdef HAVE_SK_BUFF_XMIT_MORE
 	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+#else
+	if (skb->cb[47] != MLX5E_XMIT_MORE ||
+		(!mlx5e_sq_has_room_for(sq, MLX5_SEND_WQE_MAX_WQEBBS)))
 #endif
 	{
 		int bf_sz = 0;
@@ -283,12 +307,50 @@ dma_unmap_wqe_err:
 	return NETDEV_TX_OK;
 }
 
+#ifdef HAVE_NETDEV_OPS
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
+#else
+int mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
-	struct mlx5e_sq *sq = priv->txq_to_sq_map[skb_get_queue_mapping(skb)];
+	struct mlx5e_sq *sq;
+	int ix;
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	int ret;
+#endif
+#ifdef HAVE_OLD_NAPI
+	struct mlx5e_channel *c;
+#endif
+
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
+	ix = skb_get_queue_mapping(skb);
+#else
+	/*
+	 * This check doesn't fully resolve the race
+	 * between TX and closing the resources,
+	 * but decreases the severity */
+	if (!test_bit(MLX5E_STATE_OPENED, &priv->state))
+		return NETDEV_TX_BUSY;
 
+	ix = priv->tx_indir_table[smp_processor_id() & MLX5E_TX_INDIR_MASK];
+#endif
+#ifndef HAVE_OLD_NAPI
+	sq = priv->txq_to_sq_map[ix];
+#else
+	c = priv->channel[(ix >= priv->params.num_channels / 2) ?
+		ix : ix + (priv->params.num_channels / 2)];
+	sq = &c->sq[0];
+#endif
+
+#ifndef HAVE_ALLOC_ETHERDEV_MQ
+	spin_lock(&sq->queue_lock);
+	ret = mlx5e_sq_xmit(sq, skb);
+	spin_unlock(&sq->queue_lock);
+	return ret;
+#else
 	return mlx5e_sq_xmit(sq, skb);
+#endif
 }
 
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq)
@@ -373,15 +435,17 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *c
 
 	sq->dma_fifo_cc = dma_fifo_cc;
 	sq->cc = sqcc;
-
+#ifdef HAVE_ALLOC_ETHERDEV_MQ
+#ifdef CONFIG_BQL
 	netdev_tx_completed_queue(sq->txq, npkts, nbytes);
-
+#endif
 	if (netif_tx_queue_stopped(sq->txq) &&
 	    mlx5e_sq_has_room_for(sq, MLX5E_SQ_STOP_ROOM) &&
 	    likely(test_bit(MLX5E_SQ_STATE_WAKE_TXQ_ENABLE, &sq->state))) {
 				netif_tx_wake_queue(sq->txq);
 				sq->stats.wake++;
 	}
+#endif
 	if (i == MLX5E_TX_CQ_POLL_BUDGET) {
 		set_bit(MLX5E_CQ_HAS_CQES, &cq->flags);
 		return true;
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -75,10 +75,18 @@ static inline bool mlx5e_no_channel_affi
 #endif
 }
 
+#ifndef HAVE_OLD_NAPI
 int mlx5e_napi_poll(struct napi_struct *napi, int budget)
+#else
+int mlx5e_napi_poll(struct net_device *poll_dev, int *budget)
+#endif
 {
+#ifndef HAVE_OLD_NAPI
 	struct mlx5e_channel *c = container_of(napi, struct mlx5e_channel,
 					       napi);
+#else
+	struct mlx5e_channel *c = poll_dev->priv;
+#endif
 	bool busy = false;
 	int i;
 
@@ -92,16 +100,32 @@ int mlx5e_napi_poll(struct napi_struct *
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq);
 
 #if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+#ifndef HAVE_OLD_NAPI
 	c->tot_rx += budget;
+#else
+	c->tot_rx += *budget;
+#endif
 #endif
 	if (busy && likely(mlx5e_no_channel_affinity_change(c)))
+#ifndef HAVE_OLD_NAPI
 		return budget;
+#else
+		return *budget;
+#endif
 
+#ifndef HAVE_OLD_NAPI
 	napi_complete(napi);
+#else
+	netif_rx_complete(poll_dev);
+#endif
 
 	/* avoid losing completion event during/after polling cqs */
 	if (test_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags)) {
+#ifndef HAVE_OLD_NAPI
 		napi_schedule(napi);
+#else
+		netif_rx_schedule(poll_dev);
+#endif
 		return 0;
 	}
 
@@ -119,15 +143,21 @@ void mlx5e_completion_event(struct mlx5_
 	set_bit(MLX5E_CQ_HAS_CQES, &cq->flags);
 	set_bit(MLX5E_CHANNEL_NAPI_SCHED, &cq->channel->flags);
 	barrier();
+#ifndef HAVE_OLD_NAPI
 	napi_schedule(cq->napi);
+#else
+	netif_rx_schedule(cq->poll_dev);
+#endif
 }
 
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event)
 {
+#ifndef HAVE_OLD_NAPI
 	struct mlx5e_cq *cq = container_of(mcq, struct mlx5e_cq, mcq);
 	struct mlx5e_channel *c = cq->channel;
 	struct mlx5e_priv *priv = c->priv;
 	struct net_device *netdev = priv->netdev;
+#endif
 
 	netdev_err(netdev, "%s: cqn=0x%.6x event=0x%.2x\n",
 		   __func__, mcq->cqn, event);
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@ -313,10 +313,11 @@ static int mlx5_eq_int(struct mlx5_core_
 			mlx5_eq_pagefault(dev, eqe);
 			break;
 #endif
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 		case MLX5_EVENT_TYPE_NIC_VPORT_CHANGE:
 			mlx5_vport_change(dev, eqe);
 			break;
-
+#endif
 		default:
 			mlx5_core_warn(dev, "Unhandled event 0x%x on EQ 0x%x\n",
 				       eqe->type, eq->eqn);
@@ -420,9 +421,11 @@ int mlx5_create_map_eq(struct mlx5_core_
 	if (err)
 		goto err_eq;
 
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_debug_eq_add(dev, eq);
 	if (err)
 		goto err_irq;
+#endif
 
 	/* EQs are created in ARMED state
 	 */
@@ -431,7 +434,9 @@ int mlx5_create_map_eq(struct mlx5_core_
 	kvfree(in);
 	return 0;
 
+#ifndef HAVE_NO_DEBUGFS
 err_irq:
+#endif
 	free_irq(priv->msix_arr[vecidx].vector, eq);
 
 err_eq:
@@ -451,7 +456,9 @@ int mlx5_destroy_unmap_eq(struct mlx5_co
 	struct mlx5_priv *priv = &dev->priv;
 	int err;
 
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_debug_eq_remove(dev, eq);
+#endif
 	free_irq(priv->msix_arr[eq->irqn].vector, eq);
 	err = mlx5_cmd_destroy_eq(dev, eq->eqn);
 	if (err)
@@ -470,7 +477,11 @@ int mlx5_eq_init(struct mlx5_core_dev *d
 
 	spin_lock_init(&dev->priv.eq_table.lock);
 
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_eq_debugfs_init(dev);
+#else
+	err = 0;
+#endif
 
 	return err;
 }
@@ -478,7 +489,9 @@ int mlx5_eq_init(struct mlx5_core_dev *d
 
 void mlx5_eq_cleanup(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_eq_debugfs_cleanup(dev);
+#endif
 }
 
 int mlx5_start_eqs(struct mlx5_core_dev *dev)
--- a/drivers/net/ethernet/mellanox/mlx5/core/fs_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/fs_core.h
@@ -279,9 +279,23 @@ struct fs_client_priv_data {
 };
 
 /* debugfs API */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 void fs_debugfs_remove(struct fs_base *base);
 int fs_debugfs_add(struct fs_base *base);
 void update_debugfs_dir_name(struct fs_base *base, const char *name);
+#else
+static inline void fs_debugfs_remove(struct fs_base *base)
+{
+}
+static inline int fs_debugfs_add(struct fs_base *base)
+{
+	return 0;
+}
+static inline void update_debugfs_dir_name(struct fs_base *base,
+					   const char *name)
+{
+}
+#endif
 void _fs_remove_node(struct kref *kref);
 #define fs_get_obj(v, _base)  {v = container_of((_base), typeof(*v), base); }
 #define fs_get_parent(v, child)  {v = (child)->base.parent ?		     \
--- a/drivers/net/ethernet/mellanox/mlx5/core/health.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/health.c
@@ -107,6 +107,7 @@ static int in_fatal(struct mlx5_core_dev
 
 void mlx5_enter_error_state(struct mlx5_core_dev *dev)
 {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (dev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR)
 		return;
 
@@ -116,6 +117,7 @@ void mlx5_enter_error_state(struct mlx5_
 
 	mlx5_core_event(dev, MLX5_DEV_EVENT_SYS_ERROR, 0);
 	mlx5_core_err(dev, "end\n");
+#endif
 }
 
 static void mlx5_handle_bad_state(struct mlx5_core_dev *dev)
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -64,6 +64,12 @@ static int prof_sel = MLX5_DEFAULT_PROF;
 module_param_named(prof_sel, prof_sel, int, 0444);
 MODULE_PARM_DESC(prof_sel, "profile selector. Valid range 0 - 2");
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+int num_channels = 0;
+module_param_named(num_channels, num_channels, int, 0444);
+MODULE_PARM_DESC(num_channels, "number of TX/RX channels. Valid values: 0 - #number_of_cores.\n\t\t\tDefault is 0 (equals to number of cores)");
+#endif
+
 static LIST_HEAD(intf_list);
 static LIST_HEAD(dev_list);
 static DEFINE_MUTEX(intf_mutex);
@@ -207,7 +213,9 @@ static int set_dma_caps(struct pci_dev *
 		}
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 25)
 	dma_set_max_seg_size(&pdev->dev, 2u * 1024 * 1024 * 1024);
+#endif
 	return err;
 }
 
@@ -275,8 +283,15 @@ static int mlx5_enable_msix(struct mlx5_
 #endif 
 	int i;
 
-	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
-	       MLX5_EQ_VEC_COMP_BASE;
+	nvec = MLX5_CAP_GEN(dev, num_ports) *
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+	       (num_channels ?
+		min_t(int, num_online_cpus(), num_channels) :
+		num_online_cpus()) +
+		MLX5_EQ_VEC_COMP_BASE;
+#else
+	       num_online_cpus() + MLX5_EQ_VEC_COMP_BASE;
+#endif
 	nvec = min_t(int, nvec, num_eqs);
 #ifdef CONFIG_PPC
 	nvec = min_t(int, nvec, PPC_MAX_VECTORS);
@@ -607,6 +622,7 @@ static int mlx5_core_set_issi(struct mlx
 	return -ENOTSUPP;
 }
 
+#ifndef HAVE_NO_AFFINITY
 static void mlx5_irq_set_affinity_hint(struct mlx5_core_dev *mdev, int i)
 {
 	struct mlx5_priv *priv  = &mdev->priv;
@@ -679,6 +695,7 @@ static void mlx5_irq_clear_affinity_hint
 		mlx5_irq_clear_affinity_hint(mdev, i);
 	}
 }
+#endif
 
 int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn, int *irqn)
 {
@@ -905,8 +922,17 @@ static int mlx5_pci_init(struct mlx5_cor
 {
 	struct pci_dev *pdev = dev->pdev;
 	int err = 0;
+#ifdef HAVE_CANNOT_KZALLOC_THAT_MUCH
+	int i;
+#endif
 
 	pci_set_drvdata(dev->pdev, dev);
+#ifdef HAVE_CANNOT_KZALLOC_THAT_MUCH
+	for (i = 0; i < MLX5_CAP_NUM; i++) {
+		kfree(dev->hca_caps_cur[i]);
+		kfree(dev->hca_caps_max[i]);
+	}
+#endif
 	strncpy(priv->name, dev_name(&pdev->dev), MLX5_MAX_NAME_LEN);
 	priv->name[MLX5_MAX_NAME_LEN - 1] = 0;
 
@@ -918,9 +944,11 @@ static int mlx5_pci_init(struct mlx5_cor
 
 	priv->numa_node = dev_to_node(&dev->pdev->dev);
 
+#ifndef HAVE_NO_DEBUGFS
 	priv->dbg_root = debugfs_create_dir(dev_name(&pdev->dev), mlx5_debugfs_root);
 	if (!priv->dbg_root)
 		return -ENOMEM;
+#endif
 
 	err = mlx5_pci_enable_device(dev);
 	if (err) {
@@ -953,7 +981,9 @@ static int mlx5_pci_init(struct mlx5_cor
 	return 0;
 
 err_clr_master:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#endif
 	release_bar(dev->pdev);
 err_disable:
 	mlx5_pci_disable_device(dev);
@@ -966,12 +996,15 @@ err_dbg:
 static void mlx5_pci_close(struct mlx5_core_dev *dev, struct mlx5_priv *priv)
 {
 	iounmap(dev->iseg);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#endif
 	release_bar(dev->pdev);
 	mlx5_pci_disable_device(dev);
 	debugfs_remove(priv->dbg_root);
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 /* TODO: Calling to io_mapping_create_wc spoils the IB user BF mapping as WC
  *       Fix this before enabling this function.
 static int map_bf_area(struct mlx5_core_dev *dev)
@@ -990,6 +1023,7 @@ static void unmap_bf_area(struct mlx5_co
 	if (dev->priv.bf_mapping)
 		io_mapping_free(dev->priv.bf_mapping);
 }
+#endif
 
 static void enable_vfs(struct pci_dev *pdev)
 {
@@ -1194,13 +1228,16 @@ static int mlx5_load_one(struct mlx5_cor
 		goto err_stop_eqs;
 	}
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	/*
 	 * if (map_bf_area(dev))
 	 *	dev_err(&pdev->dev, "Failed to map blue flame area\n");
 	 * TODO: Open this mapping when map_bf_area is fixed
 	 */
-
+#endif
+#ifndef HAVE_NO_AFFINITY
 	mlx5_irq_set_affinity_hints(dev);
+#endif
 	MLX5_INIT_DOORBELL_LOCK(&priv->cq_uar_lock);
 
 	mlx5_init_cq_table(dev);
@@ -1214,7 +1251,7 @@ static int mlx5_load_one(struct mlx5_cor
 		mlx5_core_err(dev, "flow steering init %d\n", err);
 		goto err_reg_dev;
 	}
-
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	err = mlx5_eswitch_init(dev);
 	if (err) {
 		dev_err(&pdev->dev, "eswitch init failed %d\n", err);
@@ -1226,16 +1263,18 @@ static int mlx5_load_one(struct mlx5_cor
 		dev_err(&pdev->dev, "sriov init failed %d\n", err);
 		goto err_eswitch;
 	}
-
+#endif
 	err = mlx5_register_device(dev);
 	if (err) {
 		dev_err(&pdev->dev, "mlx5_register_device failed %d\n", err);
 		goto err_sriov;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 30)
 	err = request_module_nowait(MLX5_IB_MOD);
 	if (err)
 		pr_info("failed request module on %s\n", MLX5_IB_MOD);
+#endif
 
 	dev->interface_state = MLX5_INTERFACE_STATE_UP;
 out:
@@ -1244,11 +1283,13 @@ out:
 	return 0;
 
 err_sriov:
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (mlx5_sriov_cleanup(dev))
 		dev_err(&dev->pdev->dev, "sriov cleanup failed\n");
 err_eswitch:
 	mlx5_eswitch_cleanup(dev);
 err_fs:
+#endif
 	mlx5_cleanup_fs(dev);
 err_reg_dev:
 	mlx5_cleanup_dct_table(dev);
@@ -1256,7 +1297,9 @@ err_reg_dev:
 	mlx5_cleanup_srq_table(dev);
 	mlx5_cleanup_qp_table(dev);
 	mlx5_cleanup_cq_table(dev);
+#ifndef HAVE_NO_AFFINITY
 	mlx5_irq_clear_affinity_hints(dev);
+#endif
 	free_comp_eqs(dev);
 
 err_stop_eqs:
@@ -1303,12 +1346,14 @@ static int mlx5_unload_one(struct mlx5_c
 {
 	int err = 0;
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	err = mlx5_sriov_cleanup(dev);
 	if (err) {
 		dev_warn(&dev->pdev->dev, "%s: sriov cleanup failed - abort\n",
 			 __func__);
 		return err;
 	}
+#endif
 	mutex_lock(&dev->intf_state_mutex);
 	if (dev->interface_state == MLX5_INTERFACE_STATE_DOWN) {
 		dev_warn(&dev->pdev->dev, "%s: interface is down, NOP\n",
@@ -1318,15 +1363,21 @@ static int mlx5_unload_one(struct mlx5_c
 
 	mlx5_unregister_device(dev);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	mlx5_eswitch_cleanup(dev);
+#endif
 	mlx5_cleanup_fs(dev);
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
 	mlx5_cleanup_srq_table(dev);
 	mlx5_cleanup_qp_table(dev);
 	mlx5_cleanup_cq_table(dev);
+#ifndef HAVE_NO_AFFINITY
 	mlx5_irq_clear_affinity_hints(dev);
+#endif
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	unmap_bf_area(dev);
+#endif
 	free_comp_eqs(dev);
 	mlx5_stop_eqs(dev);
 	mlx5_free_uuars(dev, &priv->uuari);
@@ -1378,12 +1429,40 @@ static int init_one(struct pci_dev *pdev
 	struct mlx5_core_dev *dev;
 	struct mlx5_priv *priv;
 	int err;
+#ifdef HAVE_CANNOT_KZALLOC_THAT_MUCH
+	int i;
+#endif
 
 	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
 	if (!dev) {
 		dev_err(&pdev->dev, "kzalloc failed\n");
 		return -ENOMEM;
 	}
+
+#ifdef HAVE_CANNOT_KZALLOC_THAT_MUCH
+	for (i = 0; i < MLX5_CAP_NUM; ++i) {
+		dev->hca_caps_cur[i] = kzalloc(MLX5_UN_SZ_BYTES(hca_cap_union), GFP_KERNEL);
+		if (!dev->hca_caps_cur[i]) {
+			dev_err(&pdev->dev, "kzalloc failed\n");
+			for (; i >= 0; i--)
+				kfree(dev->hca_caps_cur[i]);
+			return -ENOMEM;
+		}
+	}
+
+	for (i = 0; i < MLX5_CAP_NUM; ++i) {
+		dev->hca_caps_max[i] = kzalloc(MLX5_UN_SZ_BYTES(hca_cap_union), GFP_KERNEL);
+		if (!dev->hca_caps_max[i]) {
+			dev_err(&pdev->dev, "kzalloc failed\n");
+			for (; i >= 0; i--)
+				kfree(dev->hca_caps_max[i]);
+			for (i = 0; i < MLX5_CAP_NUM; ++i)
+				kfree(dev->hca_caps_cur[i]);
+			return -ENOMEM;
+		}
+	}
+#endif
+
 	priv = &dev->priv;
 	priv->pci_dev_data = id->driver_data;
 
@@ -1438,6 +1517,9 @@ static void remove_one(struct pci_dev *p
 {
 	struct mlx5_core_dev *dev  = pci_get_drvdata(pdev);
 	struct mlx5_priv *priv = &dev->priv;
+#ifdef HAVE_CANNOT_KZALLOC_THAT_MUCH
+	int i;
+#endif
 
 	if (mlx5_unload_one(dev, priv)) {
 		dev_err(&dev->pdev->dev, "mlx5_unload_one failed\n");
@@ -1450,6 +1532,7 @@ static void remove_one(struct pci_dev *p
 	kfree(dev);
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 #ifdef CONFIG_PM
 static int suspend(struct device *device)
 {
@@ -1530,6 +1613,7 @@ static const struct dev_pm_ops mlnx_pm =
 	.resume = resume,
 };
 #endif	/* CONFIG_PM */
+#endif
 
 static pci_ers_result_t mlx5_pci_err_detected(struct pci_dev *pdev,
 					      pci_channel_state_t state)
@@ -1695,11 +1779,13 @@ MODULE_DEVICE_TABLE(pci, mlx5_core_pci_t
 static struct pci_driver mlx5_core_driver = {
 	.name           = DRIVER_NAME,
 	.id_table       = mlx5_core_pci_table,
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 #ifdef CONFIG_PM
 	.driver = {
 		.pm	= &mlnx_pm,
 	},
 #endif /* CONFIG_PM */
+#endif
 	.probe			= init_one,
 	.remove			= remove_one,
 	.shutdown		= shutdown,
@@ -1713,7 +1799,9 @@ static int __init init(void)
 {
 	int err;
 
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_register_debugfs();
+#endif
 	err = pci_register_driver(&mlx5_core_driver);
 	if (err)
 		goto err_debug;
@@ -1723,7 +1811,9 @@ static int __init init(void)
 	return 0;
 
 err_debug:
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_unregister_debugfs();
+#endif
 	return err;
 }
 
@@ -1731,7 +1821,9 @@ static void __exit cleanup(void)
 {
 	mlx5e_cleanup();
 	pci_unregister_driver(&mlx5_core_driver);
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_unregister_debugfs();
+#endif
 }
 
 module_init(init);
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -118,6 +118,7 @@ int mlx5_query_port_ets_rate_limit(struc
 				   u8 max_bw_value[MLX5_MAX_NUM_TC],
 				   u8 max_bw_unit[MLX5_MAX_NUM_TC]);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 int mlx5_eswitch_init(struct mlx5_core_dev *mdev);
 void mlx5_eswitch_cleanup(struct mlx5_core_dev *mdev);
 void mlx5_vport_change(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe);
@@ -135,6 +136,7 @@ int mlx5_eswitch_set_vport_spoofchk(stru
 int mlx5_eswitch_get_vport_config(struct mlx5_core_dev *dev,
 				  int vport, struct ifla_vf_info *ivi);
 #endif
+#endif
 
 void mlx5e_init(void);
 void mlx5e_cleanup(void);
--- a/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
@@ -239,7 +239,11 @@ static int alloc_system_page(struct mlx5
 	}
 	addr = dma_map_page(&dev->pdev->dev, page, 0,
 			    PAGE_SIZE, DMA_BIDIRECTIONAL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 31)
 	if (dma_mapping_error(&dev->pdev->dev, addr)) {
+#else
+	if (dma_mapping_error(addr)) {
+#endif
 		mlx5_core_warn(dev, "failed dma mapping page\n");
 		err = -ENOMEM;
 		goto out_alloc;
--- a/drivers/net/ethernet/mellanox/mlx5/core/qp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/qp.c
@@ -258,10 +258,12 @@ int mlx5_core_create_qp(struct mlx5_core
 	if (err)
 		goto err_cmd;
 
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_debug_qp_add(dev, qp);
 	if (err)
 		mlx5_core_dbg(dev, "failed adding QP 0x%x to debug file system\n",
 			      qp->qpn);
+#endif
 
 	atomic_inc(&dev->num_qps);
 
@@ -285,7 +287,9 @@ int mlx5_core_destroy_qp(struct mlx5_cor
 	struct mlx5_destroy_qp_mbox_out out;
 	int err;
 
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_debug_qp_remove(dev, qp);
+#endif
 
 	destroy_qprqsq_common(dev, qp, MLX5_RES_QP);
 
@@ -330,22 +334,30 @@ void mlx5_init_qp_table(struct mlx5_core
 	memset(table, 0, sizeof(*table));
 	spin_lock_init(&table->lock);
 	INIT_RADIX_TREE(&table->tree, GFP_ATOMIC);
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_qp_debugfs_init(dev);
+#endif
 }
 
 void mlx5_cleanup_qp_table(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_qp_debugfs_cleanup(dev);
+#endif
 }
 
 void mlx5_init_dct_table(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_dct_debugfs_init(dev);
+#endif
 }
 
 void mlx5_cleanup_dct_table(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_dct_debugfs_cleanup(dev);
+#endif
 }
 
 int mlx5_core_qp_query(struct mlx5_core_dev *dev, struct mlx5_core_qp *qp,
@@ -446,10 +458,12 @@ int mlx5_core_create_dct(struct mlx5_cor
 		goto err_cmd;
 	}
 
+#ifndef HAVE_NO_DEBUGFS
 	err = mlx5_debug_dct_add(dev, dct);
 	if (err)
 		mlx5_core_dbg(dev, "failed adding DCT 0x%x to debug file system\n",
 			      dct->dctn);
+#endif
 
 	dct->pid = current->pid;
 	atomic_set(&dct->common.refcount, 1);
@@ -506,7 +520,9 @@ int mlx5_core_destroy_dct(struct mlx5_co
 
 	wait_for_completion(&dct->drained);
 
+#ifndef HAVE_NO_DEBUGFS
 	mlx5_debug_dct_remove(dev, dct);
+#endif
 
 	spin_lock_irqsave(&table->lock, flags);
 	if (radix_tree_delete(&table->tree, dct->dctn) != dct)
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18))
 #include <linux/pci.h>
 #include <linux/sysfs.h>
 #include <linux/mlx5/driver.h>
@@ -654,3 +655,4 @@ int mlx5_sriov_cleanup(struct mlx5_core_
 	mlx5_sriov_sysfs_cleanup(dev);
 	return 0;
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/uar.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/uar.c
@@ -209,9 +209,11 @@ int mlx5_alloc_map_uar(struct mlx5_core_
 		goto err_free_uar;
 	}
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	if (mdev->priv.bf_mapping)
 		uar->bf_map = io_mapping_map_wc(mdev->priv.bf_mapping,
 						uar->index << PAGE_SHIFT);
+#endif
 
 	return 0;
 
@@ -224,7 +226,9 @@ EXPORT_SYMBOL(mlx5_alloc_map_uar);
 
 void mlx5_unmap_free_uar(struct mlx5_core_dev *mdev, struct mlx5_uar *uar)
 {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	io_mapping_unmap(uar->bf_map);
+#endif
 	iounmap(uar->map);
 	mlx5_cmd_free_uar(mdev, uar->index);
 }
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -385,7 +385,9 @@ struct mlx5_eq {
 	struct list_head	list;
 	int			index;
 	struct mlx5_rsc_debug	*dbg;
+#ifndef HAVE_NO_AFFINITY
 	cpumask_var_t		affinity_mask;
+#endif
 };
 
 struct mlx5_core_psv {
@@ -442,7 +444,9 @@ struct mlx5_eq_table {
 	struct mlx5_eq		pages_eq;
 	struct mlx5_eq		async_eq;
 	struct mlx5_eq		cmd_eq;
+#ifndef HAVE_NO_AFFINITY
 	cpumask_var_t		*irq_masks;
+#endif
 	int			num_comp_vectors;
 	/* protect EQs list
 	 */
@@ -563,7 +567,9 @@ struct mlx5_core_sriov {
 };
 
 struct mlx5_irq_info {
+#ifndef HAVE_NO_AFFINITY
 	cpumask_var_t mask;
+#endif
 	char name[MLX5_MAX_IRQ_NAME];
 };
 
@@ -659,8 +665,13 @@ struct mlx5_core_dev {
 	char			board_id[MLX5_BOARD_ID_LEN];
 	struct mlx5_cmd		cmd;
 	struct mlx5_port_caps	port_caps[MLX5_MAX_PORTS];
+#ifndef HAVE_CANNOT_KZALLOC_THAT_MUCH
 	u32 hca_caps_cur[MLX5_CAP_NUM][MLX5_UN_SZ_DW(hca_cap_union)];
 	u32 hca_caps_max[MLX5_CAP_NUM][MLX5_UN_SZ_DW(hca_cap_union)];
+#else
+	u32 *hca_caps_cur[MLX5_CAP_NUM];
+	u32 *hca_caps_max[MLX5_CAP_NUM];
+#endif
 	phys_addr_t		iseg_base;
 	struct mlx5_init_seg __iomem *iseg;
 	enum mlx5_device_state	state;
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2340,9 +2340,11 @@ int ib_modify_port(struct ib_device *dev
 		   u8 port_num, int port_modify_mask,
 		   struct ib_port_modify *port_modify);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION (2, 6, 18)
 int ib_find_gid(struct ib_device *device, union ib_gid *gid,
 		enum ib_gid_type gid_type, struct net *net,
 		int if_index, u8 *port_num, u16 *index);
+#endif
 
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);
@@ -2742,7 +2744,11 @@ static inline int ib_dma_mapping_error(s
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->mapping_error(dev, dma_addr);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
 	return dma_mapping_error(dev->dma_device, dma_addr);
+#else
+	return dma_mapping_error(dma_addr);
+#endif
 }
 
 /**
@@ -3419,11 +3425,15 @@ int ib_query_mkey(struct ib_mr *mr, u64
  */
 static inline int ib_is_virtfn(struct ib_device *ibdev)
 {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
 	struct pci_dev *pdev;
 
 	pdev = container_of(ibdev->dma_device, struct pci_dev, dev);
 
 	return !pdev->is_physfn;
+#else
+	return 0;
+#endif
 }
 
 /**
