From: Eli Cohen <eli@mellanox.com>
Subject: [PATCH] BACKPORT: mlx5 support for SLES10 SP3

Change-Id: I06e7ca0486b71959b3d30d010b7d816d172108b6
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/infiniband/hw/mlx5/doorbell.c              |   8 ++
 drivers/infiniband/hw/mlx5/main.c                  | 144 ++++++++++++++++++++-
 drivers/infiniband/hw/mlx5/mem.c                   |  94 ++++++++++++++
 drivers/infiniband/hw/mlx5/mr.c                    |  78 +++++++++++
 drivers/infiniband/hw/mlx5/qp.c                    |  17 +++
 drivers/infiniband/hw/mlx5/roce.c                  |   2 +
 drivers/net/ethernet/mellanox/mlx5/core/Makefile   |   4 +-
 drivers/net/ethernet/mellanox/mlx5/core/alloc.c    |  11 ++
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      |  16 +++
 drivers/net/ethernet/mellanox/mlx5/core/debugfs.c  |  19 +++
 drivers/net/ethernet/mellanox/mlx5/core/main.c     |  42 +++++-
 .../net/ethernet/mellanox/mlx5/core/mlx5_core.h    |   9 ++
 .../net/ethernet/mellanox/mlx5/core/pagealloc.c    |   4 +
 drivers/net/ethernet/mellanox/mlx5/core/uar.c      |   2 +
 include/linux/mlx5/driver.h                        |  10 +-
 15 files changed, 453 insertions(+), 7 deletions(-)

--- a/drivers/infiniband/hw/mlx5/doorbell.c
+++ b/drivers/infiniband/hw/mlx5/doorbell.c
@@ -47,6 +47,9 @@ int mlx5_ib_db_map_user(struct mlx5_ib_u
 			struct mlx5_db *db)
 {
 	struct mlx5_ib_user_db_page *page;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	struct ib_umem_chunk *chunk;
+#endif
 	int err = 0;
 
 	mutex_lock(&context->db_page_mutex);
@@ -74,7 +77,12 @@ int mlx5_ib_db_map_user(struct mlx5_ib_u
 	list_add(&page->list, &context->db_page_list);
 
 found:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
+#else
+	chunk = list_entry(page->umem->chunk_list.next, struct ib_umem_chunk, list);
+	db->dma		= sg_dma_address(chunk->page_list) + (virt & ~PAGE_MASK);
+#endif
 	db->u.user_page = page;
 	++page->refcnt;
 
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -36,7 +36,9 @@
 #include <linux/pci.h>
 #include <linux/dma-mapping.h>
 #include <linux/slab.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/sched.h>
 #include <linux/highmem.h>
 #include <linux/spinlock.h>
@@ -69,6 +71,15 @@ static char mlx5_version[] =
 	DRIVER_NAME ": Mellanox Connect-IB Infiniband driver v"
 	DRIVER_VERSION " (" DRIVER_RELDATE ")\n";
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+#define MLX5_WC_FLAGS   (_PAGE_PWT)
+
+pgprot_t pgprot_wc(pgprot_t _prot)
+{
+	return __pgprot(pgprot_val(_prot) | MLX5_WC_FLAGS);
+}
+#endif
+
 static void ext_atomic_caps(struct mlx5_ib_dev *dev,
 			    struct ib_exp_device_attr *props)
 {
@@ -208,10 +219,14 @@ static int mlx5_query_system_image_guid(
 		return err;
 
 	case MLX5_VPORT_ACCESS_METHOD_NIC:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		err = mlx5_query_nic_vport_system_image_guid(mdev, &tmp);
 		if (!err)
 			*sys_image_guid = cpu_to_be64(tmp);
 		return err;
+#else
+		return 0;
+#endif
 
 	default:
 		return -EINVAL;
@@ -280,11 +295,15 @@ static int mlx5_query_node_guid(struct m
 		return err;
 
 	case MLX5_VPORT_ACCESS_METHOD_NIC:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		err = mlx5_query_nic_vport_node_guid(dev->mdev, &tmp);
 		if (!err)
 			*node_guid = cpu_to_be64(tmp);
 
 		return err;
+#else
+		return 0;
+#endif
 
 	default:
 		return -EINVAL;
@@ -374,7 +393,11 @@ static int query_device(struct ib_device
 		props->device_cap_flags |= IB_DEVICE_BLOCK_MULTICAST_LOOPBACK;
 
 	props->vendor_part_id	   = mdev->pdev->device;
-	props->hw_ver		   = mdev->pdev->revision;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+	props->hw_ver = mdev->pdev->revision;
+#else
+	pci_read_config_byte(mdev->pdev, PCI_REVISION_ID, &mdev->rev_id);
+#endif
 
 	props->max_mr_size	   = ~0ull;
 	props->page_size_cap	   = ~(u32)((1ull << MLX5_CAP_GEN(mdev, log_pg_sz)) -1);
@@ -928,7 +951,11 @@ static int mlx5_ib_dealloc_ucontext(stru
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static phys_addr_t uar_index2pfn(struct mlx5_ib_dev *dev, int index)
+#else
+static u64 uar_index2pfn(struct mlx5_ib_dev *dev, int index)
+#endif
 {
 	return (pci_resource_start(dev->mdev->pdev, 0) >> PAGE_SHIFT) + index;
 }
@@ -1073,7 +1100,11 @@ static inline bool mlx5_writecombine_ava
 {
 	pgprot_t prot = __pgprot(0);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (pgprot_val(pgprot_writecombine(prot)) == pgprot_val(pgprot_noncached(prot)))
+#else
+	if (pgprot_val(pgprot_wc(prot)) == pgprot_val(pgprot_noncached(prot)))
+#endif
 		return false;
 
 	return true;
@@ -1084,7 +1115,11 @@ static int uar_mmap(struct vm_area_struc
 		    struct mlx5_ib_ucontext *context)
 {
 	unsigned long idx;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	phys_addr_t pfn;
+#else
+       u64 pfn;
+#endif
 	struct mlx5_ib_vma_private_data *vma_prv;
 
 	if (vma->vm_end - vma->vm_start != PAGE_SIZE)
@@ -1172,10 +1207,12 @@ static int mlx5_ib_mmap(struct ib_uconte
 	struct mlx5_dc_tracer *dct;
 	unsigned long command;
 	int err;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long total_size;
 	unsigned long order;
 	struct ib_cmem *ib_cmem;
 	int numa_node;
+#endif
 	phys_addr_t pfn;
 
 	command = get_command(vma->vm_pgoff);
@@ -1192,7 +1229,11 @@ static int mlx5_ib_mmap(struct ib_uconte
 		break;
 
 	case MLX5_IB_MMAP_REGULAR_PAGE:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		return uar_mmap(vma, pgprot_writecombine(vma->vm_page_prot),
+#else
+		return uar_mmap(vma, pgprot_wc(vma->vm_page_prot),
+#endif
 				mlx5_writecombine_available(),
 				uuari, dev, context);
 
@@ -1200,6 +1241,7 @@ static int mlx5_ib_mmap(struct ib_uconte
 
 	case MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA:
 	case MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_DEV_NUMA:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES:
 		if (command == MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA)
 			numa_node = numa_node_id();
@@ -1226,7 +1268,11 @@ static int mlx5_ib_mmap(struct ib_uconte
 		if (!mlx5_writecombine_available())
 			return -EPERM;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		return uar_mmap(vma, pgprot_writecombine(vma->vm_page_prot),
+#else
+		return uar_mmap(vma, pgprot_wc(vma->vm_page_prot),
+#endif
 				true, uuari, dev, context);
 		break;
 
@@ -1239,6 +1285,7 @@ static int mlx5_ib_mmap(struct ib_uconte
 		return alloc_and_map_wc(dev, context, get_index(vma->vm_pgoff),
 					vma);
 		break;
+#endif
 
 	default:
 		return -EINVAL;
@@ -1383,6 +1430,7 @@ static int mlx5_ib_dealloc_pd(struct ib_
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static struct mlx5_ib_fs_mc_flow  *get_mc_flow(struct mlx5_ib_qp *mqp,
 					       union ib_gid *gid)
 {
@@ -1948,6 +1996,7 @@ unlock:
 	return ERR_PTR(err);
 }
 
+#endif
 static int init_node_data(struct mlx5_ib_dev *dev)
 {
 	int err;
@@ -1956,11 +2005,16 @@ static int init_node_data(struct mlx5_ib
 	if (err)
 		return err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev->mdev->rev_id = dev->mdev->pdev->revision;
+#else
+	pci_read_config_byte(dev->mdev->pdev, PCI_REVISION_ID, &dev->mdev->rev_id);
+#endif
 
 	return mlx5_query_node_guid(dev, &dev->ib_dev.node_guid);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_fw_pages(struct device *device, struct device_attribute *attr,
 			     char *buf)
 {
@@ -2028,6 +2082,74 @@ static struct device_attribute *mlx5_cla
 	&dev_attr_fw_pages,
 	&dev_attr_reg_pages,
 };
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
+static ssize_t show_fw_pages(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d\n", dev->mdev->priv.fw_pages);
+}
+
+static ssize_t show_reg_pages(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d\n", dev->mdev->priv.reg_pages);
+}
+
+static ssize_t show_hca(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "MT%d\n", dev->mdev->pdev->device);
+}
+
+static ssize_t show_fw_ver(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d.%d.%d\n", fw_rev_maj(dev->mdev),
+		       fw_rev_min(dev->mdev), fw_rev_sub(dev->mdev));
+}
+
+static ssize_t show_rev(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%x\n", dev->mdev->rev_id);
+}
+
+static ssize_t show_board(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%.*s\n", MLX5_BOARD_ID_LEN,
+		       dev->mdev->board_id);
+}
+
+static CLASS_DEVICE_ATTR(hw_rev,   S_IRUGO, show_rev,    NULL);
+static CLASS_DEVICE_ATTR(fw_ver,   S_IRUGO, show_fw_ver, NULL);
+static CLASS_DEVICE_ATTR(hca_type, S_IRUGO, show_hca,    NULL);
+static CLASS_DEVICE_ATTR(board_id, S_IRUGO, show_board,  NULL);
+static CLASS_DEVICE_ATTR(fw_pages, S_IRUGO, show_fw_pages, NULL);
+static CLASS_DEVICE_ATTR(reg_pages, S_IRUGO, show_reg_pages, NULL);
+
+static struct class_device_attribute *mlx5_class_attributes[] = {
+	&class_device_attr_hw_rev,
+	&class_device_attr_fw_ver,
+	&class_device_attr_hca_type,
+	&class_device_attr_board_id,
+	&class_device_attr_fw_pages,
+	&class_device_attr_reg_pages,
+};
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 static void mlx5_ib_handle_internal_error(struct mlx5_ib_dev *ibdev)
 {
@@ -2669,7 +2791,11 @@ static void enable_dc_tracer(struct mlx5
 	dct->size = size;
 	dct->order = order;
 	dct->dma = dma_map_page(device, dct->pg, 0, size, DMA_FROM_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(device, dct->dma))
+#else
+	if (dma_mapping_error(dct->dma))
+#endif
 		goto map_err;
 
 	err = mlx5_core_set_dc_cnak_trace(dev->mdev, 1, dct->dma);
@@ -3410,6 +3536,7 @@ static void *mlx5_ib_add(struct mlx5_cor
 
 	if (mlx5_use_mad_ifc(dev))
 		get_ext_port_caps(dev);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET) {
 		if (MLX5_CAP_GEN(mdev, roce)) {
@@ -3420,6 +3547,7 @@ static void *mlx5_ib_add(struct mlx5_cor
 			goto err_dealloc;
 		}
 	}
+#endif
 
 	MLX5_INIT_DOORBELL_LOCK(&dev->uar_lock);
 
@@ -3510,10 +3638,12 @@ static void *mlx5_ib_add(struct mlx5_cor
 	dev->ib_dev.reg_user_mr		= mlx5_ib_reg_user_mr;
 	dev->ib_dev.dereg_mr		= mlx5_ib_dereg_mr;
 	dev->ib_dev.destroy_mr		= mlx5_ib_destroy_mr;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev->ib_dev.attach_mcast	= mlx5_ib_mcg_attach;
 	dev->ib_dev.detach_mcast	= mlx5_ib_mcg_detach;
 	dev->ib_dev.create_flow		= mlx5_ib_create_flow;
 	dev->ib_dev.destroy_flow	= mlx5_ib_destroy_flow;
+#endif
 	dev->ib_dev.process_mad		= mlx5_ib_process_mad;
 	dev->ib_dev.create_mr		= mlx5_ib_create_mr;
 	dev->ib_dev.alloc_fast_reg_mr	= mlx5_ib_alloc_fast_reg_mr;
@@ -3608,12 +3738,20 @@ static void *mlx5_ib_add(struct mlx5_cor
 			mlx5_ib_dbg(dev, "init_dc_improvements - continuing\n");
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	for (i = 0; i < ARRAY_SIZE(mlx5_class_attributes); i++) {
 		err = device_create_file(&dev->ib_dev.dev,
 					 mlx5_class_attributes[i]);
 		if (err)
 			goto err_dc;
 	}
+#else
+	for (i = 0; i < ARRAY_SIZE(mlx5_class_attributes); ++i) {
+		if (class_device_create_file(&dev->ib_dev.class_dev,
+				       mlx5_class_attributes[i]))
+			goto err_dc;
+	}
+#endif
 
 	dev->ib_active = true;
 
@@ -3635,9 +3773,11 @@ err_rsrc:
 	destroy_dev_resources(&dev->devr);
 
 err_disable_roce:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET && MLX5_CAP_GEN(mdev, roce))
 		mlx5_nic_vport_disable_roce(mdev);
+#endif
 
 err_dealloc:
 	ib_dealloc_device((struct ib_device *)dev);
@@ -3657,9 +3797,11 @@ static void mlx5_ib_remove(struct mlx5_c
 	mlx5_ib_odp_remove_one(dev);
 	destroy_dev_resources(&dev->devr);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET && MLX5_CAP_GEN(mdev, roce))
 		mlx5_nic_vport_disable_roce(mdev);
+#endif
 
 	ib_dealloc_device(&dev->ib_dev);
 }
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -45,6 +45,7 @@
 void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift,
 			int *ncont, int *order)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long tmp;
 	unsigned long m;
 	int i, k;
@@ -119,6 +120,69 @@ void mlx5_ib_cont_pages(struct ib_umem *
 	}
 	*shift = page_shift + m;
 	*count = i;
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+	struct ib_umem_chunk *chunk;
+	int i, j, k;
+	u64 len;
+	u64 pfn;
+	u64 base = 0;
+	unsigned long m;
+	int skip;
+	int mask;
+	int p = 0;
+	unsigned long tmp;
+
+	addr = addr >> PAGE_SHIFT;
+	tmp = (unsigned long)addr;
+	m = find_first_bit(&tmp, sizeof(tmp));
+	skip = 1 << m;
+	mask = skip - 1;
+	i = 0;
+	list_for_each_entry(chunk, &umem->chunk_list, list)
+		for (j = 0; j < chunk->nmap; ++j) {
+			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
+			pfn = sg_dma_address(&chunk->page_list[j]) >> PAGE_SHIFT;
+			for (k = 0; k < len; ++k) {
+				if (!(i & mask)) {
+					tmp = (unsigned long)pfn;
+					m = min(m, find_first_bit(&tmp, sizeof(tmp)));
+					skip = 1 << m;
+					mask = skip - 1;
+					base = pfn;
+					p = 0;
+				} else {
+					if (base + p != pfn) {
+						tmp = (unsigned long)p;
+						m = find_first_bit(&tmp, sizeof(tmp));
+						skip = 1 << m;
+						mask = skip - 1;
+						base = pfn;
+						p = 0;
+					}
+				}
+				++p;
+				++i;
+			}
+		}
+
+	if (i) {
+		m = min_t(unsigned long, ilog2(roundup_pow_of_two(i)), m);
+
+		if (order)
+			*order = ilog2(roundup_pow_of_two(i) >> m);
+
+		*ncont = DIV_ROUND_UP(i, (1 << m));
+	} else {
+		m  = 0;
+
+		if (order)
+			*order = 0;
+
+		*ncont = 0;
+	}
+	*shift = PAGE_SHIFT + m;
+	*count = i;
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@ -152,6 +216,7 @@ void __mlx5_ib_populate_pas(struct mlx5_
 			    int page_shift, size_t offset, size_t num_pages,
 			    __be64 *pas, int access_flags)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long umem_page_shift = ilog2(umem->page_size);
 	int shift = page_shift - umem_page_shift;
 	int mask = (1 << shift) - 1;
@@ -195,6 +260,35 @@ void __mlx5_ib_populate_pas(struct mlx5_
 			i++;
 		}
 	}
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+	struct ib_umem_chunk *chunk;
+	int i, j, k;
+	int len;
+	u64 cur = 0;
+	u64 base;
+	int shift = page_shift - PAGE_SHIFT;
+	int mask = (1 << shift) - 1;
+
+	i = 0;
+	list_for_each_entry(chunk, &umem->chunk_list, list)
+		for (j = 0; j < chunk->nmap; ++j) {
+			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
+			base = sg_dma_address(&chunk->page_list[j]);
+			for (k = 0; k < len; ++k) {
+				if (!(i & mask)) {
+					cur = base + (k << PAGE_SHIFT);
+					cur |= access_flags;
+
+					pas[i >> shift] = cpu_to_be64(cur);
+					mlx5_ib_dbg(dev, "pas[%d] 0x%llx\n",
+						    i >> shift, be64_to_cpu(pas[i >> shift]));
+				}  else
+					mlx5_ib_dbg(dev, "=====> 0x%llx\n",
+						    base + (k << PAGE_SHIFT));
+				++i;
+			}
+		}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -45,9 +45,11 @@
 #include <rdma/ib_umem_odp.h>
 #include <rdma/ib_verbs.h>
 #include "mlx5_ib.h"
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void mlx5_invalidate_umem(void *invalidation_cookie,
 				 struct ib_umem *umem,
 				 unsigned long addr, size_t size);
+#endif
 
 enum {
 	MAX_PENDING_REG_MR = 8,
@@ -696,7 +698,11 @@ static struct mlx5_ib_mr *reg_umr(struct
 	memset(pas + npages, 0, size - npages * sizeof(u64));
 
 	dma = dma_map_single(ddev, pas, size, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(mr->dma)) {
+#endif
 		err = -ENOMEM;
 		goto free_pas;
 	}
@@ -793,7 +799,11 @@ int mlx5_ib_update_mtt(struct mlx5_ib_mr
 	}
 	pages_iter = size / sizeof(u64);
 	dma = dma_map_single(ddev, pas, size, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(mr->dma)) {
+#endif
 		mlx5_ib_err(dev, "unable to map DMA during MTT update.\n");
 		err = -ENOMEM;
 		goto free_pas;
@@ -1265,7 +1275,11 @@ static struct mlx5_ib_mr *reg_klm(struct
 	}
 
 	dma = dma_map_single(ddev, dptr, dsize, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(dma)) {
+#endif
 		err = -ENOMEM;
 		mlx5_ib_warn(dev, "dma map failed\n");
 		goto out;
@@ -1392,17 +1406,26 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 	int ncont;
 	int order;
 	int err;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct ib_peer_memory_client *ib_peer_mem;
+#endif
 
 	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 		    start, virt_addr, length, access_flags);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	umem = ib_umem_get_ex(pd->uobject->context, start, length, access_flags,
 			      0, 1);
+#else
+	umem = ib_umem_get(pd->uobject->context, start, length, access_flags,
+			   0);
+#endif
 	if (IS_ERR(umem)) {
 		mlx5_ib_dbg(dev, "umem get failed (%ld)\n", PTR_ERR(umem));
 		return (void *)umem;
 	}
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ib_peer_mem = umem->ib_peer_mem;
+#endif
 
 	mlx5_ib_cont_pages(umem, start, &npages, &page_shift, &ncont, &order);
 	if (!npages) {
@@ -1454,6 +1477,7 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 	atomic_add(npages, &dev->mdev->priv.reg_pages);
 	mr->ibmr.lkey = mr->mmr.key;
 	mr->ibmr.rkey = mr->mmr.key;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	atomic_set(&mr->invalidated, 0);
 
 	if (ib_peer_mem) {
@@ -1461,6 +1485,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 		ib_umem_activate_invalidation_notifier(umem,
 					mlx5_invalidate_umem, mr);
 	}
+#endif
+
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (umem->odp_data) {
 		/*
@@ -1576,6 +1602,7 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void mlx5_invalidate_umem(void *invalidation_cookie,
 				 struct ib_umem *umem,
 				 unsigned long addr, size_t size)
@@ -1596,6 +1623,7 @@ out:
 
 
 }
+#endif
 
 static int create_mr_sig(struct ib_pd *pd,
 			 struct ib_mr_init_attr *mr_init_attr,
@@ -1926,7 +1954,11 @@ static ssize_t limit_store(struct cache_
 	u32 var;
 	int err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var > ent->size)
@@ -1963,7 +1995,11 @@ static ssize_t miss_store(struct cache_o
 	struct mlx5_cache_ent *ent = &cache->ent[co->index];
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var != 0)
@@ -1995,7 +2031,11 @@ static ssize_t size_store(struct cache_o
 	u32 var;
 	int err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var < ent->limit)
@@ -2007,7 +2047,11 @@ static ssize_t size_store(struct cache_o
 			if (err && err != -EAGAIN)
 				return err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 			usleep_range(3000, 5000);
+#else
+			msleep(4);
+#endif
 		} while (err);
 	} else if (var < ent->size) {
 		remove_keys(dev, co->index, ent->size - var);
@@ -2098,7 +2142,11 @@ static ssize_t rel_imm_store(struct mlx5
 	int i;
 	int found = 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var > 1)
@@ -2136,7 +2184,11 @@ static ssize_t rel_timeout_store(struct
 	int var;
 	int i;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtoint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%d", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var < -1 || var > MAX_MR_RELEASE_TIMEOUT)
@@ -2213,7 +2265,11 @@ static struct kobj_type cache_type = {
 static int mlx5_mr_sysfs_init(struct mlx5_ib_dev *dev)
 {
 	struct mlx5_mr_cache *cache = &dev->cache;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *device = &dev->ib_dev.dev;
+#else
+	struct class_device *device = &dev->ib_dev.class_dev;
+#endif
 	struct cache_order *co;
 	int o;
 	int i;
@@ -2242,9 +2298,18 @@ static int mlx5_mr_sysfs_init(struct mlx
 err_put:
 	for (; i >= 0; i--) {
 		co = &cache->ent[i].co;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(&co->kobj);
+#else
+		kobject_unregister(&co->kobj);
+#endif
 	}
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_put(&dev->mr_cache);
+#else
+	kobject_unregister(&dev->mr_cache);
+#endif
 
 	return err;
 }
@@ -2257,9 +2322,17 @@ static void mlx5_mr_sysfs_cleanup(struct
 
 	for (i = MAX_MR_CACHE_ENTRIES - 1; i >= 0; i--) {
 		co = &cache->ent[i].co;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(&co->kobj);
+#else
+		kobject_unregister(&co->kobj);
+#endif
 	}
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_put(&dev->mr_cache);
+#else
+	kobject_unregister(&dev->mr_cache);
+#endif
 }
 
 int mlx5_ib_exp_query_mkey(struct ib_mr *mr, u64 mkey_attr_mask,
@@ -2296,6 +2369,7 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 #ifdef ARCH_KMALLOC_MINALIGN
 	dsize += max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);
 #else
+#define CRYPTO_MINALIGN __alignof__(unsigned long long)
 	dsize += max_t(int, MLX5_UMR_ALIGN - CRYPTO_MINALIGN, 0);
 #endif
 	mirl->mapped_ilist = kzalloc(dsize, GFP_KERNEL);
@@ -2308,7 +2382,11 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 			      MLX5_UMR_ALIGN);
 	mirl->map = dma_map_single(ddev, mirl->klms,
 				   dsize, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, mirl->map)) {
+#else
+	if (dma_mapping_error(mirl->map)) {
+#endif
 		err = -ENOMEM;
 		goto err_dma_map;
 	}
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -135,6 +135,7 @@ void *mlx5_get_send_wqe(struct mlx5_ib_q
  *
  * Return: the number of bytes copied, or an error code.
  */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 			  void *buffer, u32 length)
 {
@@ -188,6 +189,7 @@ int mlx5_ib_read_user_wqe(struct mlx5_ib
 
 	return wqe_length;
 }
+#endif
 
 static int
 query_wqe_idx(struct mlx5_ib_qp *qp)
@@ -2000,6 +2002,7 @@ static struct mlx5_ib_pd *get_pd(struct
 	return to_mpd(qp->ibqp.pd);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void destroy_raw_qp_rules(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
 {
 	struct mlx5_ib_fs_mc_flow *flow_iter;
@@ -2017,10 +2020,13 @@ static void destroy_raw_qp_rules(struct
 	}
 	mutex_unlock(&qp->mc_flows_list.lock);
 }
+#endif
 
 static void destroy_raw_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	destroy_raw_qp_rules(dev, qp);
+#endif
 
 	if (qp->rq.wqe_cnt) {
 		destroy_raw_qp_tir(dev, qp);
@@ -3801,7 +3807,11 @@ static void dump_wqe(struct mlx5_ib_qp *
 static void mlx5_bf_copy(u64 __iomem *dst, u64 *src,
 			 unsigned bytecnt, struct mlx5_ib_qp *qp)
 {
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	int i;
+#endif
 	while (bytecnt > 0) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
@@ -3810,6 +3820,13 @@ static void mlx5_bf_copy(u64 __iomem *ds
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
+#else
+		i = 64;
+		while (i > 0) {
+			*dst++=*src++;
+			i--;
+		}
+#endif
 		bytecnt -= 64;
 		if (unlikely(src == qp->sq.qend))
 			src = mlx5_get_send_wqe(qp, 0);
--- a/drivers/infiniband/hw/mlx5/roce.c
+++ b/drivers/infiniband/hw/mlx5/roce.c
@@ -208,9 +208,11 @@ int mlx5_query_port_roce(struct ib_devic
 	props->state            = IB_PORT_DOWN;
 	props->phys_state       = 3;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_query_nic_vport_qkey_viol_cntr(dev->mdev,
 						(u16 *)&props->qkey_viol_cntr))
 		pr_warn("%s failed to query qkey violations counter\n", __func__);
+#endif
 
 
 	if (!netdev)
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@ -4,6 +4,4 @@ obj-$(CONFIG_MLX5_CORE)		+= mlx5_core.o
 
 mlx5_core-y :=	main.o cmd.o debugfs.o fw.o eq.o uar.o pagealloc.o \
 		health.o mcg.o cq.o srq.o alloc.o qp.o port.o mr.o pd.o   \
-		mad.o wq.o vport.o transobj.o en_main.o \
-		en_flow_table.o en_ethtool.o en_tx.o en_rx.o en_txrx.o \
-		sriov.o params.o en_selftest.o fs_cmd.o fs_tree.o en_flow_table.o
+		mad.o wq.o transobj.o params.o
--- a/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
@@ -58,7 +58,11 @@ static void *mlx5_dma_zalloc_coherent_no
 	mutex_lock(&priv->alloc_mutex);
 	original_node = dev_to_node(&dev->pdev->dev);
 	set_dev_node(&dev->pdev->dev, node);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	cpu_handle = dma_zalloc_coherent(&dev->pdev->dev, size,
+#else
+	cpu_handle = dma_alloc_coherent(&dev->pdev->dev, size,
+#endif
 					 dma_handle, GFP_KERNEL);
 	set_dev_node(&dev->pdev->dev, original_node);
 	mutex_unlock(&priv->alloc_mutex);
@@ -86,6 +90,10 @@ int mlx5_buf_alloc_node(struct mlx5_core
 			--buf->page_shift;
 			buf->npages *= 2;
 		}
+
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+		memset(buf->direct.buf, 0, size);
+#endif
 	} else {
 		int i;
 
@@ -107,6 +115,9 @@ int mlx5_buf_alloc_node(struct mlx5_core
 				goto err_free;
 
 			buf->page_list[i].map = t;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+			memset(buf->page_list[i].buf, 0, PAGE_SIZE);
+#endif
 		}
 
 		if (BITS_PER_LONG == 64) {
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -30,7 +30,9 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#endif
 #include <linux/module.h>
 #include <linux/errno.h>
 #include <linux/pci.h>
@@ -38,7 +40,9 @@
 #include <linux/slab.h>
 #include <linux/delay.h>
 #include <linux/random.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/mlx5/driver.h>
 #include <linux/debugfs.h>
 #include <linux/sysfs.h>
@@ -1037,7 +1041,11 @@ static ssize_t dbg_write(struct file *fi
 
 static const struct file_operations fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= dbg_write,
 };
 #endif
@@ -1257,7 +1265,11 @@ static ssize_t data_read(struct file *fi
 
 static const struct file_operations dfops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= data_write,
 	.read	= data_read,
 };
@@ -1325,7 +1337,11 @@ static ssize_t outlen_write(struct file
 
 static const struct file_operations olfops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= outlen_write,
 	.read	= outlen_read,
 };
--- a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
@@ -184,6 +184,10 @@ static ssize_t average_read(struct file
 {
 	struct mlx5_cmd_stats *stats;
 	u64 field = 0;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	u64 dividend;
+	u32 divisor;
+#endif
 	int ret;
 	char tbuf[22];
 
@@ -193,7 +197,14 @@ static ssize_t average_read(struct file
 	stats = filp->private_data;
 	spin_lock_irq(&stats->lock);
 	if (stats->n)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		field = div64_u64(stats->sum, stats->n);
+#else
+		dividend = stats->sum;
+		divisor = stats->n;
+		do_div(dividend, divisor);
+		field = dividend;
+#endif
 	spin_unlock_irq(&stats->lock);
 	ret = snprintf(tbuf, sizeof(tbuf), "%llu\n", field);
 	if (ret > 0) {
@@ -224,7 +235,11 @@ static ssize_t average_write(struct file
 
 static const struct file_operations stats_fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.read	= average_read,
 	.write	= average_write,
 };
@@ -565,7 +580,11 @@ static ssize_t dbg_read(struct file *fil
 
 static const struct file_operations fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.read	= dbg_read,
 };
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -30,14 +30,18 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#endif
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/errno.h>
 #include <linux/pci.h>
 #include <linux/dma-mapping.h>
 #include <linux/slab.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/interrupt.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cq.h>
@@ -157,6 +161,7 @@ static struct mlx5_profile profile[] = {
 			.size	= 64,
 			.limit	= 32
 		},
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		.mr_cache[13]	= {
 			.size	= 32,
 			.limit	= 16
@@ -165,6 +170,7 @@ static struct mlx5_profile profile[] = {
 			.size	= 16,
 			.limit	= 8
 		},
+#endif
 	},
 };
 
@@ -280,7 +286,7 @@ static int mlx5_enable_msix(struct mlx5_
 	int nvec;
 #ifndef HAVE_PCI_ENABLE_MSIX_RANGE
 	int err;
-#endif 
+#endif
 	int i;
 
 	nvec = MLX5_CAP_GEN(dev, num_ports) *
@@ -576,6 +582,25 @@ int mlx5_core_disable_hca(struct mlx5_co
 	return mlx5_cmd_exec_check_status(dev, in, sizeof(in), out, sizeof(out));
 }
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static void compat_pci_set_master(struct pci_dev *dev, bool enable)
+{
+	u16 old_cmd, cmd;
+
+	pci_read_config_word(dev, PCI_COMMAND, &old_cmd);
+	if (enable)
+		cmd = old_cmd | PCI_COMMAND_MASTER;
+	else
+		cmd = old_cmd & ~PCI_COMMAND_MASTER;
+	if (cmd != old_cmd) {
+		dev_dbg(&dev->dev, "%s bus mastering\n",
+			enable ? "enabling" : "disabling");
+		pci_write_config_word(dev, PCI_COMMAND, cmd);
+	}
+	dev->is_busmaster = enable;
+}
+#endif
+
 static int mlx5_core_set_issi(struct mlx5_core_dev *dev)
 {
 	u32 query_in[MLX5_ST_SZ_DW(query_issi_in)];
@@ -983,6 +1008,8 @@ static int mlx5_pci_init(struct mlx5_cor
 err_clr_master:
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#else
+	compat_pci_set_master(dev->pdev, true);
 #endif
 	release_bar(dev->pdev);
 err_disable:
@@ -998,6 +1025,8 @@ static void mlx5_pci_close(struct mlx5_c
 	iounmap(dev->iseg);
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#else
+	compat_pci_set_master(dev->pdev, true);
 #endif
 	release_bar(dev->pdev);
 	mlx5_pci_disable_device(dev);
@@ -1245,12 +1274,13 @@ static int mlx5_load_one(struct mlx5_cor
 	mlx5_init_srq_table(dev);
 	mlx5_init_mr_table(dev);
 	mlx5_init_dct_table(dev);
-
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	err = mlx5_init_fs(dev);
 	if (err) {
 		mlx5_core_err(dev, "flow steering init %d\n", err);
 		goto err_reg_dev;
 	}
+#endif
 #if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	err = mlx5_eswitch_init(dev);
 	if (err) {
@@ -1290,7 +1320,9 @@ err_eswitch:
 	mlx5_eswitch_cleanup(dev);
 err_fs:
 #endif
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5_cleanup_fs(dev);
+#endif
 err_reg_dev:
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
@@ -1366,7 +1398,9 @@ static int mlx5_unload_one(struct mlx5_c
 #if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 	mlx5_eswitch_cleanup(dev);
 #endif
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5_cleanup_fs(dev);
+#endif
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
 	mlx5_cleanup_srq_table(dev);
@@ -1806,7 +1840,9 @@ static int __init init(void)
 	if (err)
 		goto err_debug;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5e_init();
+#endif
 
 	return 0;
 
@@ -1819,7 +1855,9 @@ err_debug:
 
 static void __exit cleanup(void)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5e_cleanup();
+#endif
 	pci_unregister_driver(&mlx5_core_driver);
 #ifndef HAVE_NO_DEBUGFS
 	mlx5_unregister_debugfs();
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -141,4 +141,13 @@ int mlx5_eswitch_get_vport_config(struct
 void mlx5e_init(void);
 void mlx5e_cleanup(void);
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static inline int compat_mlx5_simple_open(struct inode *inode, struct file *file)
+{
+	file->private_data = inode->i_private;
+
+	return 0;
+}
+#endif
+
 #endif /* __MLX5_CORE_H__ */
--- a/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
@@ -30,7 +30,11 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#else
+#include <linux/vmalloc.h>
+#endif
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/mlx5/driver.h>
--- a/drivers/net/ethernet/mellanox/mlx5/core/uar.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/uar.c
@@ -32,7 +32,9 @@
 
 #include <linux/kernel.h>
 #include <linux/module.h>
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 #include <linux/io-mapping.h>
+#endif
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cmd.h>
 #include "mlx5_core.h"
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -672,7 +672,11 @@ struct mlx5_core_dev {
 	u32 *hca_caps_cur[MLX5_CAP_NUM];
 	u32 *hca_caps_max[MLX5_CAP_NUM];
 #endif
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	phys_addr_t		iseg_base;
+#else
+	u64			iseg_base;
+#endif
 	struct mlx5_init_seg __iomem *iseg;
 	enum mlx5_device_state	state;
 	struct mutex		intf_state_mutex;
@@ -1116,7 +1120,11 @@ enum {
 };
 
 enum {
-	MAX_MR_CACHE_ENTRIES    = 15,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+       MAX_MR_CACHE_ENTRIES    = 15,
+#else
+	MAX_MR_CACHE_ENTRIES    = 13,
+#endif
 };
 
 enum {
