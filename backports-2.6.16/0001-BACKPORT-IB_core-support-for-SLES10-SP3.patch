From: Yishai Hadas <yishaih@mellanox.com>
Subject: [PATCH] BACKPORT: IB_core support for SLES10 SP3

Change-Id: I2428f9d6edf65f96b55735be6794f560fe3b17f9
Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/infiniband/core/Makefile         |    4 +-
 drivers/infiniband/core/addr.c           |   80 ++++++++
 drivers/infiniband/core/cm.c             |   33 +++
 drivers/infiniband/core/cma.c            |   78 +++++++-
 drivers/infiniband/core/cma_configfs.c   |    4 +
 drivers/infiniband/core/device.c         |   12 ++
 drivers/infiniband/core/iwpm_msg.c       |   24 +++
 drivers/infiniband/core/mad.c            |  122 ++++++++++++
 drivers/infiniband/core/netlink.c        |    2 +
 drivers/infiniband/core/peer_mem.c       |    3 +-
 drivers/infiniband/core/roce_gid_cache.c |    2 +-
 drivers/infiniband/core/roce_gid_mgmt.c  |    2 +
 drivers/infiniband/core/sysfs.c          |  144 +++++++++++++-
 drivers/infiniband/core/ucm.c            |  126 ++++++++++++
 drivers/infiniband/core/ucma.c           |   52 +++++
 drivers/infiniband/core/umem.c           |  316 +++++++++++++++++++++++++++++-
 drivers/infiniband/core/user_mad.c       |  275 ++++++++++++++++++++++++++-
 drivers/infiniband/core/uverbs.h         |    7 +
 drivers/infiniband/core/uverbs_cmd.c     |    4 +
 drivers/infiniband/core/uverbs_main.c    |  229 +++++++++++++++++++++-
 drivers/infiniband/core/verbs.c          |   24 +++-
 include/linux/mlx4/cmd.h                 |    2 +
 include/rdma/ib_addr.h                   |   24 +++
 include/rdma/ib_cm.h                     |   14 ++
 include/rdma/ib_pma.h                    |   12 ++
 include/rdma/ib_smi.h                    |   20 ++
 include/rdma/ib_umem.h                   |   28 +++-
 include/rdma/ib_verbs.h                  |    7 +-
 include/rdma/peer_mem.h                  |    3 +
 29 files changed, 1639 insertions(+), 14 deletions(-)

diff --git a/drivers/infiniband/core/Makefile b/drivers/infiniband/core/Makefile
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/Makefile
+++ b/drivers/infiniband/core/Makefile
@@ -4,7 +4,7 @@ infiniband-$(CONFIG_INFINIBAND_ADDR_TRANS)	:= rdma_cm.o
 user_access-$(CONFIG_INFINIBAND_ADDR_TRANS)	:= rdma_ucm.o
 
 obj-$(CONFIG_INFINIBAND) +=		ib_core.o ib_mad.o ib_sa.o \
-					ib_cm.o iw_cm.o ib_addr.o \
+					ib_cm.o ib_addr.o \
 					$(infiniband-y)
 obj-$(CONFIG_INFINIBAND_USER_MAD) +=	ib_umad.o
 obj-$(CONFIG_INFINIBAND_USER_ACCESS) +=	ib_uverbs.o ib_ucm.o \
@@ -22,8 +22,6 @@ ib_sa-y :=			sa_query.o multicast.o
 
 ib_cm-y :=			cm.o
 
-iw_cm-y :=			iwcm.o iwpm_util.o iwpm_msg.o
-
 rdma_cm-y :=			cma.o cma_configfs.o
 
 rdma_ucm-y :=			ucma.o
diff --git a/drivers/infiniband/core/addr.c b/drivers/infiniband/core/addr.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/addr.c
+++ b/drivers/infiniband/core/addr.c
@@ -86,7 +86,9 @@ int rdma_addr_size(struct sockaddr *addr)
 }
 EXPORT_SYMBOL(rdma_addr_size);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 static struct rdma_addr_client self;
+#endif
 
 void rdma_addr_register_client(struct rdma_addr_client *client)
 {
@@ -128,7 +130,11 @@ int rdma_translate_ip(struct sockaddr *addr, struct rdma_dev_addr *dev_addr,
 	int ret = -EADDRNOTAVAIL;
 
 	if (dev_addr->bound_dev_if) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev = dev_get_by_index(&init_net, dev_addr->bound_dev_if);
+#else
+		dev = dev_get_by_index(dev_addr->bound_dev_if);
+#endif
 		if (!dev)
 			return -ENODEV;
 		ret = rdma_copy_addr(dev_addr, dev, NULL);
@@ -152,6 +158,7 @@ int rdma_translate_ip(struct sockaddr *addr, struct rdma_dev_addr *dev_addr,
 
 #if IS_ENABLED(CONFIG_IPV6)
 	case AF_INET6:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		rcu_read_lock();
 		for_each_netdev_rcu(&init_net, dev) {
 			if (ipv6_chk_addr(&init_net,
@@ -164,6 +171,17 @@ int rdma_translate_ip(struct sockaddr *addr, struct rdma_dev_addr *dev_addr,
 			}
 		}
 		rcu_read_unlock();
+#else
+		read_lock(&dev_base_lock);
+		for (dev = dev_base; dev; dev = dev->next) {
+			if (ipv6_chk_addr(&((struct sockaddr_in6 *) addr)->sin6_addr,
+					  dev, 1)) {
+				ret = rdma_copy_addr(dev_addr, dev, NULL);
+				break;
+			}
+		}
+		read_unlock(&dev_base_lock);
+#endif
 		break;
 #endif
 	}
@@ -175,11 +193,18 @@ static void set_timeout(unsigned long time)
 {
 	unsigned long delay;
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	cancel_delayed_work(&work);
+#endif
 	delay = time - jiffies;
 	if ((long)delay < 0)
 		delay = 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mod_delayed_work(addr_wq, &work, delay);
+#else
+	queue_delayed_work(addr_wq, &work, delay);
+#endif
 }
 
 static void queue_req(struct addr_req *req)
@@ -370,14 +395,22 @@ static int addr6_resolve(struct sockaddr_in6 *src_in,
 	fl6.saddr = src_in->sin6_addr;
 	fl6.flowi6_oif = addr->bound_dev_if;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 	dst = ip6_route_output(&init_net, NULL, &fl6);
+#else
+	dst = ip6_route_output(NULL, &fl6);
+#endif
 	if ((ret = dst->error))
 		goto put;
 
 	rt = (struct rt6_info *)dst;
 	if (ipv6_addr_any(&fl6.saddr)) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 		ret = ipv6_dev_get_saddr(&init_net, ip6_dst_idev(dst)->dev,
 					 &fl6.daddr, 0, &fl6.saddr);
+#else
+		ret = ipv6_get_saddr(dst, &fl6.fl6_dst, &fl6.fl6_src);
+#endif
 		if (ret)
 			goto put;
 
@@ -390,14 +423,22 @@ static int addr6_resolve(struct sockaddr_in6 *src_in,
 	ipv6_addr_copy(&fl.fl6_src, &src_in->sin6_addr);
 	fl.oif = addr->bound_dev_if;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 	dst = ip6_route_output(&init_net, NULL, &fl);
+#else
+	dst = ip6_route_output(NULL, &fl);
+#endif
 	if ((ret = dst->error))
 		goto put;
 
 	rt = (struct rt6_info *)dst;
 	if (ipv6_addr_any(&fl.fl6_src)) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 		ret = ipv6_dev_get_saddr(&init_net, ip6_dst_idev(dst)->dev,
 					 &fl.fl6_dst, 0, &fl.fl6_src);
+#else
+		ret = ipv6_get_saddr(dst, &fl.fl6_dst, &fl.fl6_src);
+#endif
 		if (ret)
 			goto put;
 
@@ -577,6 +618,7 @@ void rdma_addr_cancel(struct rdma_dev_addr *addr)
 }
 EXPORT_SYMBOL(rdma_addr_cancel);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 struct resolve_cb_context {
 	struct rdma_dev_addr *addr;
 	struct completion comp;
@@ -694,6 +736,44 @@ static void __exit addr_cleanup(void)
 	unregister_netevent_notifier(&nb);
 	destroy_workqueue(addr_wq);
 }
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+static int addr_arp_recv(struct sk_buff *skb, struct net_device *dev,
+			 struct packet_type *pkt, struct net_device *orig_dev)
+{
+	struct arphdr *arp_hdr;
+
+	arp_hdr = (struct arphdr *) skb->nh.raw;
+
+	if (arp_hdr->ar_op == htons(ARPOP_REQUEST) ||
+	    arp_hdr->ar_op == htons(ARPOP_REPLY))
+		set_timeout(jiffies);
+
+	kfree_skb(skb);
+	return 0;
+}
+
+static struct packet_type addr_arp = {
+	.type           = __constant_htons(ETH_P_ARP),
+	.func           = addr_arp_recv,
+	.af_packet_priv = (void*) 1,
+};
+
+static int addr_init(void)
+{
+	addr_wq = create_singlethread_workqueue("ib_addr");
+	if (!addr_wq)
+		return -ENOMEM;
+
+	dev_add_pack(&addr_arp);
+	return 0;
+}
+
+static void addr_cleanup(void)
+{
+	dev_remove_pack(&addr_arp);
+	destroy_workqueue(addr_wq);
+}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 module_init(addr_init);
 module_exit(addr_cleanup);
diff --git a/drivers/infiniband/core/cm.c b/drivers/infiniband/core/cm.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/cm.c
+++ b/drivers/infiniband/core/cm.c
@@ -167,7 +167,11 @@ struct cm_port {
 struct cm_device {
 	struct list_head list;
 	struct ib_device *ib_device;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *device;
+#else
+	struct class_device *device;
+#endif
 	u8 ack_delay;
 	struct cm_port *port[0];
 };
@@ -3696,6 +3700,7 @@ static struct kobj_type cm_port_obj_type = {
 	.release = cm_release_port_obj
 };
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *cm_devnode(struct device *dev, umode_t *mode)
 #else
@@ -3706,11 +3711,16 @@ static char *cm_devnode(struct device *dev, mode_t *mode)
 		*mode = 0666;
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
+#endif
 
 struct class cm_class = {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 	.owner   = THIS_MODULE,
 	.name    = "infiniband_cm",
 	.devnode = cm_devnode,
+#else
+	.name    = "infiniband_cm",
+#endif
 };
 EXPORT_SYMBOL(cm_class);
 
@@ -3739,8 +3749,13 @@ static int cm_create_port_fs(struct cm_port *port)
 
 error:
 	while (i--)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 		kobject_put(&port->counter_group[i].obj);
 	kobject_put(&port->port_obj);
+#else
+		kobject_unregister(&port->counter_group[i].obj);
+	kobject_unregister(&port->port_obj);
+#endif
 	return ret;
 
 }
@@ -3750,9 +3765,15 @@ static void cm_remove_port_fs(struct cm_port *port)
 	int i;
 
 	for (i = 0; i < CM_COUNTER_GROUPS; i++)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 		kobject_put(&port->counter_group[i].obj);
 
 	kobject_put(&port->port_obj);
+#else
+		kobject_unregister(&port->counter_group[i].obj);
+
+	kobject_unregister(&port->port_obj);
+#endif
 }
 
 static void cm_add_one(struct ib_device *ib_device)
@@ -3781,7 +3802,11 @@ static void cm_add_one(struct ib_device *ib_device)
 	cm_dev->ib_device = ib_device;
 	cm_get_ack_delay(cm_dev);
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 	cm_dev->device = device_create(&cm_class, &ib_device->dev,
+#else
+	cm_dev->device = class_device_create(&cm_class, &ib_device->class_dev,
+#endif
 				       MKDEV(0, 0), NULL,
 				       "%s", ib_device->name);
 	if (IS_ERR(cm_dev->device)) {
@@ -3838,7 +3863,11 @@ error1:
 		ib_unregister_mad_agent(port->mad_agent);
 		cm_remove_port_fs(port);
 	}
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 	device_unregister(cm_dev->device);
+#else
+	class_device_unregister(cm_dev->device);
+#endif
 	kfree(cm_dev);
 }
 
@@ -3867,7 +3896,11 @@ static void cm_remove_one(struct ib_device *ib_device)
 		flush_workqueue(cm.wq);
 		cm_remove_port_fs(port);
 	}
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
 	device_unregister(cm_dev->device);
+#else
+	class_device_unregister(cm_dev->device);
+#endif
 	kfree(cm_dev);
 }
 
diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -513,7 +513,11 @@ static int cma_acquire_dev(struct rdma_id_private *id_priv,
 							 &iboe_gid,
 							 cma_dev->default_gid_type,
 							 port,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 							 &init_net,
+#else
+							 NULL,
+#endif
 							 if_index,
 							 NULL);
 		} else {
@@ -546,7 +550,11 @@ static int cma_acquire_dev(struct rdma_id_private *id_priv,
 									 &iboe_gid,
 									 cma_dev->default_gid_type,
 									 port,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 									 &init_net,
+#else
+									 NULL,
+#endif
 									 if_index,
 									 NULL);
 				} else {
@@ -765,7 +773,9 @@ static int cma_modify_qp_rtr(struct rdma_id_private *id_priv,
 {
 	struct ib_qp_attr qp_attr;
 	int qp_attr_mask, ret;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 	union ib_gid sgid;
+#endif
 
 	mutex_lock(&id_priv->qp_mutex);
 	if (!id_priv->id.qp) {
@@ -788,10 +798,12 @@ static int cma_modify_qp_rtr(struct rdma_id_private *id_priv,
 	if (ret)
 		goto out;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 	ret = ib_query_gid(id_priv->id.device, id_priv->id.port_num,
 			   qp_attr.ah_attr.grh.sgid_index, &sgid, NULL);
 	if (ret)
 		goto out;
+#endif
 
 	if (conn_param)
 		qp_attr.max_dest_rd_atomic = conn_param->responder_resources;
@@ -903,6 +915,7 @@ int rdma_init_qp_attr(struct rdma_cm_id *id, struct ib_qp_attr *qp_attr,
 		if (qp_attr->qp_state == IB_QPS_RTR)
 			qp_attr->rq_psn = id_priv->seq_num;
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		if (!id_priv->cm_id.iw) {
 			qp_attr->qp_access_flags = 0;
@@ -911,6 +924,7 @@ int rdma_init_qp_attr(struct rdma_cm_id *id, struct ib_qp_attr *qp_attr,
 			ret = iw_cm_init_qp_attr(id_priv->cm_id.iw, qp_attr,
 						 qp_attr_mask);
 		break;
+#endif
 	default:
 		ret = -ENOSYS;
 		break;
@@ -1200,8 +1214,12 @@ static void cma_leave_mc_groups(struct rdma_id_private *id_priv)
 				struct net_device *ndev = NULL;
 
 				if (dev_addr->bound_dev_if)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 					ndev = dev_get_by_index(&init_net,
 								dev_addr->bound_dev_if);
+#else
+					ndev = dev_get_by_index(dev_addr->bound_dev_if);
+#endif
 				if (ndev) {
 					cma_igmp_send(ndev,
 						      &mc->multicast.ib->rec.mgid,
@@ -1239,10 +1257,12 @@ void rdma_destroy_id(struct rdma_cm_id *id)
 			if (id_priv->cm_id.ib)
 				ib_destroy_cm_id(id_priv->cm_id.ib);
 			break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		case RDMA_TRANSPORT_IWARP:
 			if (id_priv->cm_id.iw)
 				iw_destroy_cm_id(id_priv->cm_id.iw);
 			break;
+#endif
 		default:
 			break;
 		}
@@ -1650,6 +1670,7 @@ static void cma_set_compare_data(enum rdma_port_space ps, struct sockaddr *addr,
 	}
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_iw_handler(struct iw_cm_id *iw_id, struct iw_cm_event *iw_event)
 {
 	struct rdma_id_private *id_priv = iw_id->context;
@@ -1800,6 +1821,7 @@ out:
 	mutex_unlock(&listen_id->handler_mutex);
 	return ret;
 }
+#endif
 
 static int cma_ib_listen(struct rdma_id_private *id_priv)
 {
@@ -1832,6 +1854,7 @@ static int cma_ib_listen(struct rdma_id_private *id_priv)
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_iw_listen(struct rdma_id_private *id_priv, int backlog)
 {
 	int ret;
@@ -1857,6 +1880,7 @@ static int cma_iw_listen(struct rdma_id_private *id_priv, int backlog)
 
 	return ret;
 }
+#endif
 
 static int cma_listen_handler(struct rdma_cm_id *id,
 			      struct rdma_cm_event *event)
@@ -2143,6 +2167,7 @@ err:
 }
 EXPORT_SYMBOL(rdma_set_ib_paths);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_resolve_iw_route(struct rdma_id_private *id_priv, int timeout_ms)
 {
 	struct cma_work *work;
@@ -2159,6 +2184,7 @@ static int cma_resolve_iw_route(struct rdma_id_private *id_priv, int timeout_ms)
 	queue_work(cma_wq, &work->work);
 	return 0;
 }
+#endif
 
 #if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK) && defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
 static int iboe_tos_to_sl(struct net_device *ndev, int tos)
@@ -2209,8 +2235,13 @@ static int cma_resolve_iboe_route(struct rdma_id_private *id_priv)
 	route->num_paths = 1;
 
 	if (addr->dev_addr.bound_dev_if) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 		ndev = dev_get_by_index(&init_net, addr->dev_addr.bound_dev_if);
 		route->path_rec->net = &init_net;
+#else
+		ndev = dev_get_by_index(addr->dev_addr.bound_dev_if);
+		route->path_rec->net = NULL;
+#endif
 		route->path_rec->ifindex = addr->dev_addr.bound_dev_if;
 		route->path_rec->gid_type = id_priv->gid_type;
 	}
@@ -2297,9 +2328,11 @@ int rdma_resolve_route(struct rdma_cm_id *id, int timeout_ms)
 			ret = -ENOSYS;
 		}
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		ret = cma_resolve_iw_route(id_priv, timeout_ms);
 		break;
+#endif
 	default:
 		ret = -ENOSYS;
 		break;
@@ -2910,11 +2943,13 @@ int rdma_listen(struct rdma_cm_id *id, int backlog)
 			if (ret)
 				goto err;
 			break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		case RDMA_TRANSPORT_IWARP:
 			ret = cma_iw_listen(id_priv, backlog);
 			if (ret)
 				goto err;
 			break;
+#endif
 		default:
 			ret = -ENOSYS;
 			goto err;
@@ -2958,6 +2993,7 @@ int rdma_bind_addr(struct rdma_cm_id *id, struct sockaddr *addr)
 			goto err1;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (!(id_priv->options & (1 << CMA_OPTION_AFONLY))) {
 		if (addr->sa_family == AF_INET)
 			id_priv->afonly = 1;
@@ -2966,6 +3002,7 @@ int rdma_bind_addr(struct rdma_cm_id *id, struct sockaddr *addr)
 			id_priv->afonly = init_net.ipv6.sysctl.bindv6only;
 #endif
 	}
+#endif
 	ret = cma_get_port(id_priv);
 	if (ret)
 		goto err2;
@@ -3219,6 +3256,7 @@ out:
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_connect_iw(struct rdma_id_private *id_priv,
 			  struct rdma_conn_param *conn_param)
 {
@@ -3259,6 +3297,7 @@ out:
 	}
 	return ret;
 }
+#endif
 
 int rdma_connect(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)
 {
@@ -3281,9 +3320,11 @@ int rdma_connect(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)
 		else
 			ret = cma_connect_ib(id_priv, conn_param);
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		ret = cma_connect_iw(id_priv, conn_param);
 		break;
+#endif
 	default:
 		ret = -ENOSYS;
 		break;
@@ -3330,6 +3371,7 @@ out:
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_accept_iw(struct rdma_id_private *id_priv,
 		  struct rdma_conn_param *conn_param)
 {
@@ -3351,6 +3393,7 @@ static int cma_accept_iw(struct rdma_id_private *id_priv,
 
 	return iw_cm_accept(id_priv->cm_id.iw, &iw_param);
 }
+#endif
 
 static int cma_send_sidr_rep(struct rdma_id_private *id_priv,
 			     enum ib_cm_sidr_status status, u32 qkey,
@@ -3410,9 +3453,11 @@ int rdma_accept(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)
 				ret = cma_rep_recv(id_priv);
 		}
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		ret = cma_accept_iw(id_priv, conn_param);
 		break;
+#endif
 	default:
 		ret = -ENOSYS;
 		break;
@@ -3472,10 +3517,12 @@ int rdma_reject(struct rdma_cm_id *id, const void *private_data,
 					     0, private_data, private_data_len);
 		}
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		ret = iw_cm_reject(id_priv->cm_id.iw,
 				   private_data, private_data_len);
 		break;
+#endif
 	default:
 		ret = -ENOSYS;
 		break;
@@ -3504,9 +3551,11 @@ int rdma_disconnect(struct rdma_cm_id *id)
 			ib_send_cm_drep(id_priv->cm_id.ib, NULL, 0);
 		}
 		break;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case RDMA_TRANSPORT_IWARP:
 		ret = iw_cm_disconnect(id_priv->cm_id.iw, 0);
 		break;
+#endif
 	default:
 		ret = -EINVAL;
 		break;
@@ -3579,7 +3628,11 @@ static void cma_set_mgid(struct rdma_id_private *id_priv,
 	} else if (addr->sa_family == AF_IB) {
 		memcpy(mgid, &((struct sockaddr_ib *) addr)->sib_addr, sizeof *mgid);
 	} else if ((addr->sa_family == AF_INET6)) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ipv6_ib_mc_map(&sin6->sin6_addr, dev_addr->broadcast, mc_map);
+#else
+		ipv6_ib_mc_map(&sin6->sin6_addr, mc_map);
+#endif
 		if (id_priv->id.ps == RDMA_PS_UDP)
 			mc_map[7] = 0x01;	/* Use RDMA CM signature */
 		*mgid = *(union ib_gid *) (mc_map + 4);
@@ -3632,11 +3685,18 @@ static int cma_join_ib_multicast(struct rdma_id_private *id_priv,
 						id_priv->id.port_num, &rec,
 						comp_mask, GFP_KERNEL,
 						cma_ib_mc_handler, mc);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #ifdef HAVE_PTR_ERR_OR_ZERO
 	return PTR_ERR_OR_ZERO(mc->multicast.ib);
 #else
 	return PTR_RET(mc->multicast.ib);
-#endif
+#endif /* HAVE_PTR_ERR_OR_ZERO */
+#else
+	if (IS_ERR(mc->multicast.ib))
+		return PTR_ERR(mc->multicast.ib);
+
+	return 0;
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 static void iboe_mcast_work_handler(struct work_struct *work)
@@ -3706,7 +3766,11 @@ static int cma_iboe_join_multicast(struct rdma_id_private *id_priv,
 		mc->multicast.ib->rec.qkey = cpu_to_be32(RDMA_UDP_QKEY);
 
 	if (dev_addr->bound_dev_if)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ndev = dev_get_by_index(&init_net, dev_addr->bound_dev_if);
+#else
+		ndev = dev_get_by_index(dev_addr->bound_dev_if);
+#endif
 	if (!ndev) {
 		err = -ENODEV;
 		goto out2;
@@ -3834,8 +3898,12 @@ void rdma_leave_multicast(struct rdma_cm_id *id, struct sockaddr *addr)
 						struct net_device *ndev = NULL;
 
 						if (dev_addr->bound_dev_if)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 							ndev = dev_get_by_index(&init_net,
 										dev_addr->bound_dev_if);
+#else
+							ndev = dev_get_by_index(dev_addr->bound_dev_if);
+#endif
 						if (ndev) {
 							cma_igmp_send(ndev,
 								      &mc->multicast.ib->rec.mgid,
@@ -3938,8 +4006,10 @@ static int cma_netdev_callback(struct notifier_block *self, unsigned long event,
 	struct rdma_id_private *id_priv;
 	int ret = NOTIFY_DONE;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dev_net(ndev) != &init_net)
 		return NOTIFY_DONE;
+#endif
 
 	if (event != NETDEV_BONDING_FAILOVER &&
 	    event != NETDEV_UNREGISTER)
@@ -4030,6 +4100,7 @@ static void cma_remove_one(struct ib_device *device)
 	kfree(cma_dev);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int cma_get_id_stats(struct sk_buff *skb, struct netlink_callback *cb)
 {
 	struct nlmsghdr *nlh;
@@ -4108,6 +4179,7 @@ static const struct ibnl_client_cbs cma_cb_table[] = {
 	[RDMA_NL_RDMA_CM_ID_STATS] = { .dump = cma_get_id_stats,
 				       .module = THIS_MODULE },
 };
+#endif
 
 static int __init cma_init(void)
 {
@@ -4125,8 +4197,10 @@ static int __init cma_init(void)
 	if (ret)
 		goto err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (ibnl_add_client(RDMA_NL_RDMA_CM, RDMA_NL_RDMA_CM_NUM_OPS, cma_cb_table))
 		printk(KERN_WARNING "RDMA CMA: failed to add netlink callback\n");
+#endif
 	cma_configfs_init();
 
 	return 0;
@@ -4142,7 +4216,9 @@ err:
 static void __exit cma_cleanup(void)
 {
 	cma_configfs_exit();
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ibnl_remove_client(RDMA_NL_RDMA_CM);
+#endif
 	ib_unregister_client(&cma_client);
 	unregister_netdevice_notifier(&cma_nb);
 	rdma_addr_unregister_client(&addr_client);
diff --git a/drivers/infiniband/core/cma_configfs.c b/drivers/infiniband/core/cma_configfs.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/cma_configfs.c
+++ b/drivers/infiniband/core/cma_configfs.c
@@ -213,7 +213,11 @@ static struct configfs_subsystem cma_subsys = {
 int __init cma_configfs_init(void)
 {
 	config_group_init(&cma_subsys.su_group);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 	mutex_init(&cma_subsys.su_mutex);
+#else
+	init_MUTEX(&cma_subsys.su_sem);
+#endif
 	return configfs_register_subsystem(&cma_subsys);
 }
 
diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -38,7 +38,11 @@
 #include <linux/slab.h>
 #include <linux/init.h>
 #include <linux/mutex.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <rdma/rdma_netlink.h>
+#else
+#include <linux/workqueue.h>
+#endif
 #include <rdma/ib_addr.h>
 #include <rdma/ib_cache.h>
 
@@ -809,6 +813,7 @@ int ib_modify_port(struct ib_device *device,
 }
 EXPORT_SYMBOL(ib_modify_port);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 /**
  * ib_find_gid - Returns the port number and GID table index where
  *   a specified GID value occurs.
@@ -854,6 +859,7 @@ int ib_find_gid(struct ib_device *device, union ib_gid *gid,
 	return -ENOENT;
 }
 EXPORT_SYMBOL(ib_find_gid);
+#endif
 
 /**
  * ib_find_pkey - Returns the PKey table index where a specified
@@ -908,11 +914,13 @@ static int __init ib_core_init(void)
 		goto err;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ret = ibnl_init();
 	if (ret) {
 		printk(KERN_WARNING "Couldn't init IB netlink interface\n");
 		goto err_sysfs;
 	}
+#endif
 
 	roce_gid_cache_setup();
 
@@ -925,9 +933,11 @@ static int __init ib_core_init(void)
 	return 0;
 
 err_nl:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ibnl_cleanup();
 
 err_sysfs:
+#endif
 	ib_sysfs_cleanup();
 
 err:
@@ -939,7 +949,9 @@ static void __exit ib_core_cleanup(void)
 {
 	roce_gid_cache_cleanup();
 	ib_cache_cleanup();
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ibnl_cleanup();
+#endif
 	ib_sysfs_cleanup();
 	/* Make sure that any pending umem accounting work is done. */
 	destroy_workqueue(ib_wq);
diff --git a/drivers/infiniband/core/iwpm_msg.c b/drivers/infiniband/core/iwpm_msg.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/iwpm_msg.c
+++ b/drivers/infiniband/core/iwpm_msg.c
@@ -334,9 +334,17 @@ EXPORT_SYMBOL(iwpm_remove_mapping);
 static const struct nla_policy resp_reg_policy[IWPM_NLA_RREG_PID_MAX] = {
 	[IWPM_NLA_RREG_PID_SEQ]     = { .type = NLA_U32 },
 	[IWPM_NLA_RREG_IBDEV_NAME]  = { .type = NLA_STRING,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 					.len = IWPM_DEVNAME_SIZE - 1 },
+#else
+					.minlen = IWPM_DEVNAME_SIZE - 1 },
+#endif
 	[IWPM_NLA_RREG_ULIB_NAME]   = { .type = NLA_STRING,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 					.len = IWPM_ULIBNAME_SIZE - 1 },
+#else
+					.minlen = IWPM_ULIBNAME_SIZE - 1 },
+#endif
 	[IWPM_NLA_RREG_ULIB_VER]    = { .type = NLA_U16 },
 	[IWPM_NLA_RREG_PID_ERR]     = { .type = NLA_U16 }
 };
@@ -402,8 +410,13 @@ EXPORT_SYMBOL(iwpm_register_pid_cb);
 /* netlink attribute policy for the received response to add mapping request */
 static const struct nla_policy resp_add_policy[IWPM_NLA_RMANAGE_MAPPING_MAX] = {
 	[IWPM_NLA_MANAGE_MAPPING_SEQ]     = { .type = NLA_U32 },
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	[IWPM_NLA_MANAGE_ADDR]            = { .len = sizeof(struct sockaddr_storage) },
 	[IWPM_NLA_MANAGE_MAPPED_LOC_ADDR] = { .len = sizeof(struct sockaddr_storage) },
+#else
+	[IWPM_NLA_MANAGE_ADDR]            = { .minlen = sizeof(struct sockaddr_storage) },
+	[IWPM_NLA_MANAGE_MAPPED_LOC_ADDR] = { .minlen = sizeof(struct sockaddr_storage) },
+#endif
 	[IWPM_NLA_RMANAGE_MAPPING_ERR]	  = { .type = NLA_U16 }
 };
 
@@ -471,10 +484,17 @@ EXPORT_SYMBOL(iwpm_add_mapping_cb);
 /* netlink attribute policy for the response to add and query mapping request */
 static const struct nla_policy resp_query_policy[IWPM_NLA_RQUERY_MAPPING_MAX] = {
 	[IWPM_NLA_QUERY_MAPPING_SEQ]      = { .type = NLA_U32 },
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	[IWPM_NLA_QUERY_LOCAL_ADDR]       = { .len = sizeof(struct sockaddr_storage) },
 	[IWPM_NLA_QUERY_REMOTE_ADDR]      = { .len = sizeof(struct sockaddr_storage) },
 	[IWPM_NLA_RQUERY_MAPPED_LOC_ADDR] = { .len = sizeof(struct sockaddr_storage) },
 	[IWPM_NLA_RQUERY_MAPPED_REM_ADDR] = { .len = sizeof(struct sockaddr_storage) },
+#else
+	[IWPM_NLA_QUERY_LOCAL_ADDR]       = { .minlen = sizeof(struct sockaddr_storage) },
+	[IWPM_NLA_QUERY_REMOTE_ADDR]      = { .minlen = sizeof(struct sockaddr_storage) },
+	[IWPM_NLA_RQUERY_MAPPED_LOC_ADDR] = { .minlen = sizeof(struct sockaddr_storage) },
+	[IWPM_NLA_RQUERY_MAPPED_REM_ADDR] = { .minlen = sizeof(struct sockaddr_storage) },
+#endif
 	[IWPM_NLA_RQUERY_MAPPING_ERR]	  = { .type = NLA_U16 }
 };
 
@@ -562,7 +582,11 @@ EXPORT_SYMBOL(iwpm_add_and_query_mapping_cb);
 /* netlink attribute policy for the received request for mapping info */
 static const struct nla_policy resp_mapinfo_policy[IWPM_NLA_MAPINFO_REQ_MAX] = {
 	[IWPM_NLA_MAPINFO_ULIB_NAME] = { .type = NLA_STRING,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 					.len = IWPM_ULIBNAME_SIZE - 1 },
+#else
+					.minlen = IWPM_ULIBNAME_SIZE - 1 },
+#endif
 	[IWPM_NLA_MAPINFO_ULIB_VER]  = { .type = NLA_U16 }
 };
 
diff --git a/drivers/infiniband/core/mad.c b/drivers/infiniband/core/mad.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -67,6 +67,7 @@ static struct kmem_cache *ib_mad_cache;
 static struct list_head ib_mad_port_list;
 static u32 ib_mad_client_id = 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 /*
  * Timeout FIFO (tf) param
  */
@@ -83,6 +84,7 @@ enum {
 	MIN_TIME_FOR_SA_MAD_SEND_MS = 20,
 	MAX_SA_MADS = 10000
 };
+#endif
 
 /* Port list lock */
 static DEFINE_SPINLOCK(ib_mad_port_list_lock);
@@ -104,6 +106,7 @@ static int add_nonoui_reg_req(struct ib_mad_reg_req *mad_reg_req,
 			      u8 mgmt_class);
 static int add_oui_reg_req(struct ib_mad_reg_req *mad_reg_req,
 			   struct ib_mad_agent_private *agent_priv);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static int send_sa_cc_mad(struct ib_mad_send_wr_private *mad_send_wr,
 			  u32 timeout_ms, u32 retries_left);
 
@@ -606,6 +609,7 @@ static void sa_cc_destroy(struct sa_cc_data *cc_obj)
 	}
 	tf_free(cc_obj->tf);
 }
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 /*
  * Returns a ib_mad_port_private structure or NULL for a device/port
@@ -868,11 +872,21 @@ struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,
 	}
 
 	if (mad_reg_req) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		reg_req = kmemdup(mad_reg_req, sizeof *reg_req, GFP_KERNEL);
 		if (!reg_req) {
 			ret = ERR_PTR(-ENOMEM);
 			goto error3;
 		}
+#else
+		reg_req = kmalloc(sizeof *reg_req, GFP_KERNEL);
+		if (!reg_req) {
+			ret = ERR_PTR(-ENOMEM);
+			goto error3;
+		}
+		/* Make a copy of the MAD registration request */
+		memcpy(reg_req, mad_reg_req, sizeof *reg_req);
+#endif
 	}
 
 	/* Now, fill in the various structures */
@@ -990,14 +1004,28 @@ static int register_snoop_agent(struct ib_mad_qp_info *qp_info,
 
 	if (i == qp_info->snoop_table_size) {
 		/* Grow table. */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		new_snoop_table = krealloc(qp_info->snoop_table,
 					   sizeof mad_snoop_priv *
 					   (qp_info->snoop_table_size + 1),
 					   GFP_ATOMIC);
+#else
+		new_snoop_table = kmalloc(sizeof mad_snoop_priv *
+					  (qp_info->snoop_table_size + 1),
+					  GFP_ATOMIC);
+#endif
 		if (!new_snoop_table) {
 			i = -ENOMEM;
 			goto out;
 		}
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+		if (qp_info->snoop_table) {
+			memcpy(new_snoop_table, qp_info->snoop_table,
+			       sizeof mad_snoop_priv *
+			       qp_info->snoop_table_size);
+			kfree(qp_info->snoop_table);
+		}
+#endif
 
 		qp_info->snoop_table = new_snoop_table;
 		qp_info->snoop_table_size++;
@@ -1643,6 +1671,7 @@ int ib_send_mad(struct ib_mad_send_wr_private *mad_send_wr)
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 /*
  * Send SA MAD that passed congestion control
  */
@@ -1681,6 +1710,7 @@ static int send_sa_cc_mad(struct ib_mad_send_wr_private *mad_send_wr,
 
 	return ret;
 }
+#endif
 
 /*
  * ib_post_send_mad - Posts MAD(s) to the send queue of the QP associated
@@ -1745,6 +1775,7 @@ int ib_post_send_mad(struct ib_mad_send_buf *send_buf,
 		mad_send_wr->refcount = 1 + (mad_send_wr->timeout > 0);
 		mad_send_wr->status = IB_WC_SUCCESS;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (is_sa_cc_mad(mad_send_wr)) {
 			mad_send_wr->is_sa_cc_mad = 1;
 			ret = sa_cc_mad_send(mad_send_wr);
@@ -1774,6 +1805,29 @@ int ib_post_send_mad(struct ib_mad_send_buf *send_buf,
 				goto error;
 			}
 		}
+#else
+		/* Reference MAD agent until send completes */
+		atomic_inc(&mad_agent_priv->refcount);
+		spin_lock_irqsave(&mad_agent_priv->lock, flags);
+		list_add_tail(&mad_send_wr->agent_list,
+			      &mad_agent_priv->send_list);
+		spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
+
+		if (mad_agent_priv->agent.rmpp_version) {
+			ret = ib_send_rmpp_mad(mad_send_wr);
+			if (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)
+				ret = ib_send_mad(mad_send_wr);
+		} else
+			ret = ib_send_mad(mad_send_wr);
+		if (ret < 0) {
+			/* Fail send request */
+			spin_lock_irqsave(&mad_agent_priv->lock, flags);
+			list_del(&mad_send_wr->agent_list);
+			spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
+			atomic_dec(&mad_agent_priv->refcount);
+			goto error;
+		}
+#endif
 	}
 	return 0;
 error:
@@ -1835,7 +1889,14 @@ static int method_in_use(struct ib_mad_mgmt_method_table **method,
 {
 	int i;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	for_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS) {
+#else
+	for (i = find_first_bit(mad_reg_req->method_mask, IB_MGMT_MAX_METHODS);
+	     i < IB_MGMT_MAX_METHODS;
+	     i = find_next_bit(mad_reg_req->method_mask, IB_MGMT_MAX_METHODS,
+			       1+i)) {
+#endif
 		if ((*method)->agent[i]) {
 			pr_err("Method %d already in use\n", i);
 			return -EINVAL;
@@ -1968,8 +2029,18 @@ static int add_nonoui_reg_req(struct ib_mad_reg_req *mad_reg_req,
 		goto error3;
 
 	/* Finally, add in methods being registered */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	for_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)
 		(*method)->agent[i] = agent_priv;
+#else
+	for (i = find_first_bit(mad_reg_req->method_mask,
+				IB_MGMT_MAX_METHODS);
+	     i < IB_MGMT_MAX_METHODS;
+	     i = find_next_bit(mad_reg_req->method_mask, IB_MGMT_MAX_METHODS,
+			       1+i)) {
+		(*method)->agent[i] = agent_priv;
+	}
+#endif
 
 	return 0;
 
@@ -2063,8 +2134,18 @@ check_in_use:
 		goto error4;
 
 	/* Finally, add in methods being registered */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	for_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)
 		(*method)->agent[i] = agent_priv;
+#else
+	for (i = find_first_bit(mad_reg_req->method_mask,
+				IB_MGMT_MAX_METHODS);
+	     i < IB_MGMT_MAX_METHODS;
+	     i = find_next_bit(mad_reg_req->method_mask, IB_MGMT_MAX_METHODS,
+			       1+i)) {
+		(*method)->agent[i] = agent_priv;
+	}
+#endif
 
 	return 0;
 
@@ -2664,11 +2745,20 @@ static void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv)
 		if (time_after(mad_agent_priv->timeout,
 			       mad_send_wr->timeout)) {
 			mad_agent_priv->timeout = mad_send_wr->timeout;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+			cancel_delayed_work(&mad_agent_priv->timed_work);
+#endif
 			delay = mad_send_wr->timeout - jiffies;
 			if ((long)delay <= 0)
 				delay = 1;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 			mod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,
 					 &mad_agent_priv->timed_work, delay);
+#else
+			queue_delayed_work(mad_agent_priv->qp_info->
+					   port_priv->wq,
+					   &mad_agent_priv->timed_work, delay);
+#endif
 		}
 	}
 }
@@ -2701,9 +2791,17 @@ static void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr)
 	list_add(&mad_send_wr->agent_list, list_item);
 
 	/* Reschedule a work item if we have a shorter timeout */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mad_agent_priv->wait_list.next == &mad_send_wr->agent_list)
 		mod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,
 				 &mad_agent_priv->timed_work, delay);
+#else
+	if (mad_agent_priv->wait_list.next == &mad_send_wr->agent_list) {
+		cancel_delayed_work(&mad_agent_priv->timed_work);
+		queue_delayed_work(mad_agent_priv->qp_info->port_priv->wq,
+				   &mad_agent_priv->timed_work, delay);
+	}
+#endif
 }
 
 void ib_reset_mad_timeout(struct ib_mad_send_wr_private *mad_send_wr,
@@ -2756,8 +2854,10 @@ void ib_mad_complete_send_wr(struct ib_mad_send_wr_private *mad_send_wr,
 	if (ret == IB_RMPP_RESULT_INTERNAL)
 		ib_rmpp_send_handler(mad_send_wc);
 	else {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (mad_send_wr->is_sa_cc_mad)
 			sa_cc_mad_done(get_cc_obj(mad_send_wr));
+#endif
 		mad_agent_priv->agent.send_handler(&mad_agent_priv->agent,
 						   mad_send_wc);
 	}
@@ -2951,7 +3051,9 @@ static void cancel_mads(struct ib_mad_agent_private *mad_agent_priv)
 
 	INIT_LIST_HEAD(&cancel_list);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	cancel_sa_cc_mads(mad_agent_priv);
+#endif
 	spin_lock_irqsave(&mad_agent_priv->lock, flags);
 	mad_agent_priv->send_list_closed = 1;
 	list_for_each_entry_safe(mad_send_wr, temp_mad_send_wr,
@@ -2974,8 +3076,10 @@ static void cancel_mads(struct ib_mad_agent_private *mad_agent_priv)
 				 &cancel_list, agent_list) {
 		mad_send_wc.send_buf = &mad_send_wr->send_buf;
 		list_del(&mad_send_wr->agent_list);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (mad_send_wr->is_sa_cc_mad)
 			sa_cc_mad_done(get_cc_obj(mad_send_wr));
+#endif
 		mad_agent_priv->agent.send_handler(&mad_agent_priv->agent,
 						   &mad_send_wc);
 		atomic_dec(&mad_agent_priv->refcount);
@@ -3015,6 +3119,7 @@ int ib_modify_mad(struct ib_mad_agent *mad_agent,
 				      agent);
 	spin_lock_irqsave(&mad_agent_priv->lock, flags);
 	mad_send_wr = find_send_wr(mad_agent_priv, send_buf);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (!mad_send_wr) {
 		spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
 		if (modify_sa_cc_mad(mad_agent_priv, send_buf, timeout_ms))
@@ -3025,6 +3130,12 @@ int ib_modify_mad(struct ib_mad_agent *mad_agent,
 		spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
 		return -EINVAL;
 	}
+#else
+	if (!mad_send_wr || mad_send_wr->status != IB_WC_SUCCESS) {
+		spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
+		return -EINVAL;
+	}
+#endif
 
 	active = (!mad_send_wr->timeout || mad_send_wr->refcount > 1);
 	if (!timeout_ms) {
@@ -3206,8 +3317,10 @@ static void timeout_sends(struct work_struct *work)
 		else
 			mad_send_wc.status = mad_send_wr->status;
 		mad_send_wc.send_buf = &mad_send_wr->send_buf;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (mad_send_wr->is_sa_cc_mad)
 			sa_cc_mad_done(get_cc_obj(mad_send_wr));
+#endif
 		mad_agent_priv->agent.send_handler(&mad_agent_priv->agent,
 						   &mad_send_wc);
 
@@ -3575,8 +3688,10 @@ static int ib_mad_port_open(struct ib_device *device,
 	}
 	INIT_WORK(&port_priv->work, ib_mad_completion_handler);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (sa_cc_init(&port_priv->sa_cc))
 		goto error9;
+#endif
 
 	spin_lock_irqsave(&ib_mad_port_list_lock, flags);
 	list_add_tail(&port_priv->port_list, &ib_mad_port_list);
@@ -3595,8 +3710,10 @@ error10:
 	list_del_init(&port_priv->port_list);
 	spin_unlock_irqrestore(&ib_mad_port_list_lock, flags);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	sa_cc_destroy(&port_priv->sa_cc);
 error9:
+#endif
 	destroy_workqueue(port_priv->wq);
 error8:
 	destroy_mad_qp(&port_priv->qp_info[1]);
@@ -3637,7 +3754,9 @@ static int ib_mad_port_close(struct ib_device *device, int port_num)
 	spin_unlock_irqrestore(&ib_mad_port_list_lock, flags);
 
 	destroy_workqueue(port_priv->wq);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	sa_cc_destroy(&port_priv->sa_cc);
+#endif
 	destroy_mad_qp(&port_priv->qp_info[1]);
 	destroy_mad_qp(&port_priv->qp_info[0]);
 	ib_dereg_mr(port_priv->mr);
@@ -3738,6 +3857,9 @@ static int __init ib_mad_init_module(void)
 	mad_sendq_size = min(mad_sendq_size, IB_MAD_QP_MAX_SIZE);
 	mad_sendq_size = max(mad_sendq_size, IB_MAD_QP_MIN_SIZE);
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	spin_lock_init(&ib_mad_port_list_lock);
+#endif
 	ib_mad_cache = kmem_cache_create("ib_mad",
 					 sizeof(struct ib_mad_private),
 					 0,
diff --git a/drivers/infiniband/core/netlink.c b/drivers/infiniband/core/netlink.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/netlink.c
+++ b/drivers/infiniband/core/netlink.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #ifdef pr_fmt
 #undef pr_fmt
 #endif
@@ -237,3 +238,4 @@ void ibnl_cleanup(void)
 
 	netlink_kernel_release(nls);
 }
+#endif
diff --git a/drivers/infiniband/core/peer_mem.c b/drivers/infiniband/core/peer_mem.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/peer_mem.c
+++ b/drivers/infiniband/core/peer_mem.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <rdma/ib_peer_mem.h>
 #include <rdma/ib_verbs.h>
 #include <rdma/ib_umem.h>
@@ -417,4 +418,4 @@ void ib_put_peer_client(struct ib_peer_memory_client *ib_peer_client,
 	return;
 }
 EXPORT_SYMBOL(ib_put_peer_client);
-
+#endif
diff --git a/drivers/infiniband/core/roce_gid_cache.c b/drivers/infiniband/core/roce_gid_cache.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/roce_gid_cache.c
+++ b/drivers/infiniband/core/roce_gid_cache.c
@@ -442,7 +442,7 @@ static int get_netdev_from_ifindex(struct net *net, int if_index,
 		gid_attr_val->ndev = dev_get_by_index_rcu(net, if_index);
 #else
 		gid_attr_val->ndev = __dev_get_by_index(net, if_index);
-#endif
+#endif /* HAVE_DEV_GET_BY_INDEX_RCU */
 		rcu_read_unlock();
 		if (gid_attr_val->ndev)
 			return GID_ATTR_FIND_MASK_NETDEV;
diff --git a/drivers/infiniband/core/roce_gid_mgmt.c b/drivers/infiniband/core/roce_gid_mgmt.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/roce_gid_mgmt.c
+++ b/drivers/infiniband/core/roce_gid_mgmt.c
@@ -712,6 +712,7 @@ static void enum_all_gids_of_dev_cb(struct ib_device *ib_dev,
 				    struct net_device *idev,
 				    void *cookie)
 {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 	struct net *net;
 	struct net_device *ndev;
 
@@ -724,6 +725,7 @@ static void enum_all_gids_of_dev_cb(struct ib_device *ib_dev,
 			if (is_eth_port_of_netdev(ib_dev, port, idev, ndev))
 				add_netdev_ips(ib_dev, port, idev, ndev);
 	rtnl_unlock();
+#endif
 }
 
 /* This function will rescan all of the network devices in the system
diff --git a/drivers/infiniband/core/sysfs.c b/drivers/infiniband/core/sysfs.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/sysfs.c
+++ b/drivers/infiniband/core/sysfs.c
@@ -347,7 +347,19 @@ static ssize_t show_port_gid(struct ib_port *p, struct port_attribute *attr,
 	if (ret)
 		return ret;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	return sprintf(buf, "%pI6\n", gid.raw);
+#else
+	return sprintf(buf, "%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n",
+		       be16_to_cpu(((__be16 *) gid.raw)[0]),
+		       be16_to_cpu(((__be16 *) gid.raw)[1]),
+		       be16_to_cpu(((__be16 *) gid.raw)[2]),
+		       be16_to_cpu(((__be16 *) gid.raw)[3]),
+		       be16_to_cpu(((__be16 *) gid.raw)[4]),
+		       be16_to_cpu(((__be16 *) gid.raw)[5]),
+		       be16_to_cpu(((__be16 *) gid.raw)[6]),
+		       be16_to_cpu(((__be16 *) gid.raw)[7]));
+#endif
 }
 
 static ssize_t show_port_gid_attr_ndev(struct ib_port *p,
@@ -600,6 +612,7 @@ static struct kobj_type gid_attr_type = {
 	.release	= ib_port_gid_attr_release
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void ib_device_release(struct device *device)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
@@ -621,6 +634,32 @@ static int ib_device_uevent(struct device *device,
 
 	return 0;
 }
+#else
+static void ib_device_release(struct class_device *cdev)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+
+	kfree(dev);
+}
+
+static int ib_device_uevent(struct class_device *cdev, char **envp,
+			    int num_envp, char *buf, int size)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+	int i = 0, len = 0;
+
+	if (add_uevent_var(envp, num_envp, &i, buf, size, &len,
+			   "NAME=%s", dev->name))
+		return -ENOMEM;
+
+	/*
+	 * It would be nice to pass the node GUID with the event...
+	 */
+
+	envp[i] = NULL;
+	return 0;
+}
+#endif
 
 static struct attribute **
 alloc_group_attrs(ssize_t (*show)(struct ib_port *,
@@ -651,7 +690,9 @@ alloc_group_attrs(ssize_t (*show)(struct ib_port *,
 		element->attr.attr.mode  = S_IRUGO;
 		element->attr.show       = show;
 		element->index		 = i;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		sysfs_attr_init(&element->attr.attr);
+#endif
 
 		tab_attr[i] = &element->attr.attr;
 	}
@@ -832,10 +873,16 @@ err_put:
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_node_type(struct device *device,
 			      struct device_attribute *attr, char *buf)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
+#else
+static ssize_t show_node_type(struct class_device *cdev, char *buf)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+#endif
 
 	switch (dev->node_type) {
 	case RDMA_NODE_IB_CA:	  return sprintf(buf, "%d: CA\n", dev->node_type);
@@ -848,10 +895,16 @@ static ssize_t show_node_type(struct device *device,
 	}
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_sys_image_guid(struct device *device,
 				   struct device_attribute *dev_attr, char *buf)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
+#else
+static ssize_t show_sys_image_guid(struct class_device *cdev, char *buf)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+#endif
 	struct ib_device_attr attr;
 	ssize_t ret;
 
@@ -866,10 +919,16 @@ static ssize_t show_sys_image_guid(struct device *device,
 		       be16_to_cpu(((__be16 *) &attr.sys_image_guid)[3]));
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_node_guid(struct device *device,
 			      struct device_attribute *attr, char *buf)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
+#else
+static ssize_t show_node_guid(struct class_device *cdev, char *buf)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+#endif
 
 	return sprintf(buf, "%04x:%04x:%04x:%04x\n",
 		       be16_to_cpu(((__be16 *) &dev->node_guid)[0]),
@@ -878,19 +937,32 @@ static ssize_t show_node_guid(struct device *device,
 		       be16_to_cpu(((__be16 *) &dev->node_guid)[3]));
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_node_desc(struct device *device,
 			      struct device_attribute *attr, char *buf)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
+#else
+static ssize_t show_node_desc(struct class_device *cdev, char *buf)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+#endif
 
 	return sprintf(buf, "%.64s\n", dev->node_desc);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t set_node_desc(struct device *device,
 			     struct device_attribute *attr,
 			     const char *buf, size_t count)
 {
 	struct ib_device *dev = container_of(device, struct ib_device, dev);
+#else
+static ssize_t set_node_desc(struct class_device *cdev, const char *buf,
+			      size_t count)
+{
+	struct ib_device *dev = container_of(cdev, struct ib_device, class_dev);
+#endif
 	struct ib_device_modify desc = {};
 	int ret;
 
@@ -905,6 +977,7 @@ static ssize_t set_node_desc(struct device *device,
 	return count;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static DEVICE_ATTR(node_type, S_IRUGO, show_node_type, NULL);
 static DEVICE_ATTR(sys_image_guid, S_IRUGO, show_sys_image_guid, NULL);
 static DEVICE_ATTR(node_guid, S_IRUGO, show_node_guid, NULL);
@@ -922,6 +995,25 @@ static struct class ib_class = {
 	.dev_release = ib_device_release,
 	.dev_uevent = ib_device_uevent,
 };
+#else
+static CLASS_DEVICE_ATTR(node_type, S_IRUGO, show_node_type, NULL);
+static CLASS_DEVICE_ATTR(sys_image_guid, S_IRUGO, show_sys_image_guid, NULL);
+static CLASS_DEVICE_ATTR(node_guid, S_IRUGO, show_node_guid, NULL);
+static CLASS_DEVICE_ATTR(node_desc, S_IRUGO | S_IWUSR, show_node_desc, set_node_desc);
+
+static struct class_device_attribute *ib_class_attributes[] = {
+	&class_device_attr_node_type,
+	&class_device_attr_sys_image_guid,
+	&class_device_attr_node_guid,
+	&class_device_attr_node_desc
+};
+
+static struct class ib_class = {
+	.name    = "infiniband",
+	.release = ib_device_release,
+	.uevent = ib_device_uevent,
+};
+#endif
 
 /* Show a given an attribute in the statistics group */
 static ssize_t show_protocol_stat(const struct device *device,
@@ -1052,17 +1144,27 @@ static void free_port_list_attributes(struct ib_device *device)
 				   &port->gid_attr_group->ndev);
 		sysfs_remove_group(&port->gid_attr_group->kobj,
 				   &port->gid_attr_group->type);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(&port->gid_attr_group->kobj);
 		kobject_put(p);
+#else
+		kobject_unregister(&port->gid_attr_group->kobj);
+		kobject_unregister(p);
+#endif
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_put(device->ports_parent);
+#else
+	kobject_unregister(device->ports_parent);
+#endif
 }
 
 int ib_device_register_sysfs(struct ib_device *device,
 			     int (*port_callback)(struct ib_device *,
 						  u8, struct kobject *))
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *class_dev = &device->dev;
 	int ret;
 	int i;
@@ -1083,6 +1185,28 @@ int ib_device_register_sysfs(struct ib_device *device,
 		if (ret)
 			goto err_unregister;
 	}
+#else
+	struct class_device *class_dev = &device->class_dev;
+	int ret;
+	int i;
+
+	class_dev->class      = &ib_class;
+	class_dev->class_data = device;
+	class_dev->dev	      = device->dma_device;
+	strlcpy(class_dev->class_id, device->name, BUS_ID_SIZE);
+
+	INIT_LIST_HEAD(&device->port_list);
+
+	ret = class_device_register(class_dev);
+	if (ret)
+		goto err;
+
+	for (i = 0; i < ARRAY_SIZE(ib_class_attributes); ++i) {
+		ret = class_device_create_file(class_dev, ib_class_attributes[i]);
+		if (ret)
+			goto err_unregister;
+	}
+#endif
 
 	device->ports_parent = kobject_create_and_add("ports",
 						      &class_dev->kobj);
@@ -1115,7 +1239,11 @@ err_put:
 	free_port_list_attributes(device);
 
 err_unregister:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	device_unregister(class_dev);
+#else
+	class_device_unregister(class_dev);
+#endif
 
 err:
 	return ret;
@@ -1123,8 +1251,12 @@ err:
 
 void ib_device_unregister_sysfs(struct ib_device *device)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	/* Hold kobject until ib_dealloc_device() */
 	struct kobject *kobj_dev = kobject_get(&device->dev.kobj);
+#else
+	struct kobject *kobj_dev = &device->dev.kobj;
+#endif
 	int i;
 
 	if (device->node_type == RDMA_NODE_RNIC && device->get_protocol_stats)
@@ -1133,9 +1265,17 @@ void ib_device_unregister_sysfs(struct ib_device *device)
 	free_port_list_attributes(device);
 
 	for (i = 0; i < ARRAY_SIZE(ib_class_attributes); ++i)
-		device_remove_file(&device->dev, ib_class_attributes[i]);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+ 		device_remove_file(&device->dev, ib_class_attributes[i]);
+#else
+		class_device_remove_file(&device->class_dev, ib_class_attributes[i]);
+#endif
 
-	device_unregister(&device->dev);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+ 	device_unregister(&device->dev);
+#else
+	class_device_unregister(&device->class_dev);
+#endif
 }
 
 int ib_sysfs_setup(void)
diff --git a/drivers/infiniband/core/ucm.c b/drivers/infiniband/core/ucm.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/ucm.c
+++ b/drivers/infiniband/core/ucm.c
@@ -58,8 +58,13 @@ MODULE_LICENSE("Dual BSD/GPL");
 
 struct ib_ucm_device {
 	int			devnum;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct cdev		cdev;
 	struct device		dev;
+#else
+	struct cdev		dev;
+	struct class_device	class_dev;
+#endif
 	struct ib_device	*ib_dev;
 };
 
@@ -106,6 +111,10 @@ enum {
 	IB_UCM_MAX_DEVICES = 32
 };
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+/* ib_cm and ib_user_cm modules share /sys/class/infiniband_cm */
+extern struct class cm_class;
+#endif
 #define IB_UCM_BASE_DEV MKDEV(IB_UCM_MAJOR, IB_UCM_BASE_MINOR)
 
 static void ib_ucm_add_one(struct ib_device *device);
@@ -407,6 +416,9 @@ static ssize_t ib_ucm_event(struct ib_ucm_file *file,
 	struct ib_ucm_event_get cmd;
 	struct ib_ucm_event *uevent;
 	int result = 0;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	DEFINE_WAIT(wait);
+#endif
 
 	if (out_len < sizeof(struct ib_ucm_event_resp))
 		return -ENOSPC;
@@ -712,9 +724,20 @@ static int ib_ucm_alloc_data(const void **dest, u64 src, u32 len)
 	if (!len)
 		return 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	data = memdup_user((void __user *)(unsigned long)src, len);
 	if (IS_ERR(data))
 		return PTR_ERR(data);
+#else
+	data = kmalloc(len, GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	if (copy_from_user(data, (void __user *)(unsigned long)src, len)) {
+		kfree(data);
+		return -EFAULT;
+	}
+#endif
 
 	*dest = data;
 	return 0;
@@ -1180,9 +1203,15 @@ static int ib_ucm_open(struct inode *inode, struct file *filp)
 
 	filp->private_data = file;
 	file->filp = filp;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	file->device = container_of(inode->i_cdev, struct ib_ucm_device, cdev);
 
 	return nonseekable_open(inode, filp);
+#else
+	file->device = container_of(inode->i_cdev, struct ib_ucm_device, dev);
+
+	return 0;
+#endif
 }
 
 static int ib_ucm_close(struct inode *inode, struct file *filp)
@@ -1211,6 +1240,7 @@ static int ib_ucm_close(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void ib_ucm_release_dev(struct device *dev)
 {
 	struct ib_ucm_device *ucm_dev;
@@ -1338,6 +1368,100 @@ static void ib_ucm_remove_one(struct ib_device *device)
 
 	device_unregister(&ucm_dev->dev);
 }
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+static void ucm_release_class_dev(struct class_device *class_dev)
+{
+	struct ib_ucm_device *dev;
+
+	dev = container_of(class_dev, struct ib_ucm_device, class_dev);
+	cdev_del(&dev->dev);
+	clear_bit(dev->devnum, dev_map);
+	kfree(dev);
+}
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+static const struct file_operations ucm_fops = {
+#else
+static struct file_operations ucm_fops = {
+#endif
+	.owner 	 = THIS_MODULE,
+	.open 	 = ib_ucm_open,
+	.release = ib_ucm_close,
+	.write 	 = ib_ucm_write,
+	.poll    = ib_ucm_poll,
+};
+
+static ssize_t show_ibdev(struct class_device *class_dev, char *buf)
+{
+	struct ib_ucm_device *dev;
+
+	dev = container_of(class_dev, struct ib_ucm_device, class_dev);
+	return sprintf(buf, "%s\n", dev->ib_dev->name);
+}
+static CLASS_DEVICE_ATTR(ibdev, S_IRUGO, show_ibdev, NULL);
+
+static void ib_ucm_add_one(struct ib_device *device)
+{
+	struct ib_ucm_device *ucm_dev;
+
+	if (!device->alloc_ucontext ||
+	    rdma_node_get_transport(device->node_type) != RDMA_TRANSPORT_IB)
+		return;
+
+	ucm_dev = kzalloc(sizeof *ucm_dev, GFP_KERNEL);
+	if (!ucm_dev)
+		return;
+
+	ucm_dev->ib_dev = device;
+
+	ucm_dev->devnum = find_first_zero_bit(dev_map, IB_UCM_MAX_DEVICES);
+	if (ucm_dev->devnum >= IB_UCM_MAX_DEVICES)
+		goto err;
+
+	set_bit(ucm_dev->devnum, dev_map);
+
+	cdev_init(&ucm_dev->dev, &ucm_fops);
+	ucm_dev->dev.owner = THIS_MODULE;
+	kobject_set_name(&ucm_dev->dev.kobj, "ucm%d", ucm_dev->devnum);
+	if (cdev_add(&ucm_dev->dev, IB_UCM_BASE_DEV + ucm_dev->devnum, 1))
+		goto err;
+
+	ucm_dev->class_dev.class = &cm_class;
+	ucm_dev->class_dev.dev = device->dma_device;
+	ucm_dev->class_dev.devt = ucm_dev->dev.dev;
+	ucm_dev->class_dev.release = ucm_release_class_dev;
+	snprintf(ucm_dev->class_dev.class_id, BUS_ID_SIZE, "ucm%d",
+		 ucm_dev->devnum);
+	if (class_device_register(&ucm_dev->class_dev))
+		goto err_cdev;
+
+	if (class_device_create_file(&ucm_dev->class_dev,
+				     &class_device_attr_ibdev))
+		goto err_class;
+
+	ib_set_client_data(device, &ucm_client, ucm_dev);
+	return;
+
+err_class:
+	class_device_unregister(&ucm_dev->class_dev);
+err_cdev:
+	cdev_del(&ucm_dev->dev);
+	clear_bit(ucm_dev->devnum, dev_map);
+err:
+	kfree(ucm_dev);
+	return;
+}
+
+static void ib_ucm_remove_one(struct ib_device *device)
+{
+	struct ib_ucm_device *ucm_dev = ib_get_client_data(device, &ucm_client);
+
+	if (!ucm_dev)
+		return;
+
+	class_device_unregister(&ucm_dev->class_dev);
+}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
@@ -1399,8 +1523,10 @@ static void __exit ib_ucm_cleanup(void)
 	class_remove_file(&cm_class, &class_attr_abi_version);
 #endif
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UCM_MAX_DEVICES);
+#endif
 	idr_destroy(&ctx_id_table);
 }
 
diff --git a/drivers/infiniband/core/ucma.c b/drivers/infiniband/core/ucma.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/ucma.c
+++ b/drivers/infiniband/core/ucma.c
@@ -56,6 +56,7 @@ MODULE_LICENSE("Dual BSD/GPL");
 
 static unsigned int max_backlog = 1024;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *ucma_ctl_table_hdr;
 static struct ctl_table ucma_ctl_table[] = {
@@ -76,6 +77,7 @@ static struct ctl_path ucma_ctl_path[] = {
 };
 #endif
 #endif
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 struct ucma_file {
 	struct mutex		mut;
@@ -1339,6 +1341,7 @@ static ssize_t ucma_set_option(struct ucma_file *file, const char __user *inbuf,
 	if (IS_ERR(ctx))
 		return PTR_ERR(ctx);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	optval = memdup_user((void __user *) (unsigned long) cmd.optval,
 			     cmd.optlen);
 	if (IS_ERR(optval)) {
@@ -1353,6 +1356,27 @@ static ssize_t ucma_set_option(struct ucma_file *file, const char __user *inbuf,
 out:
 	ucma_put_ctx(ctx);
 	return ret;
+#else
+	optval = kmalloc(cmd.optlen, GFP_KERNEL);
+	if (!optval) {
+		ret = -ENOMEM;
+		goto out1;
+	}
+
+	if (copy_from_user(optval, (void __user *) (unsigned long) cmd.optval,
+			   cmd.optlen)) {
+		ret = -EFAULT;
+		goto out2;
+	}
+
+	ret = ucma_set_option_level(ctx, cmd.level, cmd.optname, optval,
+				    cmd.optlen);
+out2:
+	kfree(optval);
+out1:
+	ucma_put_ctx(ctx);
+	return ret;
+#endif
 }
 
 static ssize_t ucma_notify(struct ucma_file *file, const char __user *inbuf,
@@ -1752,7 +1776,11 @@ static int ucma_close(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations ucma_fops = {
+#else
+static struct file_operations ucma_fops = {
+#endif
 	.owner 	 = THIS_MODULE,
 	.open 	 = ucma_open,
 	.release = ucma_close,
@@ -1764,11 +1792,14 @@ static const struct file_operations ucma_fops = {
 static struct miscdevice ucma_misc = {
 	.minor		= MISC_DYNAMIC_MINOR,
 	.name		= "rdma_cm",
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.nodename	= "infiniband/rdma_cm",
 	.mode		= 0666,
+#endif
 	.fops		= &ucma_fops,
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_abi_version(struct device *dev,
 				struct device_attribute *attr,
 				char *buf)
@@ -1776,6 +1807,13 @@ static ssize_t show_abi_version(struct device *dev,
 	return sprintf(buf, "%d\n", RDMA_USER_CM_ABI_VERSION);
 }
 static DEVICE_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#else
+static ssize_t show_abi_version(struct class_device *class_dev, char *buf)
+{
+	return sprintf(buf, "%d\n", RDMA_USER_CM_ABI_VERSION);
+}
+static CLASS_DEVICE_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static int __init ucma_init(void)
 {
@@ -1785,12 +1823,18 @@ static int __init ucma_init(void)
 	if (ret)
 		return ret;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ret = device_create_file(ucma_misc.this_device, &dev_attr_abi_version);
+#else
+	ret = class_device_create_file(ucma_misc.class,
+				       &class_device_attr_abi_version);
+#endif
 	if (ret) {
 		printk(KERN_ERR "rdma_ucm: couldn't create abi_version attr\n");
 		goto err1;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
@@ -1808,6 +1852,9 @@ static int __init ucma_init(void)
 err2:
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
 #endif
+#else
+	return 0;
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 err1:
 	misc_deregister(&ucma_misc);
 	return ret;
@@ -1815,6 +1862,7 @@ err1:
 
 static void __exit ucma_cleanup(void)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	unregister_net_sysctl_table(ucma_ctl_table_hdr);
@@ -1823,6 +1871,10 @@ static void __exit ucma_cleanup(void)
 #endif
 #endif
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
+#else
+	class_device_remove_file(ucma_misc.class,
+				 &class_device_attr_abi_version);
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 	misc_deregister(&ucma_misc);
 	idr_destroy(&ctx_idr);
 }
diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -32,6 +32,7 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/mm.h>
 #include <linux/dma-mapping.h>
 #include <linux/sched.h>
@@ -41,6 +42,7 @@
 #include <linux/slab.h>
 #include <rdma/ib_umem_odp.h>
 
+#include <linux/printk.h>
 #include "uverbs.h"
 
 
@@ -1068,7 +1070,6 @@ int ib_umem_page_count(struct ib_umem *umem)
 	return n;
 }
 EXPORT_SYMBOL(ib_umem_page_count);
-
 /*
  * Copy from the given ib_umem's pages to the given buffer.
  *
@@ -1102,3 +1103,316 @@ int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
 		return 0;
 }
 EXPORT_SYMBOL(ib_umem_copy_from);
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
+#include <linux/mm.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/hugetlb.h>
+#include <linux/dma-attrs.h>
+
+#include "uverbs.h"
+
+#define IB_UMEM_MAX_PAGE_CHUNK						\
+	((PAGE_SIZE - offsetof(struct ib_umem_chunk, page_list)) /	\
+	 ((void *) &((struct ib_umem_chunk *) 0)->page_list[1] -	\
+	  (void *) &((struct ib_umem_chunk *) 0)->page_list[0]))
+
+#ifdef __ia64__
+extern int dma_map_sg_hp_wa;
+
+static int dma_map_sg_ia64(struct ib_device *ibdev,
+			   struct scatterlist *sg,
+			   int nents,
+			   enum dma_data_direction dir)
+{
+	int i, rc, j, lents = 0;
+	struct device *dev;
+
+	if (!dma_map_sg_hp_wa)
+		return ib_dma_map_sg(ibdev, sg, nents, dir);
+
+	dev = ibdev->dma_device;
+	for (i = 0; i < nents; ++i) {
+		rc = dma_map_sg(dev, sg + i, 1, dir);
+		if (rc <= 0) {
+			for (j = 0; j < i; ++j)
+				dma_unmap_sg(dev, sg + j, 1, dir);
+
+			return 0;
+		}
+		lents += rc;
+	}
+
+	return lents;
+}
+
+static void dma_unmap_sg_ia64(struct ib_device *ibdev,
+			      struct scatterlist *sg,
+			      int nents,
+			      enum dma_data_direction dir)
+{
+	int i;
+	struct device *dev;
+
+	if (!dma_map_sg_hp_wa)
+		return ib_dma_unmap_sg(ibdev, sg, nents, dir);
+
+	dev = ibdev->dma_device;
+	for (i = 0; i < nents; ++i)
+		dma_unmap_sg(dev, sg + i, 1, dir);
+}
+
+#define ib_dma_map_sg(dev, sg, nents, dir) dma_map_sg_ia64(dev, sg, nents, dir)
+#define ib_dma_unmap_sg(dev, sg, nents, dir) dma_unmap_sg_ia64(dev, sg, nents, dir)
+
+#endif
+
+static void __ib_umem_release(struct ib_device *dev, struct ib_umem *umem, int dirty)
+{
+	struct ib_umem_chunk *chunk, *tmp;
+	int i;
+
+	list_for_each_entry_safe(chunk, tmp, &umem->chunk_list, list) {
+		ib_dma_unmap_sg(dev, chunk->page_list,
+				chunk->nents, DMA_BIDIRECTIONAL);
+		for (i = 0; i < chunk->nents; ++i) {
+			struct page *page = sg_page(&chunk->page_list[i]);
+
+			if (umem->writable && dirty)
+				set_page_dirty_lock(page);
+			put_page(page);
+		}
+
+		kfree(chunk);
+	}
+}
+
+/**
+ * ib_umem_get - Pin and DMA map userspace memory.
+ * @context: userspace context to pin memory for
+ * @addr: userspace virtual address to start at
+ * @size: length of region to pin
+ * @access: IB_ACCESS_xxx flags for memory being pinned
+ * @dmasync: flush in-flight DMA when the memory region is written
+ */
+struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
+			    size_t size, int access, int dmasync)
+{
+	struct ib_umem *umem;
+	struct page **page_list;
+	struct vm_area_struct **vma_list;
+	struct ib_umem_chunk *chunk;
+	unsigned long locked;
+	unsigned long lock_limit;
+	unsigned long cur_base;
+	unsigned long npages;
+	int ret;
+	int off;
+	int i;
+	DEFINE_DMA_ATTRS(attrs);
+
+	if (dmasync)
+		dma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);
+
+	if (!can_do_mlock())
+		return ERR_PTR(-EPERM);
+
+	umem = kmalloc(sizeof *umem, GFP_KERNEL);
+	if (!umem)
+		return ERR_PTR(-ENOMEM);
+
+	umem->context   = context;
+	umem->length    = size;
+	umem->address    = addr;
+	umem->page_size = PAGE_SIZE;
+	/*
+	 * We ask for writable memory if any access flags other than
+	 * "remote read" are set.  "Local write" and "remote write"
+	 * obviously require write access.  "Remote atomic" can do
+	 * things like fetch and add, which will modify memory, and
+	 * "MW bind" can change permissions by binding a window.
+	 */
+	umem->writable  = !!(access & ~IB_ACCESS_REMOTE_READ);
+
+	/* We assume the memory is from hugetlb until proved otherwise */
+	umem->hugetlb   = 1;
+
+	INIT_LIST_HEAD(&umem->chunk_list);
+
+	page_list = (struct page **) __get_free_page(GFP_KERNEL);
+	if (!page_list) {
+		kfree(umem);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/*
+	 * if we can't alloc the vma_list, it's not so bad;
+	 * just assume the memory is not hugetlb memory
+	 */
+	vma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);
+	if (!vma_list)
+		umem->hugetlb = 0;
+
+	npages = ib_umem_num_pages(umem);
+
+	down_write(&current->mm->mmap_sem);
+
+	locked     = npages + current->mm->locked_vm;
+	lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur >> PAGE_SHIFT;
+
+	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	cur_base = addr & PAGE_MASK;
+
+	ret = 0;
+	while (npages) {
+		ret = get_user_pages(current, current->mm, cur_base,
+				     min_t(unsigned long, npages,
+					   PAGE_SIZE / sizeof (struct page *)),
+				     1, !umem->writable, page_list, vma_list);
+
+		if (ret < 0)
+			goto out;
+
+		cur_base += ret * PAGE_SIZE;
+		npages   -= ret;
+
+		off = 0;
+
+		while (ret) {
+			chunk = kmalloc(sizeof *chunk + sizeof (struct scatterlist) *
+					min_t(int, ret, IB_UMEM_MAX_PAGE_CHUNK),
+					GFP_KERNEL);
+			if (!chunk) {
+				ret = -ENOMEM;
+				goto out;
+			}
+
+			chunk->nents = min_t(int, ret, IB_UMEM_MAX_PAGE_CHUNK);
+			sg_init_table(chunk->page_list, chunk->nents);
+			for (i = 0; i < chunk->nents; ++i) {
+				if (vma_list &&
+				    !is_vm_hugetlb_page(vma_list[i + off]))
+					umem->hugetlb = 0;
+				sg_set_page(&chunk->page_list[i], page_list[i + off], PAGE_SIZE, 0);
+			}
+
+			chunk->nmap = ib_dma_map_sg_attrs(context->device,
+							  &chunk->page_list[0],
+							  chunk->nents,
+							  DMA_BIDIRECTIONAL,
+							  &attrs);
+			if (chunk->nmap <= 0) {
+				for (i = 0; i < chunk->nents; ++i)
+					put_page(sg_page(&chunk->page_list[i]));
+				kfree(chunk);
+
+				ret = -ENOMEM;
+				goto out;
+			}
+
+			ret -= chunk->nents;
+			off += chunk->nents;
+			list_add_tail(&chunk->list, &umem->chunk_list);
+		}
+
+		ret = 0;
+	}
+
+out:
+	if (ret < 0) {
+		__ib_umem_release(context->device, umem, 0);
+		kfree(umem);
+	} else
+		current->mm->locked_vm = locked;
+
+	up_write(&current->mm->mmap_sem);
+	if (vma_list)
+		free_page((unsigned long) vma_list);
+	free_page((unsigned long) page_list);
+
+	return ret < 0 ? ERR_PTR(ret) : umem;
+}
+EXPORT_SYMBOL(ib_umem_get);
+
+static void ib_umem_account(struct work_struct *work)
+{
+	struct ib_umem *umem = container_of(work, struct ib_umem, work);
+
+	down_write(&umem->mm->mmap_sem);
+	umem->mm->locked_vm -= umem->diff;
+	up_write(&umem->mm->mmap_sem);
+	mmput(umem->mm);
+	kfree(umem);
+}
+
+/**
+ * ib_umem_release - release memory pinned with ib_umem_get
+ * @umem: umem struct to release
+ */
+void ib_umem_release(struct ib_umem *umem)
+{
+	struct ib_ucontext *context = umem->context;
+	struct mm_struct *mm;
+	unsigned long diff;
+
+	__ib_umem_release(umem->context->device, umem, 1);
+
+	mm = get_task_mm(current);
+	if (!mm) {
+		kfree(umem);
+		return;
+	}
+
+	diff = ib_umem_num_pages(umem);
+
+	/*
+	 * We may be called with the mm's mmap_sem already held.  This
+	 * can happen when a userspace munmap() is the call that drops
+	 * the last reference to our file and calls our release
+	 * method.  If there are memory regions to destroy, we'll end
+	 * up here and not be able to take the mmap_sem.  In that case
+	 * we defer the vm_locked accounting to the system workqueue.
+	 */
+	if (context->closing) {
+		if (!down_write_trylock(&mm->mmap_sem)) {
+			INIT_WORK(&umem->work, ib_umem_account);
+			umem->mm   = mm;
+			umem->diff = diff;
+
+			schedule_work(&umem->work);
+			return;
+		}
+	} else
+		down_write(&mm->mmap_sem);
+
+	current->mm->locked_vm -= diff;
+	up_write(&mm->mmap_sem);
+	mmput(mm);
+	kfree(umem);
+}
+EXPORT_SYMBOL(ib_umem_release);
+
+int ib_umem_page_count(struct ib_umem *umem)
+{
+	struct ib_umem_chunk *chunk;
+	int shift;
+	int i;
+	int n;
+
+	shift = ilog2(umem->page_size);
+
+	n = 0;
+	list_for_each_entry(chunk, &umem->chunk_list, list)
+		for (i = 0; i < chunk->nmap; ++i)
+			n += sg_dma_len(&chunk->page_list[i]) >> shift;
+
+	return n;
+}
+EXPORT_SYMBOL(ib_umem_page_count);
+
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
diff --git a/drivers/infiniband/core/user_mad.c b/drivers/infiniband/core/user_mad.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/user_mad.c
+++ b/drivers/infiniband/core/user_mad.c
@@ -92,11 +92,19 @@ enum {
  */
 
 struct ib_umad_port {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct cdev           cdev;
 	struct device	      *dev;
 
 	struct cdev           sm_cdev;
 	struct device	      *sm_dev;
+#else
+	struct cdev           *dev;
+	struct class_device   *class_dev;
+
+	struct cdev           *sm_dev;
+	struct class_device   *sm_class_dev;
+#endif
 	struct semaphore       sm_sem;
 
 	struct mutex	       file_mutex;
@@ -147,11 +155,15 @@ static struct class *umad_class;
 static const dev_t base_dev = MKDEV(IB_UMAD_MAJOR, IB_UMAD_MINOR_BASE);
 
 static DEFINE_SPINLOCK(port_lock);
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static struct ib_umad_port *umad_port[IB_UMAD_MAX_PORTS];
+#endif
 static DECLARE_BITMAP(dev_map, IB_UMAD_MAX_PORTS);
 
 static void ib_umad_add_one(struct ib_device *device);
 static void ib_umad_remove_one(struct ib_device *device);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void ib_umad_release_dev(struct kobject *kobj)
 {
 	struct ib_umad_device *dev =
@@ -163,6 +175,7 @@ static void ib_umad_release_dev(struct kobject *kobj)
 static struct kobj_type ib_umad_dev_ktype = {
 	.release = ib_umad_release_dev,
 };
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 static int hdr_size(struct ib_umad_file *file)
 {
@@ -692,7 +705,16 @@ static void update_mgmt_threshold(struct ib_umad_file *file, struct ib_mad_reg_r
 	int i;
 
 	/*Update managers' class rx threshold*/
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		for_each_set_bit(i, req.method_mask, IB_MGMT_MAX_METHODS) {
+#else
+		for (i = find_first_bit(req.method_mask, IB_MGMT_MAX_METHODS);
+		     i < IB_MGMT_MAX_METHODS;
+		     i = find_next_bit(req.method_mask,
+				       IB_MGMT_MAX_METHODS,
+				       1+i)) {
+
+#endif
 			if (i == IB_MGMT_METHOD_GET ||
 			    i == IB_MGMT_METHOD_SET ||
 			    i == IB_MGMT_METHOD_REPORT ||
@@ -717,8 +739,10 @@ static int ib_umad_reg_agent(struct ib_umad_file *file, void __user *arg,
 	mutex_lock(&file->mutex);
 
 	if (!file->port->ib_dev) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev_notice(file->port->dev,
 			   "ib_umad_reg_agent: invalid device\n");
+#endif
 		ret = -EPIPE;
 		goto out;
 	}
@@ -729,9 +753,11 @@ static int ib_umad_reg_agent(struct ib_umad_file *file, void __user *arg,
 	}
 
 	if (ureq.qpn != 0 && ureq.qpn != 1) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev_notice(file->port->dev,
 			   "ib_umad_reg_agent: invalid QPN %d specified\n",
 			   ureq.qpn);
+#endif
 		ret = -EINVAL;
 		goto out;
 	}
@@ -740,9 +766,11 @@ static int ib_umad_reg_agent(struct ib_umad_file *file, void __user *arg,
 		if (!__get_agent(file, agent_id))
 			goto found;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev_notice(file->port->dev,
 		   "ib_umad_reg_agent: Max Agents (%u) reached\n",
 		   IB_UMAD_MAX_AGENTS);
+#endif
 	ret = -ENOMEM;
 	goto out;
 
@@ -786,6 +814,7 @@ found:
 
 	if (!file->already_used) {
 		file->already_used = 1;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (!file->use_pkey_index) {
 			dev_warn(file->port->dev,
 				"process %s did not enable P_Key index support.\n",
@@ -793,6 +822,7 @@ found:
 			dev_warn(file->port->dev,
 				"   Documentation/infiniband/user_mad.txt has info on the new ABI.\n");
 		}
+#endif
 	}
 
 	file->agent[agent_id] = agent;
@@ -821,8 +851,10 @@ static int ib_umad_reg_agent2(struct ib_umad_file *file, void __user *arg)
 	mutex_lock(&file->mutex);
 
 	if (!file->port->ib_dev) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev_notice(file->port->dev,
 			   "ib_umad_reg_agent2: invalid device\n");
+#endif
 		ret = -EPIPE;
 		goto out;
 	}
@@ -833,17 +865,21 @@ static int ib_umad_reg_agent2(struct ib_umad_file *file, void __user *arg)
 	}
 
 	if (ureq.qpn != 0 && ureq.qpn != 1) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev_notice(file->port->dev,
 			   "ib_umad_reg_agent2: invalid QPN %d specified\n",
 			   ureq.qpn);
+#endif
 		ret = -EINVAL;
 		goto out;
 	}
 
 	if (ureq.flags & ~IB_USER_MAD_REG_FLAGS_CAP) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		dev_notice(file->port->dev,
 			   "ib_umad_reg_agent2 failed: invalid registration flags specified 0x%x; supported 0x%x\n",
 			   ureq.flags, IB_USER_MAD_REG_FLAGS_CAP);
+#endif
 		ret = -EINVAL;
 
 		if (put_user((u32)IB_USER_MAD_REG_FLAGS_CAP,
@@ -858,9 +894,11 @@ static int ib_umad_reg_agent2(struct ib_umad_file *file, void __user *arg)
 		if (!__get_agent(file, agent_id))
 			goto found;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev_notice(file->port->dev,
 		   "ib_umad_reg_agent2: Max Agents (%u) reached\n",
 		   IB_UMAD_MAX_AGENTS);
+#endif
 	ret = -ENOMEM;
 	goto out;
 
@@ -870,9 +908,11 @@ found:
 		req.mgmt_class         = ureq.mgmt_class;
 		req.mgmt_class_version = ureq.mgmt_class_version;
 		if (ureq.oui & 0xff000000) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 			dev_notice(file->port->dev,
 				   "ib_umad_reg_agent2 failed: oui invalid 0x%08x\n",
 				   ureq.oui);
+#endif
 			ret = -EINVAL;
 			goto out;
 		}
@@ -1034,12 +1074,14 @@ static long ib_umad_compat_ioctl(struct file *filp, unsigned int cmd,
 }
 #endif
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void init_recv_list(struct counted_list *recv_list)
 {
 	INIT_LIST_HEAD(&recv_list->list);
 	recv_list->count = 0;
 	recv_list->threshold = IB_UMAD_RX_THRESHOLD;
 }
+#endif
 
 /*
  * ib_umad_open() does not need the BKL:
@@ -1054,6 +1096,7 @@ static int ib_umad_open(struct inode *inode, struct file *filp)
 {
 	struct ib_umad_port *port;
 	struct ib_umad_file *file;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int ret = -ENXIO;
 
 	port = container_of(inode->i_cdev, struct ib_umad_port, cdev);
@@ -1070,7 +1113,7 @@ static int ib_umad_open(struct inode *inode, struct file *filp)
 
 	mutex_init(&file->mutex);
 	spin_lock_init(&file->send_lock);
-	init_recv_list(&file->recv_list);
+	init_recv_list(&file->recv_list.list);
 	INIT_LIST_HEAD(&file->send_list);
 	init_waitqueue_head(&file->recv_wait);
 
@@ -1088,6 +1131,44 @@ static int ib_umad_open(struct inode *inode, struct file *filp)
 
 	kobject_get(&port->umad_dev->kobj);
 
+#else
+	int ret = 0;
+
+	spin_lock(&port_lock);
+	port = umad_port[iminor(inode) - IB_UMAD_MINOR_BASE];
+	if (port)
+		kobject_get(&port->umad_dev->kobj);
+	spin_unlock(&port_lock);
+
+	if (!port)
+		return -ENXIO;
+
+	mutex_lock(&port->file_mutex);
+
+	if (!port->ib_dev) {
+		ret = -ENXIO;
+		goto out;
+	}
+
+	file = kzalloc(sizeof *file, GFP_KERNEL);
+	if (!file) {
+		kobject_put(&port->umad_dev->kobj);
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	mutex_init(&file->mutex);
+	spin_lock_init(&file->send_lock);
+	INIT_LIST_HEAD(&file->recv_list.list);
+	INIT_LIST_HEAD(&file->send_list);
+	init_waitqueue_head(&file->recv_wait);
+
+	file->port = port;
+	filp->private_data = file;
+
+	list_add_tail(&file->port_list, &port->file_list);
+
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 out:
 	mutex_unlock(&port->file_mutex);
 	return ret;
@@ -1130,7 +1211,11 @@ static int ib_umad_close(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations umad_fops = {
+#else
+static struct file_operations umad_fops = {
+#endif
 	.owner		= THIS_MODULE,
 	.read		= ib_umad_read,
 	.write		= ib_umad_write,
@@ -1141,7 +1226,9 @@ static const struct file_operations umad_fops = {
 #endif
 	.open		= ib_umad_open,
 	.release	= ib_umad_close,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.llseek		= no_llseek,
+#endif
 };
 
 static int ib_umad_sm_open(struct inode *inode, struct file *filp)
@@ -1152,7 +1239,15 @@ static int ib_umad_sm_open(struct inode *inode, struct file *filp)
 	};
 	int ret;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	port = container_of(inode->i_cdev, struct ib_umad_port, sm_cdev);
+#else
+	spin_lock(&port_lock);
+	port = umad_port[iminor(inode) - IB_UMAD_MINOR_BASE - IB_UMAD_MAX_PORTS];
+	if (port)
+		kobject_get(&port->umad_dev->kobj);
+	spin_unlock(&port_lock);
+#endif
 
 	if (filp->f_flags & O_NONBLOCK) {
 		if (down_trylock(&port->sm_sem)) {
@@ -1211,11 +1306,17 @@ static int ib_umad_sm_close(struct inode *inode, struct file *filp)
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations umad_sm_fops = {
+#else
+static struct file_operations umad_sm_fops = {
+#endif
 	.owner	 = THIS_MODULE,
 	.open	 = ib_umad_sm_open,
 	.release = ib_umad_sm_close,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.llseek	 = no_llseek,
+#endif
 };
 
 static struct ib_client umad_client = {
@@ -1224,6 +1325,7 @@ static struct ib_client umad_client = {
 	.remove = ib_umad_remove_one
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_ibdev(struct device *dev, struct device_attribute *attr,
 			  char *buf)
 {
@@ -1247,6 +1349,29 @@ static ssize_t show_port(struct device *dev, struct device_attribute *attr,
 	return sprintf(buf, "%d\n", port->port_num);
 }
 static DEVICE_ATTR(port, S_IRUGO, show_port, NULL);
+#else
+static ssize_t show_ibdev(struct class_device *class_dev, char *buf)
+{
+	struct ib_umad_port *port = class_get_devdata(class_dev);
+
+	if (!port)
+		return -ENODEV;
+
+	return sprintf(buf, "%s\n", port->ib_dev->name);
+}
+static CLASS_DEVICE_ATTR(ibdev, S_IRUGO, show_ibdev, NULL);
+
+static ssize_t show_port(struct class_device *class_dev, char *buf)
+{
+	struct ib_umad_port *port = class_get_devdata(class_dev);
+
+	if (!port)
+		return -ENODEV;
+
+	return sprintf(buf, "%d\n", port->port_num);
+}
+static CLASS_DEVICE_ATTR(port, S_IRUGO, show_port, NULL);
+#endif
 
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
@@ -1259,6 +1384,7 @@ static ssize_t show_abi_version(struct class *class, char *buf)
 static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
 #endif
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UMAD_MAX_PORTS);
 static int find_overflow_devnum(struct ib_device *device)
@@ -1281,11 +1407,13 @@ static int find_overflow_devnum(struct ib_device *device)
 
 	return ret;
 }
+#endif
 
 static int ib_umad_init_port(struct ib_device *device, int port_num,
 			     struct ib_umad_device *umad_dev,
 			     struct ib_umad_port *port)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int devnum;
 	dev_t base;
 
@@ -1368,6 +1496,84 @@ err_cdev:
 		clear_bit(devnum, dev_map);
 	else
 		clear_bit(devnum, overflow_map);
+#else
+	spin_lock(&port_lock);
+	port->dev_num = find_first_zero_bit(dev_map, IB_UMAD_MAX_PORTS);
+	if (port->dev_num >= IB_UMAD_MAX_PORTS) {
+		spin_unlock(&port_lock);
+		return -1;
+	}
+	set_bit(port->dev_num, dev_map);
+	spin_unlock(&port_lock);
+
+	port->ib_dev   = device;
+	port->port_num = port_num;
+	init_MUTEX(&port->sm_sem);
+	mutex_init(&port->file_mutex);
+	INIT_LIST_HEAD(&port->file_list);
+
+	port->dev = cdev_alloc();
+	if (!port->dev)
+		return -1;
+	port->dev->owner = THIS_MODULE;
+	port->dev->ops   = &umad_fops;
+	kobject_set_name(&port->dev->kobj, "umad%d", port->dev_num);
+	if (cdev_add(port->dev, base_dev + port->dev_num, 1))
+		goto err_cdev;
+
+	port->class_dev = class_device_create(umad_class, NULL, port->dev->dev,
+					      device->dma_device,
+					      "umad%d", port->dev_num);
+	if (IS_ERR(port->class_dev))
+		goto err_cdev;
+
+	if (class_device_create_file(port->class_dev, &class_device_attr_ibdev))
+		goto err_class;
+	if (class_device_create_file(port->class_dev, &class_device_attr_port))
+		goto err_class;
+
+	port->sm_dev = cdev_alloc();
+	if (!port->sm_dev)
+		goto err_class;
+	port->sm_dev->owner = THIS_MODULE;
+	port->sm_dev->ops   = &umad_sm_fops;
+	kobject_set_name(&port->sm_dev->kobj, "issm%d", port->dev_num);
+	if (cdev_add(port->sm_dev, base_dev + port->dev_num + IB_UMAD_MAX_PORTS, 1))
+		goto err_sm_cdev;
+
+	port->sm_class_dev = class_device_create(umad_class, NULL, port->sm_dev->dev,
+						 device->dma_device,
+						 "issm%d", port->dev_num);
+	if (IS_ERR(port->sm_class_dev))
+		goto err_sm_cdev;
+
+	class_set_devdata(port->class_dev,    port);
+	class_set_devdata(port->sm_class_dev, port);
+
+	if (class_device_create_file(port->sm_class_dev, &class_device_attr_ibdev))
+		goto err_sm_class;
+	if (class_device_create_file(port->sm_class_dev, &class_device_attr_port))
+		goto err_sm_class;
+
+	spin_lock(&port_lock);
+	umad_port[port->dev_num] = port;
+	spin_unlock(&port_lock);
+
+	return 0;
+
+err_sm_class:
+	class_device_destroy(umad_class, port->sm_dev->dev);
+
+err_sm_cdev:
+	cdev_del(port->sm_dev);
+
+err_class:
+	class_device_destroy(umad_class, port->dev->dev);
+
+err_cdev:
+	cdev_del(port->dev);
+	clear_bit(port->dev_num, dev_map);
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 	return -1;
 }
@@ -1375,6 +1581,7 @@ err_cdev:
 static void ib_umad_kill_port(struct ib_umad_port *port)
 {
 	struct ib_umad_file *file;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int id;
 
 	dev_set_drvdata(port->dev,    NULL);
@@ -1406,6 +1613,42 @@ static void ib_umad_kill_port(struct ib_umad_port *port)
 		clear_bit(port->dev_num, dev_map);
 	else
 		clear_bit(port->dev_num - IB_UMAD_MAX_PORTS, overflow_map);
+#else
+	int already_dead;
+	int id;
+
+	class_set_devdata(port->class_dev,    NULL);
+	class_set_devdata(port->sm_class_dev, NULL);
+
+	class_device_destroy(umad_class, port->dev->dev);
+	class_device_destroy(umad_class, port->sm_dev->dev);
+
+	cdev_del(port->dev);
+	cdev_del(port->sm_dev);
+
+	spin_lock(&port_lock);
+	umad_port[port->dev_num] = NULL;
+	spin_unlock(&port_lock);
+
+	mutex_lock(&port->file_mutex);
+
+	port->ib_dev = NULL;
+
+	list_for_each_entry(file, &port->file_list, port_list) {
+		mutex_lock(&file->mutex);
+		already_dead = file->agents_dead;
+		file->agents_dead = 1;
+		mutex_unlock(&file->mutex);
+
+		for (id = 0; id < IB_UMAD_MAX_AGENTS; ++id)
+			if (file->agent[id])
+				ib_unregister_mad_agent(file->agent[id]);
+	}
+
+	mutex_unlock(&port->file_mutex);
+
+	clear_bit(port->dev_num, dev_map);
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 static void ib_umad_add_one(struct ib_device *device)
@@ -1429,7 +1672,11 @@ static void ib_umad_add_one(struct ib_device *device)
 	if (!umad_dev)
 		return;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_init(&umad_dev->kobj, &ib_umad_dev_ktype);
+#else
+	kobject_init(&umad_dev->kobj);
+#endif
 
 	umad_dev->start_port = s;
 	umad_dev->end_port   = e;
@@ -1437,9 +1684,15 @@ static void ib_umad_add_one(struct ib_device *device)
 	for (i = s; i <= e; ++i) {
 		umad_dev->port[i - s].umad_dev = umad_dev;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (ib_umad_init_port(device, i, umad_dev,
 				      &umad_dev->port[i - s]))
 			goto err;
+#else
+		if (rdma_port_get_link_layer(device, i) == IB_LINK_LAYER_INFINIBAND)
+			if (ib_umad_init_port(device, i, umad_dev, &umad_dev->port[i - s]))
+				goto err;
+#endif
 	}
 
 	ib_set_client_data(device, &umad_client, umad_dev);
@@ -1448,9 +1701,16 @@ static void ib_umad_add_one(struct ib_device *device)
 
 err:
 	while (--i >= s)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ib_umad_kill_port(&umad_dev->port[i - s]);
 
 	kobject_put(&umad_dev->kobj);
+#else
+		if (rdma_port_get_link_layer(device, i) == IB_LINK_LAYER_INFINIBAND)
+			ib_umad_kill_port(&umad_dev->port[i - s]);
+
+	kobject_put(&umad_dev->kobj);
+#endif
 }
 
 static void ib_umad_remove_one(struct ib_device *device)
@@ -1462,11 +1722,19 @@ static void ib_umad_remove_one(struct ib_device *device)
 		return;
 
 	for (i = 0; i <= umad_dev->end_port - umad_dev->start_port; ++i)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ib_umad_kill_port(&umad_dev->port[i]);
 
 	kobject_put(&umad_dev->kobj);
+#else
+		if (rdma_port_get_link_layer(device, i + 1) == IB_LINK_LAYER_INFINIBAND)
+			ib_umad_kill_port(&umad_dev->port[i]);
+
+	kobject_put(&umad_dev->kobj);
+#endif
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *umad_devnode(struct device *dev, umode_t *mode)
 #else
@@ -1475,6 +1743,7 @@ static char *umad_devnode(struct device *dev, mode_t *mode)
 {
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 static int __init ib_umad_init(void)
 {
@@ -1494,7 +1763,9 @@ static int __init ib_umad_init(void)
 		goto out_chrdev;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	umad_class->devnode = umad_devnode;
+#endif
 
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	ret = class_create_file(umad_class, &class_attr_abi_version.attr);
@@ -1529,8 +1800,10 @@ static void __exit ib_umad_cleanup(void)
 	ib_unregister_client(&umad_client);
 	class_destroy(umad_class);
 	unregister_chrdev_region(base_dev, IB_UMAD_MAX_PORTS * 2);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UMAD_MAX_PORTS * 2);
+#endif
 }
 
 module_init(ib_umad_init);
diff --git a/drivers/infiniband/core/uverbs.h b/drivers/infiniband/core/uverbs.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/uverbs.h
+++ b/drivers/infiniband/core/uverbs.h
@@ -86,10 +86,17 @@ struct ib_uverbs_device {
 	struct kref				ref;
 	int					num_comp_vectors;
 	struct completion			comp;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device			       *dev;
+#endif
 	struct ib_device		       *ib_dev;
 	int					devnum;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct cdev			        cdev;
+#else
+	struct cdev			       *dev;
+	struct class_device		       *class_dev;
+#endif
 	struct rb_root				xrcd_tree;
 	struct mutex				xrcd_tree_mutex;
 	struct mutex				disassociate_mutex; /* protect lists of files. */
diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -2582,9 +2582,11 @@ static ssize_t __uverbs_modify_qp(struct ib_uverbs_file *file, int cmd_len,
 	}
 
 	if (qp->real_qp == qp) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ret = ib_resolve_eth_dmac(qp, attr, &cmd->attr_mask);
 		if (ret)
 			goto out;
+#endif
 		ret = qp->device->modify_qp(qp, attr,
 			modify_qp_mask(qp->qp_type, cmd->attr_mask | exp_mask), udata);
 		if (!ret && (cmd->attr_mask & IB_QP_PORT))
@@ -2678,9 +2680,11 @@ ssize_t ib_uverbs_modify_qp(struct ib_uverbs_file *file,
 	attr->alt_ah_attr.port_num 	    = cmd.alt_dest.port_num;
 
 	if (qp->real_qp == qp) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		ret = ib_resolve_eth_dmac(qp, attr, &cmd.attr_mask);
 		if (ret)
 			goto release_qp;
+#endif
 		ret = qp->device->modify_qp(qp, attr,
 			modify_qp_mask(qp->qp_type, cmd.attr_mask), &udata);
 	} else {
diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -43,7 +43,9 @@
 #include <linux/sched.h>
 #include <linux/file.h>
 #include <linux/cdev.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/anon_inodes.h>
+#endif
 #include <linux/slab.h>
 
 #include <asm/uaccess.h>
@@ -56,6 +58,10 @@ MODULE_AUTHOR("Roland Dreier");
 MODULE_DESCRIPTION("InfiniBand userspace verbs access");
 MODULE_LICENSE("Dual BSD/GPL");
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+#define INFINIBANDEVENTFS_MAGIC	0x49426576	/* "IBev" */
+#endif
+
 enum {
 	IB_UVERBS_MAJOR       = 231,
 	IB_UVERBS_BASE_MINOR  = 192,
@@ -104,7 +110,12 @@ DEFINE_IDR(ib_uverbs_xrcd_idr);
 DEFINE_IDR(ib_uverbs_rule_idr);
 DEFINE_IDR(ib_uverbs_dct_idr);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static DEFINE_SPINLOCK(map_lock);
+#else
+static spinlock_t map_lock;
+static struct ib_uverbs_device *dev_table[IB_UVERBS_MAX_DEVICES];
+#endif
 static DECLARE_BITMAP(dev_map, IB_UVERBS_MAX_DEVICES);
 
 static ssize_t (*uverbs_cmd_table[])(struct ib_uverbs_file *file,
@@ -177,6 +188,9 @@ static uverbs_ex_cmd uverbs_exp_cmd_table[] = {
 	[IB_USER_VERBS_EXP_CMD_REREG_MR]	= ib_uverbs_exp_rereg_mr,
 };
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static struct vfsmount *uverbs_event_mnt;
+#endif
 static void ib_uverbs_add_one(struct ib_device *device);
 static void ib_uverbs_remove_one(struct ib_device *device);
 
@@ -303,6 +317,7 @@ static int ib_uverbs_cleanup_ucontext(struct ib_uverbs_file *file,
 		kfree(uqp);
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	list_for_each_entry_safe(uobj, tmp, &context->dct_list, list) {
 		struct ib_dct *dct = uobj->object;
 		struct ib_udct_object *udct =
@@ -316,6 +331,7 @@ static int ib_uverbs_cleanup_ucontext(struct ib_uverbs_file *file,
 
 		kfree(udct);
 	}
+#endif
 
 	list_for_each_entry_safe(uobj, tmp, &context->srq_list, list) {
 		struct ib_srq *srq = uobj->object;
@@ -493,6 +509,9 @@ static int ib_uverbs_event_close(struct inode *inode, struct file *filp)
 		kfree(entry);
 	}
 	spin_unlock_irq(&file->lock);
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	ib_uverbs_event_fasync(-1, filp, 0);
+#endif
 
 	mutex_lock(&file->uverbs_file->device->disassociate_mutex);
 	if (!file->uverbs_file->device->disassociated) {
@@ -508,13 +527,19 @@ static int ib_uverbs_event_close(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations uverbs_event_fops = {
+#else
+static struct file_operations uverbs_event_fops = {
+#endif
 	.owner	 = THIS_MODULE,
 	.read	 = ib_uverbs_event_read,
 	.poll    = ib_uverbs_event_poll,
 	.release = ib_uverbs_event_close,
 	.fasync  = ib_uverbs_event_fasync,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.llseek	 = no_llseek,
+#endif
 };
 
 void ib_uverbs_comp_handler(struct ib_cq *cq, void *cq_context)
@@ -665,12 +690,33 @@ struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
 	ev_file->is_async    = is_async;
 	ev_file->is_closed   = 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	filp = anon_inode_getfile("[infinibandevent]", &uverbs_event_fops,
 				  ev_file, O_RDONLY);
 	if (IS_ERR(filp)) {
 		kfree(ev_file);
 		return filp;
 	}
+#else
+	filp = get_empty_filp();
+	if (!filp) {
+		kfree(ev_file);
+		return ERR_PTR(-ENFILE);
+	}
+
+	/*
+	 * fops_get() can't fail here, because we're coming from a
+	 * system call on a uverbs file, which will already have a
+	 * module reference.
+	 */
+	filp->f_op	   = fops_get(&uverbs_event_fops);
+	filp->f_vfsmnt	   = mntget(uverbs_event_mnt);
+	filp->f_dentry	   = dget(uverbs_event_mnt->mnt_root);
+	filp->f_mapping    = filp->f_dentry->d_inode->i_mapping;
+	filp->f_flags	   = O_RDONLY;
+	filp->f_mode	   = FMODE_READ;
+	filp->private_data = ev_file;
+#endif
 
 	mutex_lock(&uverbs_file->device->disassociate_mutex);
 	if (!uverbs_file->device->disassociated) {
@@ -718,9 +764,13 @@ out:
 	return ev_file;
 #else
 	struct file *filp;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int fput_needed;
 
 	filp = fget_light(fd, &fput_needed);
+#else
+	filp = fget(fd);
+#endif
 	if (!filp)
 		return NULL;
 
@@ -736,7 +786,11 @@ out:
 	kref_get(&ev_file->ref);
 
 out:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	fput_light(filp, fput_needed);
+#else
+	fput(filp);
+#endif
 	return ev_file;
 #endif
 }
@@ -956,6 +1010,7 @@ out:
 	return ret;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static unsigned long ib_uverbs_get_unmapped_area(struct file *filp,
 		unsigned long addr,
 		unsigned long len, unsigned long pgoff, unsigned long flags)
@@ -973,7 +1028,7 @@ static unsigned long ib_uverbs_get_unmapped_area(struct file *filp,
 								pgoff, flags);
 	}
 }
-
+#endif
 
 static long ib_uverbs_ioctl(struct file *filp,
 			    unsigned int cmd, unsigned long arg)
@@ -1024,11 +1079,22 @@ static int ib_uverbs_open(struct inode *inode, struct file *filp)
 	int ret;
 	int module_dependent;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev = container_of(inode->i_cdev, struct ib_uverbs_device, cdev);
 	if (dev)
 		kref_get(&dev->ref);
 	else
 		return -ENXIO;
+#else
+	spin_lock(&map_lock);
+	dev = dev_table[iminor(inode) - IB_UVERBS_BASE_MINOR];
+	if (dev)
+		kref_get(&dev->ref);
+	spin_unlock(&map_lock);
+
+	if (!dev)
+		return -ENXIO;
+#endif
 
 	mutex_lock(&dev->disassociate_mutex);
 	if (dev->disassociated) {
@@ -1109,23 +1175,35 @@ static int ib_uverbs_close(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations uverbs_fops = {
+#else
+static struct file_operations uverbs_fops = {
+#endif
 	.owner	 = THIS_MODULE,
 	.write	 = ib_uverbs_write,
 	.open	 = ib_uverbs_open,
 	.release = ib_uverbs_close,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.llseek	 = no_llseek,
+#endif
 	.unlocked_ioctl = ib_uverbs_ioctl,
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static const struct file_operations uverbs_mmap_fops = {
+#else
+static struct file_operations uverbs_mmap_fops = {
+#endif
 	.owner	 = THIS_MODULE,
 	.write	 = ib_uverbs_write,
 	.mmap    = ib_uverbs_mmap,
 	.open	 = ib_uverbs_open,
 	.release = ib_uverbs_close,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.llseek	 = no_llseek,
 	.get_unmapped_area = ib_uverbs_get_unmapped_area,
+#endif
 	.unlocked_ioctl = ib_uverbs_ioctl,
 };
 
@@ -1135,6 +1213,7 @@ static struct ib_client uverbs_client = {
 	.remove = ib_uverbs_remove_one
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_ibdev(struct device *device, struct device_attribute *attr,
 			  char *buf)
 {
@@ -1170,6 +1249,48 @@ static ssize_t show_abi_version(struct class *class, char *buf)
 static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
 #endif
 
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+static ssize_t show_ibdev(struct class_device *class_dev, char *buf)
+{
+	struct ib_uverbs_device *dev = class_get_devdata(class_dev);
+
+	if (!dev)
+		return -ENODEV;
+
+	return sprintf(buf, "%s\n", dev->ib_dev->name);
+}
+static CLASS_DEVICE_ATTR(ibdev, S_IRUGO, show_ibdev, NULL);
+
+static ssize_t show_dev_abi_version(struct class_device *class_dev, char *buf)
+{
+	struct ib_uverbs_device *dev = class_get_devdata(class_dev);
+
+	if (!dev)
+		return -ENODEV;
+
+	return sprintf(buf, "%d\n", dev->ib_dev->uverbs_abi_ver);
+}
+static CLASS_DEVICE_ATTR(abi_version, S_IRUGO, show_dev_abi_version, NULL);
+
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_VERBS_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+
+static ssize_t show_dev_ref_cnt(struct class_device *class_dev, char *buf)
+{
+	struct ib_uverbs_device *dev = class_get_devdata(class_dev);
+
+	if (!dev)
+		return -ENODEV;
+
+	return sprintf(buf, "%d\n",  atomic_read(&dev->ref.refcount));
+}
+static CLASS_DEVICE_ATTR(ref_cnt, S_IRUGO, show_dev_ref_cnt, NULL);
+
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UVERBS_MAX_DEVICES);
 
@@ -1246,6 +1367,7 @@ static void ib_uverbs_add_one(struct ib_device *device)
 	uverbs_dev->ib_dev           = device;
 	uverbs_dev->num_comp_vectors = device->num_comp_vectors;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	cdev_init(&uverbs_dev->cdev, NULL);
 	uverbs_dev->cdev.owner = THIS_MODULE;
 	uverbs_dev->cdev.ops = device->mmap ? &uverbs_mmap_fops : &uverbs_fops;
@@ -1263,6 +1385,36 @@ static void ib_uverbs_add_one(struct ib_device *device)
 		goto err_class;
 	if (device_create_file(uverbs_dev->dev, &dev_attr_abi_version))
 		goto err_class;
+#else
+	uverbs_dev->dev = cdev_alloc();
+	if (!uverbs_dev->dev)
+		goto err;
+	uverbs_dev->dev->owner = THIS_MODULE;
+	uverbs_dev->dev->ops = device->mmap ? &uverbs_mmap_fops : &uverbs_fops;
+	kobject_set_name(&uverbs_dev->dev->kobj, "uverbs%d", uverbs_dev->devnum);
+	if (cdev_add(uverbs_dev->dev, IB_UVERBS_BASE_DEV + uverbs_dev->devnum, 1))
+		goto err_cdev;
+
+	uverbs_dev->class_dev = class_device_create(uverbs_class, NULL,
+						    uverbs_dev->dev->dev,
+						    device->dma_device,
+						    "uverbs%d", uverbs_dev->devnum);
+	if (IS_ERR(uverbs_dev->class_dev))
+		goto err_cdev;
+
+	class_set_devdata(uverbs_dev->class_dev, uverbs_dev);
+
+	if (class_device_create_file(uverbs_dev->class_dev, &class_device_attr_ibdev))
+		goto err_class;
+	if (class_device_create_file(uverbs_dev->class_dev, &class_device_attr_abi_version))
+		goto err_class;
+	if (class_device_create_file(uverbs_dev->class_dev, &class_device_attr_ref_cnt))
+		goto err_class;
+
+	spin_lock(&map_lock);
+	dev_table[uverbs_dev->devnum] = uverbs_dev;
+	spin_unlock(&map_lock);
+#endif
 
 	if (device->disassociate_ucontext)
 		uverbs_dev->flags |= UVERBS_FLAG_DISASSOCIATE;
@@ -1275,10 +1427,18 @@ static void ib_uverbs_add_one(struct ib_device *device)
 	return;
 
 err_class:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	device_destroy(uverbs_class, uverbs_dev->cdev.dev);
+#else
+	class_device_destroy(uverbs_class, uverbs_dev->dev->dev);
+#endif
 
 err_cdev:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	cdev_del(&uverbs_dev->cdev);
+#else
+	cdev_del(uverbs_dev->dev);
+#endif
 	if (uverbs_dev->devnum < IB_UVERBS_MAX_DEVICES)
 		clear_bit(devnum, dev_map);
 	else
@@ -1354,9 +1514,19 @@ static void ib_uverbs_remove_one(struct ib_device *device)
 	if (!uverbs_dev)
 		return;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev_set_drvdata(uverbs_dev->dev, NULL);
 	device_destroy(uverbs_class, uverbs_dev->cdev.dev);
 	cdev_del(&uverbs_dev->cdev);
+#else
+	class_set_devdata(uverbs_dev->class_dev, NULL);
+	class_device_destroy(uverbs_class, uverbs_dev->dev->dev);
+	cdev_del(uverbs_dev->dev);
+
+	spin_lock(&map_lock);
+	dev_table[uverbs_dev->devnum] = NULL;
+	spin_unlock(&map_lock);
+#endif
 
 	if (uverbs_dev->devnum < IB_UVERBS_MAX_DEVICES)
 		clear_bit(uverbs_dev->devnum, dev_map);
@@ -1388,6 +1558,23 @@ static void ib_uverbs_remove_one(struct ib_device *device)
 	}
 }
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static struct super_block *uverbs_event_get_sb(struct file_system_type *fs_type, int flags,
+			       const char *dev_name, void *data)
+{
+	return get_sb_pseudo(fs_type, "infinibandevent:", NULL,
+			     INFINIBANDEVENTFS_MAGIC);
+}
+
+static struct file_system_type uverbs_event_fs = {
+	/* No owner field so module can be unloaded */
+	.name    = "infinibandeventfs",
+	.get_sb  = uverbs_event_get_sb,
+	.kill_sb = kill_litter_super
+};
+#endif
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *uverbs_devnode(struct device *dev, umode_t *mode)
 #else
@@ -1398,11 +1585,16 @@ static char *uverbs_devnode(struct device *dev, mode_t *mode)
 		*mode = 0666;
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 static int __init ib_uverbs_init(void)
 {
 	int ret;
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	spin_lock_init(&map_lock);
+#endif
+
 	ret = register_chrdev_region(IB_UVERBS_BASE_DEV, IB_UVERBS_MAX_DEVICES,
 				     "infiniband_verbs");
 	if (ret) {
@@ -1417,7 +1609,9 @@ static int __init ib_uverbs_init(void)
 		goto out_chrdev;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 	uverbs_class->devnode = uverbs_devnode;
+#endif
 
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	ret = class_create_file(uverbs_class, &class_attr_abi_version.attr);
@@ -1429,14 +1623,40 @@ static int __init ib_uverbs_init(void)
 		goto out_class;
 	}
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,16))
+	ret = register_filesystem(&uverbs_event_fs);
+	if (ret) {
+		printk(KERN_ERR "user_verbs: couldn't register infinibandeventfs\n");
+		goto out_class;
+	}
+
+	uverbs_event_mnt = kern_mount(&uverbs_event_fs);
+	if (IS_ERR(uverbs_event_mnt)) {
+		ret = PTR_ERR(uverbs_event_mnt);
+		printk(KERN_ERR "user_verbs: couldn't mount infinibandeventfs\n");
+		goto out_fs;
+	}
+#endif
+
 	ret = ib_register_client(&uverbs_client);
 	if (ret) {
 		printk(KERN_ERR "user_verbs: couldn't register client\n");
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16))
 		goto out_class;
+#else
+		goto out_mnt;
+#endif
 	}
 
 	return 0;
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,16))
+out_mnt:
+	mntput(uverbs_event_mnt);
+
+out_fs:
+	unregister_filesystem(&uverbs_event_fs);
+#endif
 out_class:
 	class_destroy(uverbs_class);
 
@@ -1450,8 +1670,15 @@ out:
 static void __exit ib_uverbs_cleanup(void)
 {
 	ib_unregister_client(&uverbs_client);
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,16))
+	mntput(uverbs_event_mnt);
+	unregister_filesystem(&uverbs_event_fs);
+#endif
 	class_destroy(uverbs_class);
 	unregister_chrdev_region(IB_UVERBS_BASE_DEV, IB_UVERBS_MAX_DEVICES);
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,16))
+	flush_scheduled_work();
+#endif
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UVERBS_MAX_DEVICES);
 	idr_destroy(&ib_uverbs_pd_idr);
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -195,6 +195,7 @@ struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr)
 }
 EXPORT_SYMBOL(ib_create_ah);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 int ib_get_grh_header_version(const void *h)
 {
 	const struct iphdr *ip4h = (struct iphdr *)(h + 20);
@@ -239,12 +240,14 @@ static enum rdma_network_type ib_get_net_type_by_grh(struct ib_device *device,
 
 	return RDMA_NETWORK_IB;
 }
+#endif
 
 struct find_gid_index_context {
 	u16 vlan_id;
 	enum ib_gid_type gid_type;
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static bool find_gid_index(const union ib_gid *gid,
 			   const struct ib_gid_attr *gid_attr,
 			   void *context)
@@ -274,6 +277,7 @@ static int get_sgid_index_from_eth(struct ib_device *device, u8 port_num,
 	return ib_find_gid_by_filter(device, sgid, port_num, find_gid_index,
 				     &context, gid_index);
 }
+#endif
 
 int ib_get_gids_from_grh(struct ib_grh *grh, enum rdma_network_type net_type,
 			 union ib_gid *sgid, union ib_gid *dgid)
@@ -314,16 +318,21 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
 		       struct ib_grh *grh, struct ib_ah_attr *ah_attr)
 {
 	u32 flow_class;
-	u16 gid_index;
+	u16 gid_index = 0;
 	int ret;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int is_eth = (rdma_port_get_link_layer(device, port_num) ==
 			IB_LINK_LAYER_ETHERNET);
+#endif
 	enum rdma_network_type net_type = RDMA_NETWORK_IB;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	enum ib_gid_type gid_type = IB_GID_TYPE_IB;
+#endif
 	union ib_gid dgid;
 	union ib_gid sgid;
 
 	memset(ah_attr, 0, sizeof *ah_attr);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (is_eth) {
 		if (wc->wc_flags & IB_WC_WITH_NETWORK_HDR_TYPE)
 			net_type = wc->network_hdr_type;
@@ -331,10 +340,12 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
 			net_type = ib_get_net_type_by_grh(device, port_num, grh);
 		gid_type = ib_network_to_gid_type(net_type, grh);
 	}
+#endif
 	ret = ib_get_gids_from_grh(grh, net_type, &sgid, &dgid);
 	if (ret)
 		return ret;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (is_eth) {
 		u16 vlan_id = wc->wc_flags & IB_WC_WITH_VLAN ?
 				wc->vlan_id : 0xffff;
@@ -361,6 +372,7 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
 		if (wc->wc_flags & IB_WC_WITH_SMAC)
 			memcpy(ah_attr->dmac, wc->smac, ETH_ALEN);
 	}
+#endif
 
 	ah_attr->dlid = wc->slid;
 	ah_attr->sl = wc->sl;
@@ -371,6 +383,7 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
 		ah_attr->ah_flags = IB_AH_GRH;
 		ah_attr->grh.dgid = sgid;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (!is_eth) {
 			ret = ib_find_cached_gid_by_port(device, &dgid,
 							 IB_GID_TYPE_IB,
@@ -379,6 +392,7 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
 			if (ret)
 				return ret;
 		}
+#endif
 
 		ah_attr->grh.sgid_index = (u8) gid_index;
 		flow_class = be32_to_cpu(grh->version_tclass_flow);
@@ -1109,6 +1123,7 @@ int ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
 }
 EXPORT_SYMBOL(ib_modify_qp_is_ok);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 int ib_resolve_eth_dmac(struct ib_qp *qp,
 			struct ib_qp_attr *qp_attr, int *qp_attr_mask)
 {
@@ -1150,10 +1165,12 @@ int ib_resolve_eth_dmac(struct ib_qp *qp,
 
 			rcu_read_unlock();
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 			ret = rdma_addr_find_dmac_by_grh(&sgid,
 							 &qp_attr->ah_attr.grh.dgid,
 							 qp_attr->ah_attr.dmac,
 							 NULL, ifindex);
+#endif
 
 			dev_put(sgid_attr.ndev);
 		}
@@ -1162,13 +1179,16 @@ out:
 	return ret;
 }
 EXPORT_SYMBOL(ib_resolve_eth_dmac);
+#endif
 
 
 int ib_modify_qp(struct ib_qp *qp,
 		 struct ib_qp_attr *qp_attr,
 		 int qp_attr_mask)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	int ret;
+#endif
 
 	if (qp->qpg_type == IB_QPG_PARENT) {
 		struct ib_qpg_attr *pattr = &qp->qpg_attr.parent_attr;
@@ -1178,10 +1198,12 @@ int ib_modify_qp(struct ib_qp *qp,
 			return -EINVAL;
 	}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ret = ib_resolve_eth_dmac(qp, qp_attr, &qp_attr_mask);
 
 	if (ret)
 		return ret;
+#endif
 
 	return qp->device->modify_qp(qp->real_qp, qp_attr, qp_attr_mask, NULL);
 }
diff --git a/include/linux/mlx4/cmd.h b/include/linux/mlx4/cmd.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/linux/mlx4/cmd.h
+++ b/include/linux/mlx4/cmd.h
@@ -34,7 +34,9 @@
 #define MLX4_CMD_H
 
 #include <linux/dma-mapping.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/if_link.h>
+#endif
 
 enum {
 	/* initialization and general commands */
diff --git a/include/rdma/ib_addr.h b/include/rdma/ib_addr.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_addr.h
+++ b/include/rdma/ib_addr.h
@@ -47,6 +47,9 @@
 #include <rdma/ib_verbs.h>
 #include <rdma/ib_pack.h>
 #include <net/ipv6.h>
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+#include <linux/ethtool.h>
+#endif
 
 struct rdma_addr_client {
 	atomic_t refcount;
@@ -183,7 +186,11 @@ static inline void iboe_addr_get_sgid(struct rdma_dev_addr *dev_addr,
 	struct net_device *dev;
 	struct in_device *ip4;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev = dev_get_by_index(&init_net, dev_addr->bound_dev_if);
+#else
+	dev = dev_get_by_index(dev_addr->bound_dev_if);
+#endif
 	if (dev) {
 		ip4 = (struct in_device *)dev->ip_ptr;
 		if (ip4 && ip4->ifa_list && ip4->ifa_list->ifa_address)
@@ -243,6 +250,7 @@ static inline enum ib_mtu iboe_get_mtu(int mtu)
 static inline int iboe_get_rate(struct net_device *dev)
 {
 	struct ethtool_cmd cmd;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	u32 speed;
 	int err;
 
@@ -263,6 +271,22 @@ static inline int iboe_get_rate(struct net_device *dev)
 		return IB_RATE_10_GBPS;
 	else
 		return IB_RATE_PORT_CURRENT;
+#else
+	if (!dev->ethtool_ops || !dev->ethtool_ops->get_settings ||
+	    dev->ethtool_ops->get_settings(dev, &cmd))
+		return IB_RATE_PORT_CURRENT;
+
+	if (cmd.speed >= 40000)
+		return IB_RATE_40_GBPS;
+	else if (cmd.speed >= 30000)
+		return IB_RATE_30_GBPS;
+	else if (cmd.speed >= 20000)
+		return IB_RATE_20_GBPS;
+	else if (cmd.speed >= 10000)
+		return IB_RATE_10_GBPS;
+	else
+		return IB_RATE_PORT_CURRENT;
+#endif
 }
 
 static inline int rdma_link_local_addr(struct in6_addr *addr)
diff --git a/include/rdma/ib_cm.h b/include/rdma/ib_cm.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_cm.h
+++ b/include/rdma/ib_cm.h
@@ -262,6 +262,7 @@ struct ib_cm_event {
 	void			*private_data;
 };
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #define CM_REQ_ATTR_ID		cpu_to_be16(0x0010)
 #define CM_MRA_ATTR_ID		cpu_to_be16(0x0011)
 #define CM_REJ_ATTR_ID		cpu_to_be16(0x0012)
@@ -273,6 +274,19 @@ struct ib_cm_event {
 #define CM_SIDR_REP_ATTR_ID	cpu_to_be16(0x0018)
 #define CM_LAP_ATTR_ID		cpu_to_be16(0x0019)
 #define CM_APR_ATTR_ID		cpu_to_be16(0x001A)
+#else
+#define CM_REQ_ATTR_ID		__constant_htons(0x0010)
+#define CM_MRA_ATTR_ID		__constant_htons(0x0011)
+#define CM_REJ_ATTR_ID		__constant_htons(0x0012)
+#define CM_REP_ATTR_ID		__constant_htons(0x0013)
+#define CM_RTU_ATTR_ID		__constant_htons(0x0014)
+#define CM_DREQ_ATTR_ID		__constant_htons(0x0015)
+#define CM_DREP_ATTR_ID		__constant_htons(0x0016)
+#define CM_SIDR_REQ_ATTR_ID	__constant_htons(0x0017)
+#define CM_SIDR_REP_ATTR_ID	__constant_htons(0x0018)
+#define CM_LAP_ATTR_ID		__constant_htons(0x0019)
+#define CM_APR_ATTR_ID		__constant_htons(0x001A)
+#endif
 
 /**
  * ib_cm_handler - User-defined callback to process communication events.
diff --git a/include/rdma/ib_pma.h b/include/rdma/ib_pma.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_pma.h
+++ b/include/rdma/ib_pma.h
@@ -40,6 +40,7 @@
 /*
  * PMA class portinfo capability mask bits
  */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #define IB_PMA_CLASS_CAP_ALLPORTSELECT  cpu_to_be16(1 << 8)
 #define IB_PMA_CLASS_CAP_EXT_WIDTH      cpu_to_be16(1 << 9)
 #define IB_PMA_CLASS_CAP_XMIT_WAIT      cpu_to_be16(1 << 12)
@@ -50,6 +51,17 @@
 #define IB_PMA_PORT_COUNTERS            cpu_to_be16(0x0012)
 #define IB_PMA_PORT_COUNTERS_EXT        cpu_to_be16(0x001D)
 #define IB_PMA_PORT_SAMPLES_RESULT_EXT  cpu_to_be16(0x001E)
+#else
+#define IB_PMA_CLASS_CAP_ALLPORTSELECT  0x0001
+#define IB_PMA_CLASS_CAP_EXT_WIDTH      0x0002
+#define IB_PMA_CLASS_CAP_XMIT_WAIT      0x0010
+#define IB_PMA_CLASS_PORT_INFO          0x0100
+#define IB_PMA_PORT_SAMPLES_CONTROL     0x1000
+#define IB_PMA_PORT_SAMPLES_RESULT      0x1100
+#define IB_PMA_PORT_COUNTERS            0x1200
+#define IB_PMA_PORT_COUNTERS_EXT        0x1d00
+#define IB_PMA_PORT_SAMPLES_RESULT_EXT  0x1e00
+#endif
 
 struct ib_pma_mad {
 	struct ib_mad_hdr mad_hdr;
diff --git a/include/rdma/ib_smi.h b/include/rdma/ib_smi.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_smi.h
+++ b/include/rdma/ib_smi.h
@@ -38,6 +38,7 @@
 #define IB_SMI_H
 
 #include <rdma/ib_mad.h>
+#include <asm/byteorder.h>
 
 #define IB_SMP_DATA_SIZE			64
 #define IB_SMP_MAX_PATH_HOPS			64
@@ -66,6 +67,7 @@ struct ib_smp {
 #define IB_SMP_DIRECTION			cpu_to_be16(0x8000)
 
 /* Subnet management attributes */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #define IB_SMP_ATTR_NOTICE			cpu_to_be16(0x0002)
 #define IB_SMP_ATTR_NODE_DESC			cpu_to_be16(0x0010)
 #define IB_SMP_ATTR_NODE_INFO			cpu_to_be16(0x0011)
@@ -82,6 +84,24 @@ struct ib_smp {
 #define IB_SMP_ATTR_VENDOR_DIAG			cpu_to_be16(0x0030)
 #define IB_SMP_ATTR_LED_INFO			cpu_to_be16(0x0031)
 #define IB_SMP_ATTR_VENDOR_MASK			cpu_to_be16(0xFF00)
+#else
+#define IB_SMP_ATTR_NOTICE			0x0200
+#define IB_SMP_ATTR_NODE_DESC			0x1000
+#define IB_SMP_ATTR_NODE_INFO			0x1100
+#define IB_SMP_ATTR_SWITCH_INFO			0x1200
+#define IB_SMP_ATTR_GUID_INFO			0x1400
+#define IB_SMP_ATTR_PORT_INFO			0x1500
+#define IB_SMP_ATTR_PKEY_TABLE			0x1600
+#define IB_SMP_ATTR_SL_TO_VL_TABLE		0x1700
+#define IB_SMP_ATTR_VL_ARB_TABLE		0x1800
+#define IB_SMP_ATTR_LINEAR_FORWARD_TABLE	0x1900
+#define IB_SMP_ATTR_RANDOM_FORWARD_TABLE	0x1a00
+#define IB_SMP_ATTR_MCAST_FORWARD_TABLE		0x1b00
+#define IB_SMP_ATTR_SM_INFO			0x2000
+#define IB_SMP_ATTR_VENDOR_DIAG			0x3000
+#define IB_SMP_ATTR_LED_INFO			0x3100
+#define IB_SMP_ATTR_VENDOR_MASK			0x00ff
+#endif
 
 struct ib_port_info {
 	__be64 mkey;
diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -36,9 +36,12 @@
 #include <linux/list.h>
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <rdma/ib_peer_mem.h>
+#endif
 
 struct ib_ucontext;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 struct ib_umem_odp;
 struct ib_umem;
 
@@ -56,6 +59,7 @@ struct invalidation_ctx {
 	int peer_invalidated;
 	struct completion comp;
 };
+#endif
 
 struct ib_umem {
 	struct ib_ucontext     *context;
@@ -64,12 +68,16 @@ struct ib_umem {
 	int			page_size;
 	int                     writable;
 	int                     hugetlb;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	struct list_head        chunk_list;
+#endif
 	struct work_struct	work;
 #if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	struct pid             *pid;
 #endif
 	struct mm_struct       *mm;
 	unsigned long		diff;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct ib_umem_odp     *odp_data;
 	struct sg_table sg_head;
 	int             nmap;
@@ -80,6 +88,7 @@ struct ib_umem {
 	int peer_mem_srcu_key;
 	/* peer memory private context */
 	void *peer_mem_client_context;
+#endif
 };
 
 /* Returns the offset of the umem start relative to the first page. */
@@ -105,6 +114,7 @@ static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 	return (ib_umem_end(umem) - ib_umem_start(umem)) >> PAGE_SHIFT;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 /* contiguous memory structure */
 struct ib_cmem {
 
@@ -133,19 +143,29 @@ struct ib_cmem_block {
 	*/
 	struct page            *page;
 };
-
+#else
+struct ib_umem_chunk {
+	struct list_head	list;
+	int                     nents;
+	int                     nmap;
+	struct scatterlist      page_list[0];
+};
+#endif
+ /* Returns the offset of the umem start relative to the first page. */
 
 
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
 struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
 			    size_t size, int access, int dmasync);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 struct ib_umem *ib_umem_get_ex(struct ib_ucontext *context, unsigned long addr,
 			    size_t size, int access, int dmasync,
 			    int invalidation_supported);
 void  ib_umem_activate_invalidation_notifier(struct ib_umem *umem,
 					       umem_invalidate_func_t func,
 					       void *cookie);
+#endif
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
 int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
@@ -153,6 +173,7 @@ int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
 
 int ib_umem_map_to_vma(struct ib_umem *umem,
 				struct vm_area_struct *vma);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 int ib_cmem_map_contiguous_pages_to_vma(struct ib_cmem *ib_cmem,
 	struct vm_area_struct *vma);
 struct ib_cmem *ib_cmem_alloc_contiguous_pages(struct ib_ucontext *context,
@@ -162,6 +183,7 @@ struct ib_cmem *ib_cmem_alloc_contiguous_pages(struct ib_ucontext *context,
 void ib_cmem_release_contiguous_pages(struct ib_cmem *cmem);
 int ib_umem_map_to_vma(struct ib_umem *umem,
 				struct vm_area_struct *vma);
+#endif
 
 #else /* CONFIG_INFINIBAND_USER_MEM */
 
@@ -172,12 +194,14 @@ static inline struct ib_umem *ib_umem_get(struct ib_ucontext *context,
 					  int access, int dmasync) {
 	return ERR_PTR(-EINVAL);
 }
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static inline struct ib_umem *ib_umem_get_ex(struct ib_ucontext *context,
 					  unsigned long addr, size_t size,
 					  int access, int dmasync,
 					  int invalidation_supported) {
 	return ERR_PTR(-EINVAL);
 }
+#endif
 static inline void ib_umem_release(struct ib_umem *umem) { }
 static inline int ib_umem_page_count(struct ib_umem *umem) { return 0; }
 static inline int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
@@ -185,9 +209,11 @@ static inline int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offs
 	return -EINVAL;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static inline void ib_umem_activate_invalidation_notifier(struct ib_umem *umem,
 					       umem_invalidate_func_t func,
 					       void *cookie) {return; }
+#endif
 #endif /* CONFIG_INFINIBAND_USER_MEM */
 
 #endif /* IB_UMEM_H */
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -60,7 +60,9 @@
 #include <linux/pci.h>
 
 #include <linux/atomic.h>
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 #include <linux/mmu_notifier.h>
+#endif
 #include <asm/uaccess.h>
 
 extern struct workqueue_struct *ib_wq;
@@ -2053,6 +2055,9 @@ struct ib_device {
 
 	struct module               *owner;
 	struct device                dev;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	struct class_device          class_dev;
+#endif
 	struct kobject               *ports_parent;
 	struct list_head             port_list;
 
@@ -2625,7 +2630,7 @@ static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->mapping_error(dev, dma_addr);
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 	return dma_mapping_error(dev->dma_device, dma_addr);
 #else
 	return dma_mapping_error(dma_addr);
diff --git a/include/rdma/peer_mem.h b/include/rdma/peer_mem.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/peer_mem.h
+++ b/include/rdma/peer_mem.h
@@ -38,6 +38,9 @@
 #include <linux/slab.h>
 #include <linux/errno.h>
 #include <linux/export.h>
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+#include <linux/scatterlist.h>
+#endif
 
 
 #define IB_PEER_MEMORY_NAME_MAX 64
