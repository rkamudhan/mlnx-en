From: Eugenia Emantayev <eugenia@mellanox.com>
Subject: [PATCH] BACKPORT-mlx5

Change-Id: Ica7de741b12f341cef85b65b0b0e090e1c00587e
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/infiniband/hw/mlx5/main.c                  |  12 ++
 drivers/infiniband/hw/mlx5/mr.c                    |  15 ++
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      |  48 +++++
 drivers/net/ethernet/mellanox/mlx5/core/en.h       |  97 +++++++++-
 .../net/ethernet/mellanox/mlx5/core/en_dcb_nl.c    |   2 +
 .../net/ethernet/mellanox/mlx5/core/en_eswitch.c   |  29 ++-
 .../net/ethernet/mellanox/mlx5/core/en_ethtool.c   | 115 +++++++++++-
 .../ethernet/mellanox/mlx5/core/en_flow_table.c    |  43 ++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_main.c  | 209 ++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c    |  60 +++++-
 .../net/ethernet/mellanox/mlx5/core/en_selftest.c  |   6 +
 drivers/net/ethernet/mellanox/mlx5/core/en_tx.c    |  26 ++-
 drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c  |  11 ++
 drivers/net/ethernet/mellanox/mlx5/core/main.c     |  36 +++-
 .../net/ethernet/mellanox/mlx5/core/mlx5_core.h    |   2 +
 drivers/net/ethernet/mellanox/mlx5/core/sriov.c    |  12 +-
 include/linux/mlx5/driver.h                        |   5 +
 17 files changed, 707 insertions(+), 21 deletions(-)

--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -953,6 +953,7 @@ static int get_pg_order(unsigned long of
 	return get_arg(offset);
 }
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void  mlx5_ib_vma_open(struct vm_area_struct *area)
 {
 	/* vma_open is called when a new VMA is created on top of our VMA.
@@ -1058,6 +1059,15 @@ static void mlx5_ib_set_vma_data(struct
 
 	list_add(&vma_prv->list, vma_head);
 }
+#else
+static void mlx5_ib_set_vma_data(struct vm_area_struct *vma,
+				 struct mlx5_ib_ucontext *ctx,
+				 struct mlx5_ib_vma_private_data *vma_prv)
+{
+	/* In case vma->vm_ops is not supported just free the vma_prv */
+	kfree(vma_prv);
+}
+#endif /* defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED) */
 
 static inline bool mlx5_writecombine_available(void)
 {
@@ -3512,7 +3522,9 @@ static void *mlx5_ib_add(struct mlx5_cor
 	dev->ib_dev.check_mr_status	= mlx5_ib_check_mr_status;
 	dev->ib_dev.alloc_indir_reg_list = mlx5_ib_alloc_indir_reg_list;
 	dev->ib_dev.free_indir_reg_list  = mlx5_ib_free_indir_reg_list;
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	dev->ib_dev.disassociate_ucontext = mlx5_ib_disassociate_ucontext;
+#endif
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	dev->ib_dev.exp_prefetch_mr	= mlx5_ib_prefetch_mr;
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -38,6 +38,9 @@
 #include <linux/delay.h>
 #include <linux/device.h>
 #include <linux/sysfs.h>
+#ifndef ARCH_KMALLOC_MINALIGN
+#include <linux/crypto.h>
+#endif
 #include <rdma/ib_umem.h>
 #include <rdma/ib_umem_odp.h>
 #include <rdma/ib_verbs.h>
@@ -2039,7 +2042,11 @@ static ssize_t order_attr_store(struct k
 	return oa->store(co, oa, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops order_sysfs_ops = {
+#else
+static struct sysfs_ops order_sysfs_ops = {
+#endif
 	.show = order_attr_show,
 	.store = order_attr_store,
 };
@@ -2177,7 +2184,11 @@ static ssize_t cache_attr_store(struct k
 	return ca->store(dev, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cache_sysfs_ops = {
+#else
+static  struct sysfs_ops cache_sysfs_ops = {
+#endif
 	.show = cache_attr_show,
 	.store = cache_attr_store,
 };
@@ -2282,7 +2293,11 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 	}
 
 	dsize = sizeof(*mirl->klms) * max_indir_list_len;
+#ifdef ARCH_KMALLOC_MINALIGN
 	dsize += max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);
+#else
+	dsize += max_t(int, MLX5_UMR_ALIGN - CRYPTO_MINALIGN, 0);
+#endif
 	mirl->mapped_ilist = kzalloc(dsize, GFP_KERNEL);
 	if (!mirl->mapped_ilist) {
 		err = -ENOMEM;
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -835,7 +835,11 @@ static void cmd_work_handler(struct work
 	lay->status_own = CMD_OWNER_HW;
 	set_signature(ent, !cmd->checksum_disabled);
 	dump_command(dev, ent, 1);
+#ifdef HAVE_KTIME_GET_NS
 	ent->ts1 = ktime_get_ns();
+#else
+	ktime_get_ts(&ent->ts1);
+#endif
 
 	/* ring doorbell after the descriptor is valid */
 	mlx5_core_dbg(dev, "writing 0x%x to command doorbell\n", 1 << ent->idx);
@@ -936,6 +940,9 @@ static int mlx5_cmd_invoke(struct mlx5_c
 	struct mlx5_cmd *cmd = &dev->cmd;
 	struct mlx5_cmd_work_ent *ent;
 	struct mlx5_cmd_stats *stats;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	int err = 0;
 	s64 ds;
 	u16 op;
@@ -965,7 +972,14 @@ static int mlx5_cmd_invoke(struct mlx5_c
 		if (err == -ETIMEDOUT)
 			goto out;
 
+#ifdef HAVE_KTIME_GET_NS
 		ds = ent->ts2 - ent->ts1;
+#else
+		t1 = timespec_to_ktime(ent->ts1);
+		t2 = timespec_to_ktime(ent->ts2);
+		delta = ktime_sub(t2, t1);
+		ds = ktime_to_ns(delta);
+#endif
 		op = be16_to_cpu(((struct mlx5_inbox_hdr *)in->first.data)->opcode);
 		if (op < ARRAY_SIZE(cmd->stats)) {
 			stats = &cmd->stats[op];
@@ -1430,6 +1444,9 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 	void *context;
 	int err;
 	int i;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	s64 ds;
 	struct mlx5_cmd_stats *stats;
 	unsigned long flags;
@@ -1446,7 +1463,11 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 				sem = &cmd->pages_sem;
 			else
 				sem = &cmd->sem;
+#ifdef HAVE_KTIME_GET_NS
 			ent->ts2 = ktime_get_ns();
+#else
+			ktime_get_ts(&ent->ts2);
+#endif
 			memcpy(ent->out->first.data, ent->lay->out, sizeof(ent->lay->out));
 			dump_command(dev, ent, 0);
 			if (!ent->ret) {
@@ -1465,7 +1486,14 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 			free_ent(cmd, ent->idx);
 
 			if (ent->callback) {
+#ifdef HAVE_KTIME_GET_NS
 				ds = ent->ts2 - ent->ts1;
+#else
+				t1 = timespec_to_ktime(ent->ts1);
+				t2 = timespec_to_ktime(ent->ts2);
+				delta = ktime_sub(t2, t1);
+				ds = ktime_to_ns(delta);
+#endif
 				if (ent->op < ARRAY_SIZE(cmd->stats)) {
 					stats = &cmd->stats[ent->op];
 					spin_lock_irqsave(&stats->lock, flags);
@@ -2012,7 +2040,11 @@ static ssize_t num_ent_store(struct mlx5
 	u32 var;
 	int i;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	spin_lock_irqsave(&ch->lock, flags);
@@ -2070,7 +2102,11 @@ static ssize_t miss_store(struct mlx5_cm
 	unsigned long flags;
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2106,7 +2142,11 @@ static ssize_t total_commands_store(stru
 	unsigned long flags;
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2165,7 +2205,11 @@ static ssize_t real_miss_store(struct de
 	struct mlx5_core_dev *cdev = pci_get_drvdata(pdev);
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var) {
@@ -2178,7 +2222,11 @@ static ssize_t real_miss_store(struct de
 	return count;
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cmd_cache_sysfs_ops = {
+#else
+static struct sysfs_ops cmd_cache_sysfs_ops = {
+#endif
 	.show = cmd_cache_attr_show,
 	.store = cmd_cache_attr_store,
 };
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -37,6 +37,12 @@
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/transobj.h>
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#include <linux/inet_lro.h>
+#else
+#include <net/ip.h>
+#endif
+
 #include "linux/mlx5/vport.h"
 #include "wq.h"
 #include "mlx5_core.h"
@@ -85,6 +91,12 @@ do {
 			    ##__VA_ARGS__);                     \
 } while (0)
 
+#if !defined(HAVE_NETDEV_EXTENDED_HW_FEATURES)     && \
+    !defined(HAVE_NETDEV_OPS_EXT_NDO_FIX_FEATURES) && \
+    !defined(HAVE_NETDEV_OPS_EXT_NDO_SET_FEATURES)
+#define LEGACY_ETHTOOL_OPS
+#endif
+
 enum {
 	MLX5E_LINK_SPEED,
 	MLX5E_LINK_STATE,
@@ -140,6 +152,10 @@ static const char vport_strings[][ETH_GS
 	"tx_queue_wake",
 	"tx_queue_dropped",
 	"rx_wqe_err",
+	/* port statistics */
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	"sw_lro_aggregated", "sw_lro_flushed", "sw_lro_no_desc",
+#endif
 };
 
 struct mlx5e_vport_stats {
@@ -180,9 +196,22 @@ struct mlx5e_vport_stats {
 	u64 tx_queue_dropped;
 	u64 rx_wqe_err;
 
+	/* SW LRO statistics */    
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	u64 sw_lro_aggregated;
+	u64 sw_lro_flushed;
+	u64 sw_lro_no_desc; 
+#define NUM_VPORT_COUNTERS     36
+#else
 #define NUM_VPORT_COUNTERS     33
+#endif
 };
 
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+/* Minimum packet number till arming the CQ */
+#define MLX5_EN_MIN_RX_ARM	2097152
+#endif
+
 static const char pport_strings[][ETH_GSTRING_LEN] = {
 	/* IEEE802.3 counters */
 	"frames_tx",
@@ -402,9 +431,29 @@ struct mlx5e_cq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
-	/* to be added in future commits */
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	"hw_lro",
+#endif
+};
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define IS_HW_LRO(priv) \
+	(priv->params.lro_en && (priv->pflags & MLX5E_PRIV_FLAG_HWLRO))
+#define IS_SW_LRO(priv) \
+	(priv->params.lro_en && !(priv->pflags & MLX5E_PRIV_FLAG_HWLRO))
+
+/* SW LRO defines for MLX5 */
+#define MLX5E_RQ_FLAG_SWLRO	(1<<0)
+
+#define MLX5E_LRO_MAX_DESC	32
+struct mlx5e_sw_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX5E_LRO_MAX_DESC];
 };
+#endif
 
 struct mlx5e_rq {
 	/* data path */
@@ -426,6 +475,9 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_sw_lro sw_lro;
+#endif
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
@@ -517,7 +569,9 @@ struct mlx5e_channel {
 
 	/* data path - accessed per napi poll */
 	struct irq_desc           *irq_desc;
-
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	u32 tot_rx;
+#endif
 	/* control */
 	struct mlx5e_priv         *priv;
 	int                        ix;
@@ -669,11 +723,18 @@ struct mlx5e_ecn_enable_ctx {
 
 #define MLX5E_NIC_DEFAULT_PRIO	0
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define MLX5E_PRIV_FLAG_HWLRO (1<<0)
+#endif
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	int                        default_vlan_prio;
 	struct mlx5e_sq            **txq_to_sq_map;
 	int tc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	struct vlan_group          *vlan_grp;
+#endif
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -703,7 +764,13 @@ struct mlx5e_priv {
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	u32                        pflags;
+#endif
+#ifndef HAVE_NDO_GET_STATS64
+	struct net_device_stats    netdev_stats;
+#endif
+
 	int                        counter_set_id;
 
 	struct dentry *dfs_root;
@@ -729,8 +796,17 @@ struct mlx5e_rx_wqe {
 };
 
 void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback);
+#else
+		       void *accel_priv);
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb);
+#endif
+
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
@@ -767,10 +843,22 @@ void mlx5e_init_eth_addr(struct mlx5e_pr
 void mlx5e_set_rx_mode_core(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#endif
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#endif
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv);
@@ -826,4 +914,9 @@ static inline void mlx5e_cq_arm(struct m
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
+#ifdef HAVE_ETHTOOL_OPS_EXT
+extern const struct ethtool_ops_ext mlx5e_ethtool_ops_ext;
+#endif
+#ifdef HAVE_IEEE_DCBNL_ETS
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_dcb_nl.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_dcb_nl.c
@@ -37,6 +37,7 @@
 #define MLX5E_GBPS_TO_KBPS 1000000
 #define MLX5E_100MBPS_TO_KBPS 100000
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 static int mlx5e_dbcnl_validate_ets(struct ieee_ets *ets)
 {
 	int bw_sum = 0;
@@ -181,3 +182,4 @@ const struct dcbnl_rtnl_ops mlx5e_dcbnl_
 	.getdcbx	= mlx5e_dcbnl_getdcbx,
 	.setdcbx	= mlx5e_dcbnl_setdcbx,
 };
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_eswitch.c
@@ -86,17 +86,23 @@ struct mlx5_l2_addr_node {
 	struct hlist_node hlist;
 	u8                addr[ETH_ALEN];
 };
-
 #define mlx5_for_each_hash_node(hn, tmp, hash, i) \
 	for (i = 0; i < MLX5_L2_ADDR_HASH_SIZE; i++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+#define COMPAT_HL_NODE struct hlist_node *hlnode;
+#else
+#define COMPAT_HL_NODE
+#endif
 
 #define mlx5_addr_hash_find(hash, mac, type) ({            \
 	struct mlx5_l2_addr_node *hn;                      \
 	int ix = MLX5_L2_ADDR_HASH(mac);                   \
 	bool found = false;                                \
+	COMPAT_HL_NODE                                     \
 							   \
-	hlist_for_each_entry(hn, &hash[ix], hlist)         \
+	compat_hlist_for_each_entry(hn, &hash[ix], hlist)  \
 		if (ether_addr_equal(hn->addr, mac)) {     \
 			found = true;                      \
 			break;                             \
@@ -104,7 +110,7 @@ struct mlx5_l2_addr_node {
 	if (!found) {                                      \
 		hn = NULL;                                 \
 	}                                                  \
-	(type *)hn;                                         \
+	(type *)hn;                                        \
 })
 
 #define mlx5_addr_hash_add(hash, mac, type, gfp) ({        \
@@ -807,6 +813,9 @@ static void mlx5_esw_apply_vport_addr_li
 	struct mlx5_l2_addr_node *node;
 	struct mlx5_vport_addr *addr;
 	struct hlist_head *hash;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	vport_addr_action vport_addr_add;
 	vport_addr_action vport_addr_del;
@@ -841,6 +850,9 @@ static void mlx5_esw_update_vport_addr_l
 	struct mlx5_l2_addr_node *node;
 	struct mlx5_vport_addr *addr;
 	struct hlist_head *hash;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	u8 (*mac_list)[ETH_ALEN];
 	int list_size;
@@ -945,6 +957,9 @@ static void mlx5_esw_cleanup_vport(struc
 	struct mlx5_vport *vport = &dev->priv.eswitch.vports[vport_num];
 	struct mlx5_l2_addr_node *node;
 	struct mlx5_vport_addr *addr;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	int hi;
 
@@ -1254,6 +1269,7 @@ int mlx5_eswitch_set_vport_link_state(st
 					     vport, link_state);
 }
 
+#ifdef HAVE_NDO_SET_VF_MAC
 int mlx5_eswitch_get_vport_config(struct mlx5_core_dev *dev,
 				  int vport, struct ifla_vf_info *ivi)
 {
@@ -1269,16 +1285,21 @@ int mlx5_eswitch_get_vport_config(struct
 	ivi->vf = vport - 1;
 
 	mlx5_query_nic_vport_mac_address(dev, vport, ivi->mac);
+#ifdef HAVE_LINKSTATE
 	ivi->linkstate = mlx5_query_vport_admin_state(dev,
 						      MLX5_QUERY_VPORT_STATE_IN_OP_MOD_ESW_VPORT,
 						      vport);
+#endif
 	mlx5_query_esw_vport_cvlan(dev, vport, &vlan, &qos);
 	ivi->vlan = vlan;
 	ivi->qos = qos;
+#ifdef HAVE_VF_INFO_SPOOFCHK
 	ivi->spoofchk = 0;
+#endif
 
 	return 0;
 }
+#endif
 
 int mlx5_eswitch_set_vport_vlan(struct mlx5_core_dev *dev,
 				int vport, u16 vlan, u8 qos)
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@ -205,8 +205,10 @@ static int mlx5e_get_sset_count(struct n
 		       NUM_Q_COUNTERS;
 	case ETH_SS_TEST:
 		return MLX5E_NUM_SELF_TEST;
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		return ARRAY_SIZE(mlx5e_priv_flags);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -219,12 +221,13 @@ static void mlx5e_get_strings(struct net
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
 	switch (stringset) {
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		for (i = 0; i < ARRAY_SIZE(mlx5e_priv_flags); i++)
 			strcpy(data + i * ETH_GSTRING_LEN,
 			       mlx5e_priv_flags[i]);
 		break;
-
+#endif
 	case ETH_SS_TEST:
 		for (i = 0; i < MLX5E_NUM_SELF_TEST; i++)
 			strcpy(data + i * ETH_GSTRING_LEN, mlx5e_test_names[i]);
@@ -376,6 +379,7 @@ static int mlx5e_set_ringparam(struct ne
 	return err;
 }
 
+#if defined(HAVE_GET_SET_CHANNELS) || defined(HAVE_GET_SET_CHANNELS_EXT)
 static void mlx5e_get_channels(struct net_device *dev,
 			       struct ethtool_channels *ch)
 {
@@ -422,6 +426,7 @@ static int mlx5e_set_channels(struct net
 
 	return err;
 }
+#endif
 
 static int mlx5e_get_coalesce(struct net_device *netdev,
 			      struct ethtool_coalesce *coal)
@@ -878,20 +883,35 @@ static void mlx5e_set_msglevel(struct ne
 	((struct mlx5e_priv *)netdev_priv(dev))->msg_level = val;
 }
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 static int mlx5e_set_priv_flags(struct net_device *dev, u32 flags)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 	u32 changes = flags ^ priv->pflags;
+#endif
 	struct mlx5e_params new_params;
 	bool update_params = false;
 
 	mutex_lock(&priv->state_lock);
 	new_params = priv->params;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (changes & MLX5E_PRIV_FLAG_HWLRO) {
+		priv->pflags ^= MLX5E_PRIV_FLAG_HWLRO;
+		if (!test_bit(MLX5E_STATE_OPENED, &priv->state))
+			goto out;
+		if (priv->params.lro_en)
+			update_params = true;
+	}
+#endif
+
 	/* will be added on future commits */
 	if (update_params)
 		mlx5e_update_priv_params(priv, &new_params);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 out:
+#endif
 	mutex_unlock(&priv->state_lock);
 	return !(flags == priv->pflags);
 }
@@ -902,6 +922,66 @@ static u32 mlx5e_get_priv_flags(struct n
 
 	return priv->pflags;
 }
+#endif
+
+#ifdef LEGACY_ETHTOOL_OPS
+#if (defined(HAVE_GET_SET_FLAGS) || defined(HAVE_GET_SET_FLAGS_EXT))
+static int mlx5e_set_flags(struct net_device *dev, u32 data)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	struct mlx5e_params new_params;
+	u32 changes = data ^ priv->netdev->flags;
+	bool update_params = false;
+
+	mutex_lock(&priv->state_lock);
+
+	new_params = priv->params;
+
+	if (changes & ETH_FLAG_LRO) {
+		new_params.lro_en = !new_params.lro_en;
+		update_params = true;
+	}
+
+	if (!update_params)
+		goto out;
+
+	mlx5e_update_priv_params(priv, &new_params);
+
+	if (priv->params.lro_en) {
+		dev->features |= NETIF_F_LRO;
+		priv->netdev->flags |= NETIF_F_LRO;
+	} else {
+		dev->features &= ~NETIF_F_LRO;
+		priv->netdev->flags &= ~NETIF_F_LRO;
+	}
+
+out:
+	mutex_unlock(&priv->state_lock);
+	return 0;
+}
+
+static u32 mlx5e_get_flags(struct net_device *dev)
+{
+	return ((struct mlx5e_priv *)netdev_priv(dev))->netdev->flags;
+}
+#endif
+
+#if (defined(HAVE_GET_SET_TSO) || defined(HAVE_GET_SET_TSO_EXT))
+static u32 mlx5e_get_tso(struct net_device *dev)
+{
+       return (dev->features & NETIF_F_TSO) != 0;
+}
+
+static int mlx5e_set_tso(struct net_device *dev, u32 data)
+{
+       if (data)
+               dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+       else
+               dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+       return 0;
+}
+#endif
+#endif
 
 const struct ethtool_ops mlx5e_ethtool_ops = {
 	.get_drvinfo       = mlx5e_get_drvinfo,
@@ -914,8 +994,10 @@ const struct ethtool_ops mlx5e_ethtool_o
 	.set_msglevel      = mlx5e_set_msglevel,
 	.get_ringparam     = mlx5e_get_ringparam,
 	.set_ringparam     = mlx5e_set_ringparam,
+#ifdef HAVE_GET_SET_CHANNELS
 	.get_channels      = mlx5e_get_channels,
 	.set_channels      = mlx5e_set_channels,
+#endif
 	.get_coalesce      = mlx5e_get_coalesce,
 	.set_coalesce      = mlx5e_set_coalesce,
 	.get_settings      = mlx5e_get_settings,
@@ -924,6 +1006,37 @@ const struct ethtool_ops mlx5e_ethtool_o
 	.get_pauseparam    = mlx5e_get_pauseparam,
 	.get_wol	   = mlx5e_get_wol,
 	.set_wol	   = mlx5e_set_wol,
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	.get_priv_flags	   = mlx5e_get_priv_flags,
 	.set_priv_flags	   = mlx5e_set_priv_flags,
+#endif
+#ifdef LEGACY_ETHTOOL_OPS
+#if defined(HAVE_GET_SET_FLAGS)
+	.get_flags	   = mlx5e_get_flags,
+	.set_flags	   = mlx5e_set_flags,
+#endif
+#if defined(HAVE_GET_SET_TSO)
+	.get_tso	   = mlx5e_get_tso,
+	.set_tso	   = mlx5e_set_tso,
+#endif
+#endif
+};
+
+#ifdef HAVE_ETHTOOL_OPS_EXT
+const struct ethtool_ops_ext mlx5e_ethtool_ops_ext = {
+	.size		   = sizeof(struct ethtool_ops_ext),
+#ifdef HAVE_GET_SET_CHANNELS_EXT
+	.get_channels	   = mlx5e_get_channels,
+	.set_channels	   = mlx5e_set_channels,
+#endif
+#if defined(HAVE_GET_SET_FLAGS_EXT)
+	.get_flags	   = mlx5e_get_flags,
+	.set_flags	   = mlx5e_set_flags,
+#endif
+#if defined(HAVE_GET_SET_TSO_EXT)
+	.get_tso	   = mlx5e_get_tso,
+	.set_tso	   = mlx5e_set_tso,
+#endif
 };
+#endif
+
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_flow_table.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_flow_table.c
@@ -74,8 +74,11 @@ static void mlx5e_add_eth_addr_to_hash(s
 	struct mlx5e_eth_addr_hash_node *hn;
 	int ix = mlx5e_hash_eth_addr(addr);
 	int found = 0;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
-	hlist_for_each_entry(hn, &hash[ix], hlist)
+	compat_hlist_for_each_entry(hn, &hash[ix], hlist)
 		if (ether_addr_equal_64bits(hn->ai.addr, addr)) {
 			found = 1;
 			break;
@@ -618,8 +621,14 @@ void mlx5e_disable_vlan_filter(struct ml
 	}
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int err = 0;
@@ -632,11 +641,20 @@ int mlx5e_vlan_rx_add_vid(struct net_dev
 					  vid);
 	mutex_unlock(&priv->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
@@ -647,7 +665,10 @@ int mlx5e_vlan_rx_kill_vid(struct net_de
 
 	mutex_unlock(&priv->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return 0;
+#endif
 }
 
 int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv)
@@ -691,7 +712,7 @@ void mlx5e_del_all_vlan_rules(struct mlx
 
 #define mlx5e_for_each_hash_node(hn, tmp, hash, i) \
 	for (i = 0; i < MLX5E_ETH_ADDR_HASH_SIZE; i++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
 
 static void mlx5e_execute_action(struct mlx5e_priv *priv,
 				 struct mlx5e_eth_addr_hash_node *hn)
@@ -713,6 +734,9 @@ static void mlx5e_sync_netdev_addr(struc
 {
 	struct net_device *netdev = priv->netdev;
 	struct netdev_hw_addr *ha;
+#ifndef HAVE_NETDEV_FOR_EACH_MC_ADDR
+	struct dev_mc_list *mclist;
+#endif
 
 	netif_addr_lock_bh(netdev);
 
@@ -722,8 +746,14 @@ static void mlx5e_sync_netdev_addr(struc
 	netdev_for_each_uc_addr(ha, netdev)
 		mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_uc, ha->addr);
 
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, netdev)
 		mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_mc, ha->addr);
+#else
+	for (mclist = netdev->mc_list; mclist; mclist = mclist->next)
+		mlx5e_add_eth_addr_to_hash(priv->eth_addr.netdev_mc,
+					   mclist->dmi_addr);
+#endif
 
 	netif_addr_unlock_bh(netdev);
 }
@@ -735,6 +765,9 @@ static void mlx5e_vport_context_update_a
 	struct net_device *ndev = priv->netdev;
 	struct mlx5e_eth_addr_hash_node *hn;
 	struct hlist_head *addr_list;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	u8 (*mac_list)[ETH_ALEN];
 	int max_list_size;
@@ -805,6 +838,9 @@ static void mlx5e_vport_context_update(s
 static void mlx5e_apply_netdev_addr(struct mlx5e_priv *priv)
 {
 	struct mlx5e_eth_addr_hash_node *hn;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	int i;
 
@@ -818,6 +854,9 @@ static void mlx5e_apply_netdev_addr(stru
 static void mlx5e_handle_netdev_addr(struct mlx5e_priv *priv)
 {
 	struct mlx5e_eth_addr_hash_node *hn;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	struct hlist_node *tmp;
 	int i;
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -129,6 +129,27 @@ free_out:
 	kvfree(out);
 }
 
+ 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void mlx5e_update_sw_lro_stats(struct mlx5e_priv *priv)
+{
+	int i;
+	struct mlx5e_vport_stats *s = &priv->stats.vport;
+
+	s->sw_lro_aggregated = 0;
+	s->sw_lro_flushed = 0;
+	s->sw_lro_no_desc = 0;
+
+	for (i = 0; i < priv->params.num_channels; i++) {
+		struct mlx5e_rq *rq = &priv->channel[i]->rq;
+
+		s->sw_lro_aggregated += rq->sw_lro.lro_mgr.stats.aggregated;
+		s->sw_lro_flushed += rq->sw_lro.lro_mgr.stats.flushed;
+		s->sw_lro_no_desc += rq->sw_lro.lro_mgr.stats.no_desc;
+	}
+}
+#endif
+
 void mlx5e_update_stats(struct mlx5e_priv *priv)
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -179,6 +200,10 @@ void mlx5e_update_stats(struct mlx5e_pri
 		}
 	}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_update_sw_lro_stats(priv);
+#endif
+
 	/* HW counters */
 	memset(in, 0, sizeof(in));
 
@@ -394,7 +419,9 @@ static int mlx5e_create_rq(struct mlx5e_
 		rq->wqe_sz = wq_sz * rq->num_of_strides_in_wqe * rq->stride_size;
 
 		priv->params.lro_en = true;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 		priv->pflags |= MLX5E_PRIV_FLAG_HWLRO;
+#endif
 		priv->netdev->features |= NETIF_F_LRO;
 		priv->netdev->flags |= NETIF_F_LRO;
 	} else {
@@ -408,8 +435,13 @@ static int mlx5e_create_rq(struct mlx5e_
 		rq->clean_rq = free_rq_res;
 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
 		rq->mlx5e_poll_specific_rx_cq = mlx5e_poll_default_rx_cq;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	rq->wqe_sz = IS_HW_LRO(priv) ? priv->params.lro_wqe_sz :
+		     MLX5E_SW2HW_MTU(priv->netdev->mtu);
+#else
 		rq->wqe_sz = (priv->params.lro_en) ? priv->params.lro_wqe_sz :
 						     MLX5E_SW2HW_MTU(priv->netdev->mtu);
+#endif
 	}
 
 	rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz + MLX5E_NET_IP_ALIGN);
@@ -545,6 +577,58 @@ static int mlx5e_wait_for_min_rx_wqes(st
 	return -ETIMEDOUT;
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int get_skb_hdr(struct sk_buff *skb, void **iphdr,
+			void **tcph, u64 *hdr_flags, void *priv)
+{
+	unsigned int ip_len;
+	struct iphdr *iph;
+
+	if (unlikely(skb->protocol != htons(ETH_P_IP)))
+		return -1;
+
+	/*
+	* In the future we may add an else clause that verifies the
+	* checksum and allows devices which do not calculate checksum
+	* to use LRO.
+	*/
+	if (unlikely(skb->ip_summed != CHECKSUM_UNNECESSARY))
+		return -1;
+
+	/* Check for non-TCP packet */
+	skb_reset_network_header(skb);
+	iph = ip_hdr(skb);
+	if (iph->protocol != IPPROTO_TCP)
+		return -1;
+
+	ip_len = ip_hdrlen(skb);
+	skb_set_transport_header(skb, ip_len);
+	*tcph = tcp_hdr(skb);
+
+	/* check if IP header and TCP header are complete */
+	if (ntohs(iph->tot_len) < ip_len + tcp_hdrlen(skb))
+		return -1;
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	*iphdr = iph;
+
+	return 0;
+}
+
+static void mlx5e_rq_sw_lro_init(struct mlx5e_rq *rq)
+{
+	rq->sw_lro.lro_mgr.max_aggr 		= 64;
+	rq->sw_lro.lro_mgr.max_desc		= MLX5E_LRO_MAX_DESC;
+	rq->sw_lro.lro_mgr.lro_arr		= rq->sw_lro.lro_desc;
+	rq->sw_lro.lro_mgr.get_skb_header	= get_skb_hdr;
+	rq->sw_lro.lro_mgr.features		= LRO_F_NAPI;
+	rq->sw_lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	rq->sw_lro.lro_mgr.dev			= rq->netdev;
+	rq->sw_lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	rq->sw_lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 static int mlx5e_open_rq(struct mlx5e_channel *c,
 			 struct mlx5e_rq_param *param,
 			 struct mlx5e_rq *rq)
@@ -559,6 +643,10 @@ static int mlx5e_open_rq(struct mlx5e_ch
 	if (err)
 		goto err_destroy_rq;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_rq_sw_lro_init(rq);
+#endif
+
 	err = mlx5e_modify_rq(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);
 	if (err)
 		goto err_disable_rq;
@@ -1001,9 +1089,13 @@ static void mlx5e_close_cq(struct mlx5e_
 
 static int mlx5e_get_cpu(struct mlx5e_priv *priv, int ix)
 {
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	cpumask_var_t affinity_mask = priv->mdev->priv.irq_info[ix].mask;
 
 	return affinity_mask ? cpumask_first(affinity_mask) : 0;
+#else                              
+	return 0;                                     
+#endif
 }
 
 static void mlx5e_build_tc_to_txq_map(struct mlx5e_priv *priv, int ix)
@@ -1122,8 +1214,15 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_sqs;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)) || \
+     defined(CONFIG_COMPAT_IS_NETIF_SET_XPS_QUEUE_NOT_CONST_CPUMASK)
+	netif_set_xps_queue(netdev, (struct cpumask *)get_cpu_mask(c->cpu), ix);
+#else
 	netif_set_xps_queue(netdev, get_cpu_mask(c->cpu), ix);
+#endif
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	c->irq_desc = irq_to_desc(priv->mdev->priv.msix_arr[c->rq.cq.mcq.irqn].vector);
+#endif
 	*cp = c;
 
 	return 0;
@@ -1454,7 +1553,11 @@ static void mlx5e_build_tir_ctx(struct m
 				 MLX5_HASH_FIELD_SEL_DST_IP   |\
 				 MLX5_HASH_FIELD_SEL_IPSEC_SPI)
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_HW_LRO(priv)) {
+#else
 	if (priv->params.lro_en) {
+#endif
 		MLX5_SET(tirc, tirc, lro_enable_mask,
 			 MLX5_TIRC_LRO_ENABLE_MASK_IPV4_LRO |
 			 MLX5_TIRC_LRO_ENABLE_MASK_IPV6_LRO);
@@ -1491,6 +1594,7 @@ static void mlx5e_build_tir_ctx(struct m
 			MLX5_SET(tirc, tirc, rx_hash_fn,
 				 MLX5_TIRC_RX_HASH_FN_HASH_TOEPLITZ);
 			MLX5_SET(tirc, tirc, rx_hash_symmetric, 1);
+
 			netdev_rss_key_fill(rss_key, len);
 		}
 		break;
@@ -1636,6 +1740,7 @@ static void mlx5e_close_tirs(struct mlx5
 
 static void mlx5e_netdev_set_tcs(struct net_device *netdev)
 {
+#ifdef HAVE_NDO_SETUP_TC
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	int nch = priv->params.num_channels;
 	int ntc = priv->params.num_tc;
@@ -1654,6 +1759,7 @@ static void mlx5e_netdev_set_tcs(struct
 
 	for (prio = 0; prio < MLX5E_MAX_NUM_PRIO; prio++)
 		netdev_set_prio_tc_map(netdev, prio, prio % ntc);
+#endif
 }
 
 static int mlx5e_set_dev_port_mtu(struct net_device *netdev)
@@ -1880,6 +1986,7 @@ int mlx5e_update_priv_params(struct mlx5
 	return err;
 }
 
+#ifdef HAVE_NDO_SETUP_TC
 static int mlx5e_setup_tc(struct net_device *netdev, u8 tc)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -1897,13 +2004,22 @@ static int mlx5e_setup_tc(struct net_dev
 
 	return err;
 }
+#endif
 
+#ifdef HAVE_NDO_GET_STATS64
 static struct rtnl_link_stats64 *
 mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+static struct net_device_stats *mlx5e_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5e_vport_stats *vstats = &priv->stats.vport;
 
+#ifndef HAVE_NDO_GET_STATS64
+	struct net_device_stats *stats = &priv->netdev_stats;
+#endif
+
 	stats->rx_packets = vstats->rx_packets;
 	stats->rx_bytes   = vstats->rx_bytes;
 	stats->tx_packets = vstats->tx_packets;
@@ -1946,8 +2062,13 @@ static int mlx5e_set_mac(struct net_devi
 	return 0;
 }
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			      u32 features)
+#else
 			      netdev_features_t features)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	netdev_features_t changes = features ^ netdev->features;
@@ -1980,6 +2101,7 @@ out:
 
 	return err;
 }
+#endif
 
 static int mlx5e_change_mtu(struct net_device *netdev, int new_mtu)
 {
@@ -2007,6 +2129,15 @@ static int mlx5e_change_mtu(struct net_d
 	return err;
 }
 
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+void mlx5e_vlan_register(struct net_device *netdev, struct vlan_group *grp)
+{
+        struct mlx5e_priv *priv = netdev_priv(netdev);
+        priv->vlan_grp = grp;
+}
+#endif
+
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2022,7 +2153,9 @@ static int mlx5e_set_vf_vlan(struct net_
 
 	return mlx5_eswitch_set_vport_vlan(mdev, vf + 1, vlan, qos);
 }
+#endif
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx5_vport_link2ifla(u8 esw_link)
 {
 	switch (esw_link) {
@@ -2054,7 +2187,9 @@ static int mlx5e_set_vf_link_state(struc
 	return mlx5_eswitch_set_vport_link_state(mdev, vf + 1,
 						 mlx5_ifla_link2vport(link_state));
 }
+#endif
 
+#if defined(HAVE_VF_INFO_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx5e_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -2062,7 +2197,9 @@ static int mlx5e_set_vf_spoofchk(struct
 
 	return mlx5_eswitch_set_vport_spoofchk(mdev, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_get_vf_config(struct net_device *dev,
 			       int vf, struct ifla_vf_info *ivi)
 {
@@ -2070,29 +2207,63 @@ static int mlx5e_get_vf_config(struct ne
 	struct mlx5_core_dev *mdev = priv->mdev;
 
 	mlx5_eswitch_get_vport_config(mdev, vf + 1, ivi);
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 	ivi->linkstate = mlx5_vport_link2ifla(ivi->linkstate);
+#endif
 	return 0;
 }
+#endif
 
 static struct net_device_ops mlx5e_netdev_ops = {
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_SETUP_TC
 	.ndo_setup_tc            = mlx5e_setup_tc,
+#endif
 	.ndo_select_queue        = mlx5e_select_queue,
+#ifdef HAVE_NDO_GET_STATS64
 	.ndo_get_stats64         = mlx5e_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_get_stats,
+#endif
 	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
 	.ndo_set_mac_address     = mlx5e_set_mac,
 	.ndo_vlan_rx_add_vid	 = mlx5e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid	 = mlx5e_vlan_rx_kill_vid,
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	.ndo_vlan_rx_register	 = mlx5e_vlan_register,
+#endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features        = mlx5e_set_features,
+#endif
 	.ndo_change_mtu		 = mlx5e_change_mtu,
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_set_vf_mac          = mlx5e_set_vf_mac,
 	.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,
 	.ndo_get_vf_config       = mlx5e_get_vf_config,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
+#endif
+};
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx5_netdev_ops_ext = {
+	.size             = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx5e_set_features,
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk	= mlx5e_set_vf_spoofchk,
+#endif
+#if defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
+	.ndo_set_vf_link_state	= mlx5e_set_vf_link_state,
+#endif
+
 };
+#endif
 
 static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
 {
@@ -2198,10 +2369,16 @@ static void mlx5e_build_netdev(struct ne
 
 	netdev->netdev_ops        = &mlx5e_netdev_ops;
 	netdev->watchdog_timeo    = 15 * HZ;
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(netdev, &mlx5e_ethtool_ops);
+	set_ethtool_ops_ext(netdev, &mlx5e_ethtool_ops_ext);
+#else
+	netdev->ethtool_ops       = &mlx5e_ethtool_ops;
+#endif
 
-	netdev->ethtool_ops	  = &mlx5e_ethtool_ops;
+#ifdef HAVE_IEEE_DCBNL_ETS
 	netdev->dcbnl_ops	  = &mlx5e_dcbnl_ops;
-
+#endif
 	netdev->vlan_features    |= NETIF_F_SG;
 	netdev->vlan_features    |= NETIF_F_IP_CSUM;
 	netdev->vlan_features    |= NETIF_F_IPV6_CSUM;
@@ -2209,22 +2386,40 @@ static void mlx5e_build_netdev(struct ne
 	netdev->vlan_features    |= NETIF_F_TSO;
 	netdev->vlan_features    |= NETIF_F_TSO6;
 	netdev->vlan_features    |= NETIF_F_RXCSUM;
+#ifdef HAVE_NETIF_F_RXHASH
 	netdev->vlan_features    |= NETIF_F_RXHASH;
+#endif
 
 	if (!!MLX5_CAP_ETH(mdev, lro_cap))
 		netdev->vlan_features    |= NETIF_F_LRO;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	netdev->hw_features       = netdev->vlan_features;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_RX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
 
 	netdev->features          = netdev->hw_features;
+#else /* HAVE_NETDEV_HW_FEATURES */
+	netdev->features       = netdev->vlan_features;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_RX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+        set_netdev_hw_features(netdev, netdev->features);
+#endif
+#endif /* HAVE_NETDEV_HW_FEATURES */
+ 
 	if (!priv->params.lro_en)
 		netdev->features  &= ~NETIF_F_LRO;
 
 	netdev->features         |= NETIF_F_HIGHDMA;
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	netdev->priv_flags       |= IFF_UNICAST_FLT;
+#endif
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	set_netdev_ops_ext(netdev, &mlx5_netdev_ops_ext);
+#endif
 
 	mlx5e_set_netdev_dev_addr(netdev);
 }
@@ -2264,10 +2459,13 @@ static void *mlx5e_create_netdev(struct
 
 	if (mlx5e_check_required_hca_cap(mdev))
 		return NULL;
-
+#ifdef HAVE_NEW_TX_RING_SCHEME
 	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
 				    ncv * MLX5E_MAX_NUM_TC,
 				    ncv);
+#else
+	netdev = alloc_etherdev_mq(sizeof(struct mlx5e_priv), ncv);
+#endif
 	if (!netdev) {
 		mlx5_core_err(mdev, "alloc_etherdev_mqs() failed\n");
 		return NULL;
@@ -2315,6 +2513,9 @@ static void *mlx5e_create_netdev(struct
 		goto err_destroy_mkey;
 	}
 
+	if (!is_valid_ether_addr(netdev->perm_addr))
+		memcpy(netdev->perm_addr, netdev->dev_addr, netdev->addr_len);
+
 	mlx5e_enable_async_events(priv);
 
 	return priv;
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -242,15 +242,21 @@ static void mlx5e_lro_update_hdr(struct
 	/* TODO: handle tcp checksum */
 }
 
+#ifdef HAVE_NETIF_F_RXHASH
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
 {
+#ifdef HAVE_SKB_SET_HASH
 	u8 cht = cqe->rss_hash_type;
 	int ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :
 		 (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
+#else
+	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
+#endif
 }
+#endif
 
 static void mlx5e_validate_loopback(struct mlx5e_priv *priv,
 				    struct sk_buff *skb)
@@ -330,12 +336,18 @@ static inline void mlx5e_build_rx_skb(st
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	if (cqe_has_vlan(cqe))
+#ifndef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
+		__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->vlan_info));
+#else
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       be16_to_cpu(cqe->vlan_info));
+#endif
 }
 
 #define SMALL_PACKET_SIZE      (256 - NET_IP_ALIGN)
@@ -392,9 +404,37 @@ static struct sk_buff *mlx5e_get_rx_skb(
 }
 
 static inline void send_skb(struct mlx5e_cq *cq, struct mlx5e_rq *rq,
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+			    struct sk_buff *skb, struct mlx5_cqe64 *prev_cqe)
+#else
 			    struct sk_buff *skb)
+#endif
 {
-	napi_gro_receive(cq->napi, skb);
+
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX || defined CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct net_device *netdev = rq->netdev;
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		if (IS_SW_LRO(priv))
+			lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+		else
+#endif
+
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+		if (priv->vlan_grp && cqe_has_vlan(prev_cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+			vlan_gro_receive(cq->napi, priv->vlan_grp,
+					 be16_to_cpu(prev_cqe->vlan_info),
+					 skb);
+#else
+			vlan_hwaccel_rx(skb, priv->vlan_grp,
+					be16_to_cpu(prev_cqe->vlan_info));
+#endif
+		else
+#endif
+			napi_gro_receive(cq->napi, skb);
 }
 
 struct sk_buff *mlx5e_poll_striding_rx_cq(struct mlx5_cqe64 *cqe,
@@ -447,6 +487,9 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_cqe64 *cqe;
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+	struct mlx5_cqe64 *prev_cqe;
+#endif
 	int i;
 
 	/* avoid accessing cq (dma coherent memory) if not needed */
@@ -486,9 +529,17 @@ bool mlx5e_poll_rx_cq(struct mlx5e_cq *c
 
 		rq->stats.packets++;
 
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+                prev_cqe = cqe;
+#endif
 		cqe = mlx5e_get_cqe(cq);
 
-		send_skb(cq, rq, skb);
+#if defined HAVE_VLAN_GRO_RECEIVE || defined HAVE_VLAN_HWACCEL_RX
+                send_skb(cq, rq, skb, prev_cqe);
+#else
+                send_skb(cq, rq, skb);
+#endif
+
 
 wq_ll_pop:
 		if (!rq->is_poll || (rq->is_poll && rq->is_poll(rq)))
@@ -505,6 +556,11 @@ wq_ll_pop:
 		set_bit(MLX5E_CQ_HAS_CQES, &cq->flags);
 		return true;
 	}
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(priv))
+		lro_flush_all(&rq->sw_lro.lro_mgr);
+#endif
+
 	return false;
 }
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_selftest.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_selftest.c
@@ -73,12 +73,16 @@ static int mlx5e_test_loopback(struct ml
 {
 	bool loopback_ok = false;
 	int i;
+#ifdef HAVE_GRO
 	bool gro_enabled;
+#endif
 
 	priv->loopback_ok = false;
 	priv->validate_loopback = true;
+#ifdef HAVE_GRO
 	gro_enabled = priv->netdev->features & NETIF_F_GRO;
 	priv->netdev->features &= ~NETIF_F_GRO;
+#endif
 
 	/* xmit */
 	if (mlx5e_test_loopback_xmit(priv)) {
@@ -102,8 +106,10 @@ static int mlx5e_test_loopback(struct ml
 mlx5e_test_loopback_exit:
 	priv->validate_loopback = false;
 
+#ifdef HAVE_GRO
 	if (gro_enabled)
 		priv->netdev->features |= NETIF_F_GRO;
+#endif
 
 	return !loopback_ok;
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -96,16 +96,32 @@ static inline void mlx5e_dma_get(struct
 	*size = sq->dma_fifo[i & sq->dma_fifo_mask].size;
 }
 
+#ifndef HAVE_SELECT_QUEUE_FALLBACK_T
+#define fallback(dev, skb) __netdev_pick_tx(dev, skb)
+#endif     
+
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback)
+#else
+		       void *accel_priv)
+#endif 
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb) 
+#endif 
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int channel_ix = fallback(dev, skb);
+#ifdef HAVE_NETDEV_GET_PRIO_TC_MAP
 	int up = skb_vlan_tag_present(skb)        ?
 		 skb->vlan_tci >> VLAN_PRIO_SHIFT :
 		 priv->default_vlan_prio;
 	int tc = netdev_get_prio_tc_map(dev, up);
-
+#else
+	/* TODO: QoS and traffic class is not fully implemented */
+	int tc = 0;
+#endif
 	return priv->tc_to_txq_map[channel_ix][tc];
 }
 
@@ -162,7 +178,9 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		sq->stats.tso_bytes += payload_len;
 	} else {
 		bf = sq->bf_budget   &&
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 		     !skb->xmit_more &&
+#endif
 		     !skb_shinfo(skb)->nr_frags;
 		ihs = mlx5e_get_inline_hdr_size(sq, skb, bf);
 		MLX5E_TX_SKB_CB(skb)->num_bytes = max_t(unsigned int, skb->len,
@@ -234,8 +252,10 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		netif_tx_stop_queue(sq->txq);
 		sq->stats.stopped++;
 	}
-
-	if (!skb->xmit_more || netif_xmit_stopped(sq->txq)) {
+#ifdef HAVE_SK_BUFF_XMIT_MORE
+	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+#endif
+	{
 		int bf_sz = 0;
 
 		if (bf && sq->uar_bf_map)
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -61,10 +61,18 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct
 
 static inline bool mlx5e_no_channel_affinity_change(struct mlx5e_channel *c)
 {
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	int current_cpu = smp_processor_id();
 	struct cpumask *aff = irq_desc_get_irq_data(c->irq_desc)->affinity;
 
 	return cpumask_test_cpu(current_cpu, aff);
+#else
+	if (c->tot_rx < MLX5_EN_MIN_RX_ARM)
+		return true;
+
+	c->tot_rx = 0;
+	return false;
+#endif
 }
 
 int mlx5e_napi_poll(struct napi_struct *napi, int budget)
@@ -83,6 +91,9 @@ int mlx5e_napi_poll(struct napi_struct *
 	for (i = 0; i < c->num_tc; i++)
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq);
 
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	c->tot_rx += budget;
+#endif
 	if (busy && likely(mlx5e_no_channel_affinity_change(c)))
 		return budget;
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -270,6 +270,9 @@ static int mlx5_enable_msix(struct mlx5_
 	int num_eqs = 1 << MLX5_CAP_GEN(dev, log_max_eq);
 	struct mlx5_priv *priv = &dev->priv;
 	int nvec;
+#ifndef HAVE_PCI_ENABLE_MSIX_RANGE
+	int err;
+#endif 
 	int i;
 
 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
@@ -289,13 +292,26 @@ static int mlx5_enable_msix(struct mlx5_
 	for (i = 0; i < nvec; i++)
 		priv->msix_arr[i].entry = i;
 
+#ifdef HAVE_PCI_ENABLE_MSIX_RANGE
 	nvec = pci_enable_msix_range(dev->pdev, priv->msix_arr,
 				     MLX5_EQ_VEC_COMP_BASE + 1, nvec);
 	if (nvec < 0)
 		return nvec;
 
 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
-
+#else
+retry:
+	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+	err = pci_enable_msix(dev->pdev, priv->msix_arr, nvec);
+	if (err <= 0) {
+		return err;
+	} else if (err > 2) {
+		nvec = err;
+		goto retry;
+	}
+	mlx5_core_dbg(dev, "received %d MSI vectors out of %d requested\n", err, nvec);
+#endif
+ 
 	return 0;
 
 err_free_msix:
@@ -618,7 +634,13 @@ static void mlx5_irq_set_affinity_hint(s
 	return;
 
 err_clear_mask:
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	priv->irq_info[i].mask = NULL;
+#else
+	/* just to keep gcc happy - (we can't have a label at the end of a
+	 * function) */
+	return;
+#endif
 }
 
 static void mlx5_irq_clear_affinity_hint(struct mlx5_core_dev *mdev, int i)
@@ -626,13 +648,19 @@ static void mlx5_irq_clear_affinity_hint
 	struct mlx5_priv *priv  = &mdev->priv;
 	struct msix_entry *msix = priv->msix_arr;
 	int irq                 = msix[i + MLX5_EQ_VEC_COMP_BASE].vector;
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	cpumask_var_t mask      = priv->irq_info[i].mask;
 
 	if (!priv->irq_info[i].mask)
 		return;
+#endif
 
 	irq_set_affinity_hint(irq, NULL);
+#ifdef CONFIG_CPUMASK_OFFSTACK
 	free_cpumask_var(mask);
+#else
+	free_cpumask_var(priv->irq_info[i].mask);
+#endif
 }
 
 static void mlx5_irq_set_affinity_hints(struct mlx5_core_dev *mdev)
@@ -1607,7 +1635,11 @@ static void mlx5_pci_resume(struct pci_d
 		dev_info(&pdev->dev, "%s: device recovered\n", __func__);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_PCI_ERROR_HANDLERS
 static const struct pci_error_handlers mlx5_err_handler = {
+#else
+static struct pci_error_handlers mlx5_err_handler = {
+#endif
 	.error_detected = mlx5_pci_err_detected,
 	.slot_reset	= mlx5_pci_slot_reset,
 	.resume		= mlx5_pci_resume
@@ -1672,7 +1704,9 @@ static struct pci_driver mlx5_core_drive
 	.remove			= remove_one,
 	.shutdown		= shutdown,
 	.err_handler		= &mlx5_err_handler,
+#ifdef HAVE_PCI_DRIVER_SRIOV_CONFIGURE
 	.sriov_configure	= mlx5_core_sriov_configure,
+#endif
 };
 
 static int __init init(void)
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -131,8 +131,10 @@ int mlx5_eswitch_set_vport_vlan(struct m
 				int vport, u16 vlan, u8 qos);
 int mlx5_eswitch_set_vport_spoofchk(struct mlx5_core_dev *dev,
 				    int vport, bool setting);
+#ifdef HAVE_NDO_SET_VF_MAC
 int mlx5_eswitch_get_vport_config(struct mlx5_core_dev *dev,
 				  int vport, struct ifla_vf_info *ivi);
+#endif
 
 void mlx5e_init(void);
 void mlx5e_cleanup(void);
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@ -421,7 +421,11 @@ static ssize_t policy_store(struct mlx5_
 	return count;
 }
 
+#ifndef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
+static struct sysfs_ops guid_sysfs_ops = {
+#else
 static const struct sysfs_ops guid_sysfs_ops = {
+#endif
 	.show = guid_attr_show,
 	.store = guid_attr_store,
 };
@@ -504,8 +508,12 @@ static ssize_t num_vf_store(struct devic
 	int req_vfs;
 	int err;
 
-	if (kstrtoint(buf, 0, &req_vfs) || req_vfs < 0 ||
-	    req_vfs > pci_sriov_get_totalvfs(pdev))
+	if (kstrtoint(buf, 0, &req_vfs) || req_vfs < 0
+#ifdef HAVE_PCI_SRIOV_GET_TOTALVFS
+	    || req_vfs > pci_sriov_get_totalvfs(pdev))
+#else
+	)
+#endif
 		return -EINVAL;
 
 	err = mlx5_core_sriov_configure(pdev, req_vfs);
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -739,8 +739,13 @@ struct mlx5_cmd_work_ent {
 	int			page_queue;
 	u8			status;
 	u8			token;
+#ifdef HAVE_KTIME_GET_NS
 	u64			ts1;
 	u64			ts2;
+#else
+	struct timespec ts1;
+	struct timespec ts2;
+#endif
 	u16			op;
 };
 
