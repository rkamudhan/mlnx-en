From: Eli Cohen <eli@mellanox.com>
Subject: [PATCH] BACKPORT: ib_ipoib

Change-Id: I4064a8d0a0806a7674d276b75448f172fba098b8
Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
---
 drivers/infiniband/ulp/ipoib/ipoib.h           |  51 ++++-
 drivers/infiniband/ulp/ipoib/ipoib_cm.c        |  26 ++-
 drivers/infiniband/ulp/ipoib/ipoib_ethtool.c   |  67 ++++++
 drivers/infiniband/ulp/ipoib/ipoib_genetlink.c |  68 +++++-
 drivers/infiniband/ulp/ipoib/ipoib_ib.c        | 273 ++++++++++++++++++++++++-
 drivers/infiniband/ulp/ipoib/ipoib_main.c      | 209 +++++++++++++++++--
 drivers/infiniband/ulp/ipoib/ipoib_multicast.c |  15 ++
 drivers/infiniband/ulp/ipoib/ipoib_netlink.c   |  18 +-
 drivers/infiniband/ulp/ipoib/ipoib_vlan.c      |   2 +
 9 files changed, 697 insertions(+), 32 deletions(-)

--- a/drivers/infiniband/ulp/ipoib/ipoib.h
+++ b/drivers/infiniband/ulp/ipoib/ipoib.h
@@ -44,6 +44,8 @@
 #include <linux/if_infiniband.h>
 #include <linux/mutex.h>
 
+#include <linux/inet_lro.h>
+
 #include <net/neighbour.h>
 #include <net/sch_generic.h>
 
@@ -74,7 +76,9 @@ enum {
 	IPOIB_UD_HEAD_SIZE	  = IB_GRH_BYTES + IPOIB_ENCAP_LEN,
 	IPOIB_UD_HEAD_BUFF_SIZE   = IPOIB_UD_HEAD_SIZE + 128, /* reserve some tailroom for IP/TCP headers */
 	IPOIB_UD_RX_SG		  = 2, /* max buffer needed for 4K mtu */
-
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	IPOIB_NUM_RX_SKB	  = 2,
+#endif
 	IPOIB_CM_MTU		  = 0x10000 - 0x10, /* padding to align header to 16 */
 	IPOIB_CM_BUF_SIZE	  = IPOIB_CM_MTU  + IPOIB_ENCAP_LEN,
 	IPOIB_CM_HEAD_SIZE	  = IPOIB_CM_BUF_SIZE % PAGE_SIZE,
@@ -110,6 +114,9 @@ enum {
 
 	IPOIB_MAX_BACKOFF_SECONDS = 16,
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39))
+	IPOIB_FLAG_CSUM = 17,
+#endif
 	IPOIB_MCAST_FLAG_FOUND	  = 0,	/* used in set_multicast_list */
 	IPOIB_MCAST_FLAG_SENDONLY = 1,
 	IPOIB_MCAST_FLAG_BUSY	  = 2,	/* joining or already joined */
@@ -119,6 +126,11 @@ enum {
 
 	IPOIB_USR_MC_MEMBER		= 7,	/* used for user-related mcg */
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	IPOIB_MAX_LRO_DESCRIPTORS = 8,
+	IPOIB_LRO_MAX_AGGR      = 64,
+#endif
+
 	MAX_SEND_CQE		  = 16,
 	IPOIB_CM_COPYBREAK	  = 256,
 	IPOIB_MAX_INLINE_SIZE     = 800,
@@ -132,6 +144,20 @@ enum {
 	IPOIB_DEFAULT_CONN_QP = 128,
 };
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+struct ipoib_lro {
+	struct net_lro_mgr lro_mgr;
+	struct net_lro_desc lro_desc[IPOIB_MAX_LRO_DESCRIPTORS];
+};
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+enum ipoib_alloc_type {
+	IPOIB_ALLOC_NEW = 0,
+	IPOIB_ALLOC_REPLACEMENT = 1,
+};
+#endif
+
 #define	IPOIB_OP_RECV   (1ul << 31)
 #ifdef CONFIG_INFINIBAND_IPOIB_CM
 #define	IPOIB_OP_CM     (1ul << 30)
@@ -175,10 +201,24 @@ struct ipoib_mcast {
 	struct net_device *dev;
 };
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+struct ipoib_skb {
+	struct sk_buff *skb;
+	u64 mapping[IPOIB_UD_RX_SG];
+};
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+struct ipoib_rx_buf {
+	struct ipoib_skb rx_skb[IPOIB_NUM_RX_SKB];
+	int skb_index;
+};
+#else
 struct ipoib_rx_buf {
 	struct sk_buff *skb;
 	u64		mapping[IPOIB_UD_RX_SG];
 };
+#endif
 
 struct ipoib_tx_buf {
 	struct sk_buff *skb;
@@ -415,6 +455,9 @@ struct ipoib_recv_ring {
 	struct ipoib_rx_ring_stats stats;
 	unsigned		index;
 	struct ipoib_ethtool_last_st ethtool;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct ipoib_lro lro;
+#endif
 };
 
 struct ipoib_arp_repath {
@@ -712,9 +755,15 @@ void ipoib_arm_cq(struct net_device *dev
 void ipoib_set_ethtool_ops(struct net_device *dev);
 int ipoib_set_dev_features(struct ipoib_dev_priv *priv, struct ib_device *hca);
 
+#ifdef CONFIG_IPOIB_NO_OPTIONS
+#define IPOIB_FLAGS_RC		0x0
+#define IPOIB_FLAGS_UC		0x0
+#define IPOIB_FLAGS_TSS		0x0
+#else
 #define IPOIB_FLAGS_RC		0x80
 #define IPOIB_FLAGS_UC		0x40
 #define IPOIB_FLAGS_TSS		0x20
+#endif
 
 /* We don't support UC connections at the moment */
 #define IPOIB_CM_SUPPORTED(ha)   (ha[0] & (IPOIB_FLAGS_RC))
--- a/drivers/infiniband/ulp/ipoib/ipoib_cm.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_cm.c
@@ -587,6 +587,9 @@ void ipoib_cm_handle_rx_wc(struct net_de
 	int frags;
 	int has_srq;
 	struct sk_buff *small_skb;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+	extern int (*eth_ipoib_handle_frame_hook)(struct sk_buff **skb);
+#endif
 
 	ipoib_dbg_data(priv, "cm recv completion: id %d, status: %d\n",
 		       wr_id, wc->status);
@@ -693,10 +696,19 @@ copied:
 	/* XXX get correct PACKET_ type here */
 	skb->pkt_type = PACKET_HOST;
 	/* if handler is registered on top of ipoib, set skb oob data. */
-        if (skb->dev->priv_flags & IFF_EIPOIB_VIF)
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+	if ((skb->dev->priv_flags & CONFIG_COMPAT_IFF_EIPOIB_VIF) && eth_ipoib_handle_frame_hook) {
+#elif (LINUX_VERSION_CODE == KERNEL_VERSION(2,6,37) || LINUX_VERSION_CODE == KERNEL_VERSION(2,6,36))
+	if (skb->dev->priv_flags & CONFIG_COMPAT_IFF_EIPOIB_VIF)
+#else
+	if (skb->dev->priv_flags & IFF_EIPOIB_VIF)
+#endif
 		set_skb_oob_cb_data(skb, wc, NULL);
-
-	netif_receive_skb(skb);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+	eth_ipoib_handle_frame_hook(&skb);
+	} else
+#endif
+		netif_receive_skb(skb);
 
 repost:
 	if (has_srq) {
@@ -1544,7 +1556,11 @@ static void ipoib_cm_skb_reap(struct wor
 			icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
 #if IS_ENABLED(CONFIG_IPV6)
 		else if (skb->protocol == htons(ETH_P_IPV6))
+#if  (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 			icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
+#else
+			icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, priv->dev);
+#endif
 #endif
 		dev_kfree_skb_any(skb);
 
@@ -1562,7 +1578,11 @@ static void ipoib_cm_update_pmtu_task(st
 		container_of(work, struct ipoib_pmtu_update, work);
 	struct sk_buff *skb = pmtu_update->skb;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,6,0)
 	skb_dst(skb)->ops->update_pmtu(skb_dst(skb), NULL, skb, pmtu_update->mtu);
+#else
+	skb_dst(skb)->ops->update_pmtu(skb_dst(skb), pmtu_update->mtu);
+#endif
 
 	consume_skb(skb);
 
--- a/drivers/infiniband/ulp/ipoib/ipoib_ethtool.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_ethtool.c
@@ -324,6 +324,18 @@ static int ipoib_get_sset_count(struct n
 	}
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,6,0)
+/* Add missing defines for supported and advertised speed features */
+#define SUPPORTED_40000baseKR4_Full     (1 << 23)
+#define SUPPORTED_40000baseCR4_Full     (1 << 24)
+#define SUPPORTED_40000baseSR4_Full     (1 << 25)
+#define SUPPORTED_40000baseLR4_Full     (1 << 26)
+#define ADVERTISED_40000baseKR4_Full    (1 << 23)
+#define ADVERTISED_40000baseCR4_Full    (1 << 24)
+#define ADVERTISED_40000baseSR4_Full    (1 << 25)
+#define ADVERTISED_40000baseLR4_Full    (1 << 26)
+#endif
+
 static int ipoib_get_settings(struct net_device *dev, struct ethtool_cmd *ecmd)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
@@ -409,6 +421,7 @@ static void ipoib_get_ethtool_stats(stru
 	}
 }
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0))
 static void ipoib_get_channels(struct net_device *dev,
 			struct ethtool_channels *channel)
 {
@@ -474,6 +487,48 @@ static int ipoib_set_channels(struct net
 
 	return ipoib_reinit(dev, channel->rx_count, channel->tx_count);
 }
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)) && (LINUX_VERSION_CODE <  KERNEL_VERSION(3,3,0)) && defined (CONFIG_COMPAT_LRO_ENABLED_IPOIB)
+int ipoib_set_flags(struct net_device *dev, u32 data)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+	int hw_support_lro = 0;
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
+		hw_support_lro = priv->dev->hw_features & NETIF_F_RXCSUM;
+#else
+		hw_support_lro = priv->dev->features & NETIF_F_RXCSUM;
+#endif
+
+	if ((data & ETH_FLAG_LRO) && hw_support_lro)
+		dev->features |= NETIF_F_LRO;
+	else
+		dev->features &= ~NETIF_F_LRO;
+
+	return 0;
+}
+#elif defined (CONFIG_COMPAT_LRO_ENABLED_IPOIB) && (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+int ipoib_set_flags(struct net_device *dev, u32 data)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+	if (data & ETH_FLAG_LRO) {
+		if (!(priv->dev->features & NETIF_F_RXCSUM))
+			return -EINVAL;
+	}
+	ethtool_op_set_flags(dev, data);
+	return 0;
+}
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39))
+static u32 ipoib_get_rx_csum(struct net_device *dev)
+{
+       	struct ipoib_dev_priv *priv = netdev_priv(dev);
+       	return test_bit(IPOIB_FLAG_CSUM, &priv->flags) &&
+		!test_bit(IPOIB_FLAG_ADMIN_CM, &priv->flags);
+}
+#endif
 
 static const struct ethtool_ops ipoib_ethtool_ops = {
 	.get_drvinfo		= ipoib_get_drvinfo,
@@ -486,8 +541,20 @@ static const struct ethtool_ops ipoib_et
 	.get_strings		= ipoib_get_strings,
 	.get_sset_count		= ipoib_get_sset_count,
 	.get_ethtool_stats	= ipoib_get_ethtool_stats,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0))
 	.get_channels		= ipoib_get_channels,
 	.set_channels		= ipoib_set_channels,
+#endif
+#if (LINUX_VERSION_CODE <  KERNEL_VERSION(3,3,0))
+#if defined (CONFIG_COMPAT_LRO_ENABLED_IPOIB)
+	.set_flags		= ipoib_set_flags,
+#endif
+	.get_flags              = ethtool_op_get_flags,
+#endif
+#if (LINUX_VERSION_CODE <  KERNEL_VERSION(2,6,39))
+       .get_rx_csum            = ipoib_get_rx_csum,
+#endif
+
 };
 
 void ipoib_set_ethtool_ops(struct net_device *dev)
--- a/drivers/infiniband/ulp/ipoib/ipoib_genetlink.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_genetlink.c
@@ -415,6 +415,7 @@ static struct nla_policy ipoib_genl_poli
 };
 
 /* ipoib mcast group for path rec */
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 struct genl_multicast_group ipoib_path_notify_grp = {
 	.name = "PATH_NOTIFY",
 };
@@ -423,8 +424,20 @@ struct genl_multicast_group ipoib_path_n
 struct genl_multicast_group ipoib_mc_notify_grp = {
 	.name = "MC_NOTIFY",
 };
+#else
+enum ipoib_multicast_groups {
+		IPOIB_MCGRP_PATH_NOTIFY,
+		IPOIB_MCGRP_MC_NOTIFY,
+};
+
+static const struct genl_multicast_group ipoib_mcgrps[] = {
+		[IPOIB_MCGRP_PATH_NOTIFY] = { .name = "PATH_NOTIFY", },
+		[IPOIB_MCGRP_MC_NOTIFY] = { .name = "MC_NOTIFY", },
+};
+#endif
 
 /* operation definition */
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 static struct genl_ops ipoib_genl_path_ops[] = {
 	{
 	.cmd		= ENABLE_PATH,
@@ -474,7 +487,31 @@ static struct genl_ops ipoib_genl_valida
 	.dumpit		= NULL,
 	}
 };
-
+#else
+static struct genl_ops ipoib_genl_ops[] = {
+	{
+	.cmd		= ENABLE_PATH,
+	.flags		= GENL_ADMIN_PERM,
+	.policy		= ipoib_genl_policy,
+	.doit		= ipoib_gnl_cb,
+	.dumpit		= NULL,
+	},
+	{
+	.cmd		= ENABLE_MC,
+	.flags		= GENL_ADMIN_PERM,
+	.policy		= ipoib_genl_policy,
+	.doit		= ipoib_gnl_cb,
+	.dumpit		= NULL,
+	},
+	{
+	.cmd		= GET_MCG,
+	.flags		= GENL_ADMIN_PERM,
+	.policy		= ipoib_genl_policy,
+	.doit		= ipoib_gnl_cb,
+	.dumpit		= NULL,
+	}
+};
+#endif
 static inline char *get_command(int command)
 {
 	switch(command) {
@@ -541,8 +578,13 @@ void generate_reply(struct work_struct *
 		memcpy(p, &record->path_rec,
 		       sizeof(struct ipoib_path_notice));
 		genlmsg_end(skb, msg_head);
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 		i = genlmsg_multicast(skb, 0, ipoib_path_notify_grp.id,
 				      GFP_KERNEL);
+#else
+		i = genlmsg_multicast(&ipoib_genl_family, skb, 0,
+				      IPOIB_MCGRP_PATH_NOTIFY, GFP_KERNEL);
+#endif
 		break;
 	}
 	case PATH_DEL:
@@ -554,8 +596,13 @@ void generate_reply(struct work_struct *
 		memcpy(p, &record->path_del,
 		       sizeof(struct ipoib_path_del_notice));
 		genlmsg_end(skb, msg_head);
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 		i = genlmsg_multicast(skb, 0, ipoib_path_notify_grp.id,
 				      GFP_KERNEL);
+#else
+		i = genlmsg_multicast(&ipoib_genl_family, skb, 0,
+				      IPOIB_MCGRP_PATH_NOTIFY, GFP_KERNEL);
+#endif
 		break;
 	}
 	case MCG_DETAILS:
@@ -568,8 +615,13 @@ void generate_reply(struct work_struct *
 		memcpy(m, &record->mc_join,
 		       sizeof(struct ipoib_mc_join_notice));
 		genlmsg_end(skb, msg_head);
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 		i = genlmsg_multicast(skb, 0, ipoib_mc_notify_grp.id,
 				      GFP_KERNEL);
+#else
+		i = genlmsg_multicast(&ipoib_genl_family, skb, 0,
+				      IPOIB_MCGRP_MC_NOTIFY, GFP_KERNEL);
+#endif
 		break;
 	}
 	case MC_LEAVE:
@@ -581,8 +633,13 @@ void generate_reply(struct work_struct *
 		memcpy(m, &record->mc_leave,
 		       sizeof(struct ipoib_mc_leave_notice));
 		genlmsg_end(skb, msg_head);
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 		i = genlmsg_multicast(skb, 0, ipoib_mc_notify_grp.id,
 				      GFP_KERNEL);
+#else
+		i = genlmsg_multicast(&ipoib_genl_family, skb, 0,
+				      IPOIB_MCGRP_MC_NOTIFY, GFP_KERNEL);
+#endif
 		break;
 	}
 	}
@@ -621,6 +678,7 @@ void ipoib_unregister_genl(void)
 int ipoib_register_genl(void)
 {
 	int rc;
+#if defined(CONFIG_GENETLINK_IS_LIST_HEAD)
 	rc = genl_register_family(&ipoib_genl_family);
 	if (rc != 0)
 		goto out;
@@ -653,6 +711,14 @@ unregister:
  *	all assigned operations to be unregistered automatically.
  *	all assigned multicast groups to be unregistered automatically. */
 	ipoib_unregister_genl();
+	return rc;
+#else
+	genl_registered = 0;
+        rc = genl_register_family_with_ops_groups(&ipoib_genl_family, ipoib_genl_ops, ipoib_mcgrps);
+        if (rc < 0)
+                goto out;
+	genl_registered = 1;
+#endif
 out:
 	return rc;
 }
--- a/drivers/infiniband/ulp/ipoib/ipoib_ib.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_ib.c
@@ -101,6 +101,32 @@ static void ipoib_ud_dma_unmap_rx(struct
 			    DMA_FROM_DEVICE);
 }
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+static int ipoib_ib_post_receive(struct net_device *dev,
+				 struct ipoib_recv_ring *recv_ring,
+				 int id, int skb_idx)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+	struct ib_recv_wr *bad_wr;
+	struct ipoib_skb *rx_skb = &recv_ring->rx_ring[id].rx_skb[skb_idx];
+	int ret;
+
+	recv_ring->rx_wr.wr_id   = id | IPOIB_OP_RECV;
+	recv_ring->rx_sge[0].addr = rx_skb->mapping[0];
+	recv_ring->rx_sge[1].addr = rx_skb->mapping[1];
+
+
+	ret = ib_post_recv(recv_ring->recv_qp, &recv_ring->rx_wr, &bad_wr);
+	if (unlikely(ret)) {
+		ipoib_warn(priv, "receive failed for buf %d (%d)\n", id, ret);
+		ipoib_ud_dma_unmap_rx(priv, rx_skb->mapping);
+		dev_kfree_skb_any(rx_skb->skb);
+		rx_skb->skb = NULL;
+	}
+
+	return ret;
+}
+#else
 static int ipoib_ib_post_receive(struct net_device *dev,
 			struct ipoib_recv_ring *recv_ring, int id)
 {
@@ -123,10 +149,17 @@ static int ipoib_ib_post_receive(struct
 
 	return ret;
 }
+#endif
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+static struct sk_buff *ipoib_alloc_rx_skb(struct net_device *dev,
+					  struct ipoib_skb *rx_skb,
+					  enum ipoib_alloc_type type)
+#else					  
 static struct sk_buff *ipoib_alloc_rx_skb(struct net_device *dev,
 					  struct ipoib_recv_ring *recv_ring,
 					  int id)
+#endif
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
 	struct sk_buff *skb;
@@ -134,10 +167,22 @@ static struct sk_buff *ipoib_alloc_rx_sk
 	u64 *mapping;
 
 	buf_size = IPOIB_UD_BUF_SIZE(priv->max_ib_mtu);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	mapping = rx_skb->mapping;
 
+	if (type == IPOIB_ALLOC_REPLACEMENT) {
+		ipoib_ud_dma_unmap_rx(priv, mapping);
+		consume_skb(rx_skb->skb);
+		rx_skb->skb = NULL;
+	}
+#endif
 	skb = dev_alloc_skb(buf_size + 4);
 	if (unlikely(!skb))
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+		goto out;
+#else
 		return NULL;
+#endif
 
 	/*
 	 * IB will leave a 40 byte gap for a GRH and IPoIB adds a 4 byte
@@ -145,18 +190,27 @@ static struct sk_buff *ipoib_alloc_rx_sk
 	 * IP header to a multiple of 16.
 	 */
 	skb_reserve(skb, 4);
-
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)) || defined(HAVE_SK_BUFF_CSUM_LEVEL)
 	mapping = recv_ring->rx_ring[id].mapping;
+#endif
 	mapping[0] = ib_dma_map_single(priv->ca, skb->data, buf_size,
 				       DMA_FROM_DEVICE);
 	if (unlikely(ib_dma_mapping_error(priv->ca, mapping[0])))
 		goto error;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	rx_skb->skb = skb;
+#else
 	recv_ring->rx_ring[id].skb = skb;
+#endif
 	return skb;
 
 error:
 	dev_kfree_skb_any(skb);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+out:
+	rx_skb->skb = NULL;
+#endif
 	return NULL;
 }
 
@@ -164,9 +218,26 @@ static int ipoib_ib_post_ring_receives(s
 				      struct ipoib_recv_ring *recv_ring)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	struct ipoib_skb *rx_skb;
+	int i, j;
+#else
 	int i;
+#endif
 
 	for (i = 0; i < priv->recvq_size; ++i) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+		for (j = 0; j < IPOIB_NUM_RX_SKB; ++j) {
+			rx_skb = &recv_ring->rx_ring[i].rx_skb[j];
+			if (!ipoib_alloc_rx_skb(dev, rx_skb, IPOIB_ALLOC_NEW)) {
+				ipoib_warn(priv,
+					   "failed to allocate buf (%d,%d)\n",
+					   recv_ring->index, i);
+				return -ENOMEM;
+			}
+		}
+		if (ipoib_ib_post_receive(dev, recv_ring, i, 0)) {
+#else
 		if (!ipoib_alloc_rx_skb(dev, recv_ring, i)) {
 			ipoib_warn(priv,
 				"failed to allocate receive buffer (%d,%d)\n",
@@ -174,11 +245,15 @@ static int ipoib_ib_post_ring_receives(s
 			return -ENOMEM;
 		}
 		if (ipoib_ib_post_receive(dev, recv_ring, i)) {
+#endif
 			ipoib_warn(priv,
 				"ipoib_ib_post_receive failed for buf (%d,%d)\n",
 				recv_ring->index, i);
 			return -EIO;
 		}
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+		recv_ring->rx_ring[i].skb_index = 0;
+#endif
 	}
 
 	return 0;
@@ -235,7 +310,81 @@ static inline void ipoib_create_repath_e
 		kfree(arp_repath);
 }
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+static void ipoib_skb_drop_fraglist(struct sk_buff *skb)
+{
+	struct sk_buff *list = skb_shinfo(skb)->frag_list;
+	skb_frag_list_init(skb);
+
+	while (list) {
+		struct sk_buff *this = list;
+		list = list->next;
+		consume_skb(this);
+	}
+}
 
+static void ipoib_reuse_skb(struct net_device *dev,
+			    struct ipoib_skb *rx_skb)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(3,4,0))
+	__u8 cur_head_frag = rx_skb->skb->head_frag;
+#endif
+	ib_dma_sync_single_for_cpu(priv->ca,
+				   rx_skb->mapping[0],
+				   IPOIB_UD_BUF_SIZE(priv->max_ib_mtu),
+				   DMA_FROM_DEVICE);
+
+	memset(rx_skb->skb, 0, offsetof(struct sk_buff, tail));
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(3,4,0))
+	/* keep origin values of necessary fields */
+	rx_skb->skb->head_frag = cur_head_frag;
+#endif
+	rx_skb->skb->data = rx_skb->skb->head + NET_SKB_PAD;
+	skb_reset_tail_pointer(rx_skb->skb);
+	skb_reserve(rx_skb->skb, 4);
+	skb_frag_list_init(rx_skb->skb);
+	skb_get(rx_skb->skb);
+}
+
+static int ipoib_prepare_next_skb(struct net_device *dev,
+				  struct ipoib_rx_buf *rx_ring)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+	struct ipoib_skb *rx_skb = NULL;
+	struct sk_buff *skb = NULL;
+	int next_skb_id;
+	int buf_size = IPOIB_UD_BUF_SIZE(priv->max_ib_mtu);
+
+	next_skb_id = (rx_ring->skb_index + 1) % IPOIB_NUM_RX_SKB;
+	rx_skb = &rx_ring->rx_skb[next_skb_id];
+ 	skb = rx_skb->skb;
+
+	if (!skb || skb_shared(skb) || skb_cloned(skb)) {
+		enum ipoib_alloc_type alloc_type;
+		alloc_type = skb ? IPOIB_ALLOC_REPLACEMENT : IPOIB_ALLOC_NEW;
+		if (!ipoib_alloc_rx_skb(dev, rx_skb, alloc_type))
+			return -ENOMEM;
+	} else
+		ib_dma_sync_single_for_device(priv->ca,
+					      rx_skb->mapping[0],
+					      buf_size,
+					      DMA_FROM_DEVICE);
+#ifdef CONFIG_COMPAT_SKB_HAS_FRAG_LIST
+	if (skb_has_frag_list(rx_skb->skb))
+#else
+	if (skb_has_frags(rx_skb->skb))	
+#endif
+		ipoib_skb_drop_fraglist(rx_skb->skb);
+
+	rx_ring->skb_index = next_skb_id;
+	return 0;
+} 
+#endif
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+int (*eth_ipoib_handle_frame_hook)(struct sk_buff **skb) = NULL;
+EXPORT_SYMBOL_GPL(eth_ipoib_handle_frame_hook);
+#endif
 
 static void ipoib_ib_handle_rx_wc(struct net_device *dev,
 				  struct ipoib_recv_ring *recv_ring,
@@ -244,10 +393,15 @@ static void ipoib_ib_handle_rx_wc(struct
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
 	unsigned int wr_id = wc->wr_id & ~IPOIB_OP_RECV;
 	struct sk_buff *skb;
-	u64 mapping[IPOIB_UD_RX_SG];
 	union ib_gid *dgid;
 	union ib_gid *sgid;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	struct ipoib_rx_buf *rx_ring = &recv_ring->rx_ring[wr_id];
+	struct ipoib_skb *cur_skb = &rx_ring->rx_skb[rx_ring->skb_index];
+#else
+	u64 mapping[IPOIB_UD_RX_SG];
+#endif
 	ipoib_dbg_data(priv, "recv completion: id %d, status: %d\n",
 		       wr_id, wc->status);
 
@@ -256,20 +410,34 @@ static void ipoib_ib_handle_rx_wc(struct
 			   wr_id, priv->recvq_size);
 		return;
 	}
-
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	skb = cur_skb->skb;
+#else
 	skb  = recv_ring->rx_ring[wr_id].skb;
+#endif
 
 	if (unlikely(wc->status != IB_WC_SUCCESS)) {
 		if (wc->status != IB_WC_WR_FLUSH_ERR)
 			ipoib_warn(priv, "failed recv event "
 				   "(status=%d, wrid=%d vend_err %x)\n",
 				   wc->status, wr_id, wc->vendor_err);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+		ipoib_ud_dma_unmap_rx(priv, cur_skb->mapping);
+#else
 		ipoib_ud_dma_unmap_rx(priv, recv_ring->rx_ring[wr_id].mapping);
+#endif
 		dev_kfree_skb_any(skb);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+		cur_skb->skb = NULL;
+#else
 		recv_ring->rx_ring[wr_id].skb = NULL;
+#endif
 		return;
 	}
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	if (unlikely(ipoib_prepare_next_skb(dev, rx_ring))) {
+#else
 	memcpy(mapping, recv_ring->rx_ring[wr_id].mapping,
 	       IPOIB_UD_RX_SG * sizeof *mapping);
 
@@ -278,16 +446,21 @@ static void ipoib_ib_handle_rx_wc(struct
 	 * this packet and reuse the old buffer.
 	 */
 	if (unlikely(!ipoib_alloc_rx_skb(dev, recv_ring, wr_id))) {
+#endif
 		++recv_ring->stats.rx_dropped;
 		goto repost;
 	}
-
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	ipoib_reuse_skb(dev, cur_skb);
+#endif
 	skb_record_rx_queue(skb, recv_ring->index);
 
 	ipoib_dbg_data(priv, "received %d bytes, SLID 0x%04x\n",
 		       wc->byte_len, wc->slid);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)) || defined(HAVE_SK_BUFF_CSUM_LEVEL)
 	ipoib_ud_dma_unmap_rx(priv, mapping);
+#endif
 	skb_put(skb, wc->byte_len);
 
 	/* First byte of dgid signals multicast when 0xff */
@@ -341,15 +514,39 @@ static void ipoib_ib_handle_rx_wc(struct
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 
 	/* if handler is registered on top of ipoib, set skb oob data. */
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,37))
+	if (dev->priv_flags & CONFIG_COMPAT_IFF_EIPOIB_VIF) {
+#else
 	if (dev->priv_flags & IFF_EIPOIB_VIF) {
+#endif
 		set_skb_oob_cb_data(skb, wc, &recv_ring->napi);
 		/*the registered handler will take care of the skb.*/
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+		if (eth_ipoib_handle_frame_hook)
+			eth_ipoib_handle_frame_hook(&skb);
+		else
+#endif
+	netif_receive_skb(skb);
+
+	}
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	else if (dev->features & NETIF_F_LRO)
+		lro_receive_skb(&recv_ring->lro.lro_mgr, skb, NULL);
+	else
 		netif_receive_skb(skb);
-	} else
+#else
+	else
 		napi_gro_receive(&recv_ring->napi, skb);
+#endif
 
 repost:
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	if (unlikely(ipoib_ib_post_receive(dev, recv_ring, wr_id,
+					   rx_ring->skb_index)))
+#else
 	if (unlikely(ipoib_ib_post_receive(dev, recv_ring, wr_id)))
+#endif
 		ipoib_warn(priv, "ipoib_ib_post_receive failed "
 			   "for buf %d\n", wr_id);
 }
@@ -561,6 +758,10 @@ poll_more:
 	}
 
 	if (n < budget) {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		if (dev->features & NETIF_F_LRO)
+			lro_flush_all(&rx_ring->lro.lro_mgr);
+#endif
 		napi_complete(napi);
 		if (unlikely(ib_req_notify_cq(rx_ring->recv_cq,
 					      IB_CQ_NEXT_COMP |
@@ -975,18 +1176,56 @@ int ipoib_ib_dev_down(struct net_device
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+/* Clean all the skb's that created for the re-use process*/
+static void clean_reused_skb(struct net_device *dev)
+{
+	struct ipoib_dev_priv *priv = netdev_priv(dev);
+	struct ipoib_recv_ring *recv_ring;
+	int i, j;
+	struct ipoib_rx_buf *rx_buf;
+	struct ipoib_skb *rx_skb;
+
+	recv_ring = priv->recv_ring;
+	for (j = 0; j < priv->num_rx_queues; j++) {
+		for (i = 0; i < priv->recvq_size; ++i) {
+			rx_buf = &recv_ring->rx_ring[i];
+			rx_skb = &rx_buf->rx_skb[(rx_buf->skb_index + 1) % IPOIB_NUM_RX_SKB];
+			if (rx_skb->skb) {
+				ipoib_ud_dma_unmap_rx(priv, rx_skb->mapping);
+				dev_kfree_skb_any(rx_skb->skb);
+				rx_skb->skb = NULL;
+			}
+		}
+		recv_ring++;
+	}
+}
+#endif
+
 static int recvs_pending(struct net_device *dev)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
 	struct ipoib_recv_ring *recv_ring;
 	int pending = 0;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	int i, j, k;
+#else
 	int i, j;
-
+#endif
 	recv_ring = priv->recv_ring;
 	for (j = 0; j < priv->num_rx_queues; j++) {
 		for (i = 0; i < priv->recvq_size; ++i) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+			struct ipoib_rx_buf *rx_req = &recv_ring->rx_ring[i];
+			for (k = 0; k < IPOIB_NUM_RX_SKB; k++) {
+				struct ipoib_skb *rx_skb = &rx_req->rx_skb[k];
+				if (rx_skb->skb)
+					++pending;
+			}
+#else
 			if (recv_ring->rx_ring[i].skb)
 				++pending;
+#endif
 		}
 		recv_ring++;
 	}
@@ -1139,11 +1378,25 @@ static void ipoib_ib_send_ring_stop(stru
 static void ipoib_ib_recv_ring_stop(struct ipoib_dev_priv *priv)
 {
 	struct ipoib_recv_ring *recv_ring;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	int i, j, k;
+#else
 	int i, j;
-
+#endif
 	recv_ring = priv->recv_ring;
 	for (j = 0; j < priv->num_rx_queues; ++j) {
 		for (i = 0; i < priv->recvq_size; ++i) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+			struct ipoib_rx_buf *rx_req = &recv_ring->rx_ring[i];
+			for (k = 0; k < IPOIB_NUM_RX_SKB; k++) {
+				struct ipoib_skb *rx_skb = &rx_req->rx_skb[k];
+				if (!rx_skb->skb)
+					continue;
+				ipoib_ud_dma_unmap_rx(priv, rx_skb->mapping);
+				dev_kfree_skb_any(rx_skb->skb);
+				rx_skb->skb = NULL;
+			}
+#else
 			struct ipoib_rx_buf *rx_req;
 
 			rx_req = &recv_ring->rx_ring[i];
@@ -1153,6 +1406,7 @@ static void ipoib_ib_recv_ring_stop(stru
 					      recv_ring->rx_ring[i].mapping);
 			dev_kfree_skb_any(rx_req->skb);
 			rx_req->skb = NULL;
+#endif
 		}
 		recv_ring++;
 	}
@@ -1273,8 +1527,9 @@ int ipoib_ib_dev_stop(struct net_device
 	mutex_lock(&priv->ring_qp_lock);
 
 	set_rings_qp_state(priv, IB_QPS_ERR);
-
-
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)) && ! defined(HAVE_SK_BUFF_CSUM_LEVEL)
+	clean_reused_skb(dev);
+#endif
 	/* Wait for all sends and receives to complete */
 	begin = jiffies;
 
--- a/drivers/infiniband/ulp/ipoib/ipoib_main.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_main.c
@@ -65,6 +65,17 @@ MODULE_PARM_DESC(send_queue_size, "Numbe
 module_param_named(recv_queue_size, ipoib_recvq_size, int, 0444);
 MODULE_PARM_DESC(recv_queue_size, "Number of descriptors in receive queue (default = 512) (2-8192)");
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int lro = 1;
+module_param_named(lro, lro, int, 0444);
+MODULE_PARM_DESC(lro,  "Enable LRO (Large Receive Offload) (default = 1) (0-1)");
+
+static int lro_max_aggr = IPOIB_LRO_MAX_AGGR;
+module_param_named(lro_max_aggr, lro_max_aggr, int, 0444);
+MODULE_PARM_DESC(lro_max_aggr, "LRO: Max packets to be aggregated must be power of 2"
+                               "(default = 64) (2-64)");
+#endif
+
 #ifdef CONFIG_INFINIBAND_IPOIB_DEBUG
 int ipoib_debug_level;
 
@@ -90,6 +101,10 @@ struct ib_sa_client ipoib_sa_client;
 static void ipoib_add_one(struct ib_device *device);
 static void ipoib_remove_one(struct ib_device *device);
 static void ipoib_neigh_reclaim(struct rcu_head *rp);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void ipoib_lro_setup(struct ipoib_recv_ring *recv_ring,
+				struct ipoib_dev_priv *priv);
+#endif
 
 static struct ib_client ipoib_client = {
 	.name   = "ipoib",
@@ -196,6 +211,7 @@ void ipoib_uninit(struct net_device *dev
 	ipoib_dev_cleanup(dev);
 }
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 static netdev_features_t ipoib_fix_features(struct net_device *dev, netdev_features_t features)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
@@ -206,6 +222,7 @@ static netdev_features_t ipoib_fix_featu
 
 	return features;
 }
+#endif
 
 static int ipoib_change_mtu(struct net_device *dev, int new_mtu)
 {
@@ -263,8 +280,15 @@ int ipoib_set_mode(struct net_device *de
 		set_bit(IPOIB_FLAG_ADMIN_CM, &priv->flags);
 		ipoib_warn(priv, "enabling connected mode "
 			   "will cause multicast packet drops\n");
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 		netdev_update_features(dev);
-		dev_set_mtu(dev, ipoib_cm_max_mtu(dev));
+#else
+                dev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_SG | NETIF_F_TSO);
+                if (ipoib_cm_max_mtu(dev) > priv->mcast_mtu)
+                        ipoib_warn(priv, "mtu > %d will cause multicast packet drops.\n",
+                                   priv->mcast_mtu);
+#endif
+ 		dev_set_mtu(dev, ipoib_cm_max_mtu(dev));
 		rtnl_unlock();
 
 		send_ring = priv->send_ring;
@@ -283,7 +307,16 @@ int ipoib_set_mode(struct net_device *de
 
 	if (!strcmp(buf, "datagram\n")) {
 		clear_bit(IPOIB_FLAG_ADMIN_CM, &priv->flags);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 		netdev_update_features(dev);
+#else
+		if (test_bit(IPOIB_FLAG_CSUM, &priv->flags)) {
+			dev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
+
+			if (priv->hca_caps & IB_DEVICE_UD_TSO)
+				dev->features |= NETIF_F_TSO;
+		}
+#endif
 		dev_set_mtu(dev, min(priv->mcast_mtu, dev->mtu));
 		rtnl_unlock();
 		ipoib_flush_paths(dev);
@@ -1111,9 +1144,16 @@ unref:
 	return NETDEV_TX_OK;
 }
 
+#ifdef CONFIG_COMPAT_SELECT_QUEUE_ACCEL
 static u16 ipoib_select_queue_hw(struct net_device *dev, struct sk_buff *skb,
-				 void *accel_priv, select_queue_fallback_t
-				 fallback)
+#ifdef CONFIG_COMPAT_SELECT_QUEUE_FALLBACK
+ 			 	 void *accel_priv, select_queue_fallback_t fallback)
+#else
+				 void *accel_priv)
+#endif
+#else /* CONFIG_COMPAT_SELECT_QUEUE_ACCEL */
+static u16 ipoib_select_queue_hw(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
 	struct ipoib_cb *cb = (struct ipoib_cb *) skb->cb;
@@ -1139,9 +1179,16 @@ static u16 ipoib_select_queue_hw(struct
 	return skb_tx_hash(dev, skb);
 }
 
+#ifdef CONFIG_COMPAT_SELECT_QUEUE_ACCEL
 static u16 ipoib_select_queue_sw(struct net_device *dev, struct sk_buff *skb,
-				 void* accel_priv, select_queue_fallback_t
-				 fallback)
+#ifdef CONFIG_COMPAT_SELECT_QUEUE_FALLBACK
+				 void* accel_priv, select_queue_fallback_t fallback)
+#else
+				 void* accel_priv)
+#endif
+#else /* CONFIG_COMPAT_SELECT_QUEUE_ACCEL */
+static u16 ipoib_select_queue_sw(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
 	struct ipoib_cb *cb = (struct ipoib_cb *) skb->cb;
@@ -1175,7 +1222,11 @@ static u16 ipoib_select_queue_sw(struct
 	header->tss_qpn_mask_sz |= priv->tss_qpn_mask_sz;
 
 	/* don't use special ring in TX */
+#ifdef CONFIG_COMPAT_IS___SKB_TX_HASH
 	return __skb_tx_hash(dev, skb, priv->tss_qp_num);
+#else
+	return skb_tx_hash(dev, skb);
+#endif
 }
 
 static void ipoib_timeout(struct net_device *dev)
@@ -1921,6 +1972,9 @@ int ipoib_dev_init(struct net_device *de
 		}
 		recv_ring->dev = dev;
 		recv_ring->index = i;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		ipoib_lro_setup(recv_ring, priv);
+#endif
 		recv_ring++;
 		rx_allocated++;
 	}
@@ -1985,6 +2039,7 @@ out:
 	return -ENOMEM;
 }
 
+#ifdef HAVE_NDO_GET_IFLINK
 static int ipoib_get_iflink(const struct net_device *dev)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
@@ -1996,7 +2051,7 @@ static int ipoib_get_iflink(const struct
 	/* child/vlan interface */
 	return priv->parent->ifindex;
 }
-
+#endif
 void ipoib_dev_uninit(struct net_device *dev)
 {
 	struct ipoib_dev_priv *priv = netdev_priv(dev);
@@ -2040,16 +2095,22 @@ void ipoib_dev_cleanup(struct net_device
 	ipoib_delete_debug_files(dev);
 
 	/* Delete any child interfaces first */
-	list_for_each_entry_safe_reverse(cpriv, tcpriv,
-					 &priv->child_intfs, list)
-		unregister_netdevice_queue(cpriv->dev, &head);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33)
+        list_for_each_entry_safe_reverse(cpriv, tcpriv,
+                                         &priv->child_intfs, list)
+                unregister_netdevice_queue(cpriv->dev, &head);
 
 	/*
-	 * the next function calls the ipoib_uninit which calls for
-	 * ipoib_dev_cleanup for each devices at the head list.
-	 */
-
-	unregister_netdevice_many(&head);
+ 	 * the next function calls the ipoib_uninit which calls for
+ 	 * ipoib_dev_cleanup for each devices at the head list.
+ 	 */
+
+        unregister_netdevice_many(&head);
+#else
+        list_for_each_entry_safe(cpriv, tcpriv,
+				 &priv->child_intfs, list)
+		unregister_netdevice(cpriv->dev);
+#endif
 
 	ipoib_dev_uninit(dev);
 	/* ipoib_dev_uninit took rings lock can't release in case of reinit */
@@ -2093,7 +2154,9 @@ int ipoib_reinit(struct net_device *dev,
 		priv->tss_qp_num = num_tx - 1;
 	else
 		priv->tss_qp_num = num_tx;
+#ifdef HAVE_NEW_TX_RING_SCHEME
 	netif_set_real_num_tx_queues(dev, num_tx);
+#endif
 	netif_set_real_num_rx_queues(dev, num_rx);
 	/*
 	 * prevent ipoib_ib_dev_init call ipoib_ib_dev_open
@@ -2128,6 +2191,75 @@ int ipoib_reinit(struct net_device *dev,
 	return ret;
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int get_skb_hdr(struct sk_buff *skb, void **iphdr,
+			void **tcph, u64 *hdr_flags, void *priv)
+{
+	unsigned int ip_len;
+	struct iphdr *iph;
+
+	if (unlikely(skb->protocol != htons(ETH_P_IP)))
+		return -1;
+
+	/*
+	* In the future we may add an else clause that verifies the
+	* checksum and allows devices which do not calculate checksum
+	* to use LRO.
+	*/
+	if (unlikely(skb->ip_summed != CHECKSUM_UNNECESSARY))
+		return -1;
+
+	/* Check for non-TCP packet */
+	skb_reset_network_header(skb);
+	iph = ip_hdr(skb);
+	if (iph->protocol != IPPROTO_TCP)
+		return -1;
+
+	ip_len = ip_hdrlen(skb);
+	skb_set_transport_header(skb, ip_len);
+	*tcph = tcp_hdr(skb);
+
+	/* check if IP header and TCP header are complete */
+	if (ntohs(iph->tot_len) < ip_len + tcp_hdrlen(skb))
+		return -1;
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	*iphdr = iph;
+
+	return 0;
+}
+
+
+static void ipoib_lro_setup(struct ipoib_recv_ring *recv_ring,
+				struct ipoib_dev_priv *priv)
+{
+	recv_ring->lro.lro_mgr.max_aggr  = lro_max_aggr;
+	recv_ring->lro.lro_mgr.max_desc  = IPOIB_MAX_LRO_DESCRIPTORS;
+	recv_ring->lro.lro_mgr.lro_arr   = recv_ring->lro.lro_desc;
+	recv_ring->lro.lro_mgr.get_skb_header = get_skb_hdr;
+	recv_ring->lro.lro_mgr.features  = LRO_F_NAPI;
+	recv_ring->lro.lro_mgr.dev               = priv->dev;
+	recv_ring->lro.lro_mgr.ip_summed_aggr = CHECKSUM_UNNECESSARY;
+}
+
+void set_lro_features_bit(struct ipoib_dev_priv *priv)
+{
+	u64 hw_support_lro = 0;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
+		hw_support_lro = priv->dev->hw_features & NETIF_F_RXCSUM;
+#else
+		hw_support_lro = (priv->dev->features & NETIF_F_RXCSUM);
+#endif
+	if (lro && hw_support_lro) {
+		priv->dev->features |= NETIF_F_LRO;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0))
+		priv->dev->hw_features |= NETIF_F_LRO;
+		priv->dev->wanted_features |= NETIF_F_LRO;
+#endif
+	}
+}
+#endif
+
 static const struct header_ops ipoib_header_ops = {
 	.create	= ipoib_hard_header,
 };
@@ -2137,12 +2269,16 @@ static const struct net_device_ops ipoib
 	.ndo_open		 = ipoib_open,
 	.ndo_stop		 = ipoib_stop,
 	.ndo_change_mtu		 = ipoib_change_mtu,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 	.ndo_fix_features	 = ipoib_fix_features,
+#endif
 	.ndo_start_xmit	 	 = ipoib_start_xmit,
 	.ndo_tx_timeout		 = ipoib_timeout,
 	.ndo_get_stats		= ipoib_get_stats,
 	.ndo_set_rx_mode	 = ipoib_set_mcast_list,
+#ifdef HAVE_NDO_GET_IFLINK
 	.ndo_get_iflink		= ipoib_get_iflink,
+#endif
 };
 
 static const struct net_device_ops ipoib_netdev_ops_hw_tss = {
@@ -2150,13 +2286,17 @@ static const struct net_device_ops ipoib
 	.ndo_open	= ipoib_open,
 	.ndo_stop	= ipoib_stop,
 	.ndo_change_mtu		= ipoib_change_mtu,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 	.ndo_fix_features		= ipoib_fix_features,
+#endif
 	.ndo_start_xmit		= ipoib_start_xmit,
 	.ndo_select_queue		= ipoib_select_queue_hw,
 	.ndo_tx_timeout		= ipoib_timeout,
 	.ndo_get_stats		= ipoib_get_stats,
 	.ndo_set_rx_mode		= ipoib_set_mcast_list,
+#ifdef HAVE_NDO_GET_IFLINK
 	.ndo_get_iflink		= ipoib_get_iflink,
+#endif
 };
 
 static const struct net_device_ops ipoib_netdev_ops_sw_tss = {
@@ -2164,13 +2304,17 @@ static const struct net_device_ops ipoib
 	.ndo_open	= ipoib_open,
 	.ndo_stop	= ipoib_stop,
 	.ndo_change_mtu		= ipoib_change_mtu,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 	.ndo_fix_features		= ipoib_fix_features,
+#endif
 	.ndo_start_xmit		= ipoib_start_xmit,
 	.ndo_select_queue		= ipoib_select_queue_sw,
 	.ndo_tx_timeout		= ipoib_timeout,
 	.ndo_get_stats		= ipoib_get_stats,
 	.ndo_set_rx_mode		= ipoib_set_mcast_list,
+#ifdef HAVE_NDO_GET_IFLINK
 	.ndo_get_iflink		= ipoib_get_iflink,
+#endif
 };
 
 
@@ -2257,7 +2401,9 @@ struct ipoib_dev_priv *ipoib_intf_alloc(
 	if (!dev)
 		return NULL;
 
+#ifdef HAVE_NEW_TX_RING_SCHEME
 	netif_set_real_num_tx_queues(dev, template_priv->num_tx_queues);
+#endif
 	netif_set_real_num_rx_queues(dev, template_priv->num_rx_queues);
 
 	return netdev_priv(dev);
@@ -2613,6 +2759,7 @@ int ipoib_set_dev_features(struct ipoib_
 		return result;
 
 	if (priv->hca_caps & IB_DEVICE_UD_IP_CSUM) {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39))
 		priv->dev->hw_features = NETIF_F_SG |
 			NETIF_F_IP_CSUM | NETIF_F_RXCSUM;
 
@@ -2620,8 +2767,20 @@ int ipoib_set_dev_features(struct ipoib_
 			priv->dev->hw_features |= NETIF_F_TSO;
 
 		priv->dev->features |= priv->dev->hw_features;
+#else
+		set_bit(IPOIB_FLAG_CSUM, &priv->flags);
+		priv->dev->features |= NETIF_F_SG |
+			NETIF_F_IP_CSUM | NETIF_F_RXCSUM;
+
+		if (priv->hca_caps & IB_DEVICE_UD_TSO)
+			priv->dev->features |= NETIF_F_TSO;
+#endif
 	}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	set_lro_features_bit(priv);
+#endif
+
 	return 0;
 }
 
@@ -2649,9 +2808,9 @@ static struct net_device *ipoib_add_port
 	SET_NETDEV_DEV(priv->dev, hca->dma_device);
 	priv->dev->dev_id = port - 1;
 
-	if (!ib_query_port(hca, port, &attr))
+	if (!ib_query_port(hca, port, &attr)) {
 		priv->max_ib_mtu = ib_mtu_enum_to_int(attr.max_mtu);
-	else {
+	} else {
 		printk(KERN_WARNING "%s: ib_query_port %d failed\n",
 		       hca->name, port);
 		goto device_init_failed;
@@ -2665,7 +2824,9 @@ static struct net_device *ipoib_add_port
 	priv->dev->mtu  = IPOIB_UD_MTU(priv->max_ib_mtu);
 	priv->mcast_mtu  = priv->admin_mtu = priv->dev->mtu;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0))
 	priv->dev->neigh_priv_len = sizeof(struct ipoib_neigh);
+#endif
 
 	result = ib_query_pkey(hca, port, 0, &priv->pkey);
 	if (result) {
@@ -2719,6 +2880,12 @@ static struct net_device *ipoib_add_port
 		goto register_failed;
 	}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	/*force lro on the dev->features, because the function
+	register_netdev disable it according to our private lro*/
+	set_lro_features_bit(priv);
+#endif
+
 	ipoib_create_debug_files(priv->dev);
 
 	result = -ENOMEM;
@@ -2903,6 +3070,16 @@ static int __init ipoib_init_module(void
 	ipoib_max_conn_qp = min(ipoib_max_conn_qp, IPOIB_CM_MAX_CONN_QP);
 #endif
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (lro < 0 || lro > 1)
+		lro = 1;
+
+	if (lro_max_aggr < 0 || lro_max_aggr > IPOIB_LRO_MAX_AGGR ||
+	    (lro_max_aggr & (lro_max_aggr - 1)) != 0)
+		lro_max_aggr = IPOIB_LRO_MAX_AGGR;
+#endif
+
+
 	/*
 	 * When copying small received packets, we only copy from the
 	 * linear data part of the SKB, so we rely on this condition.
--- a/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
@@ -898,7 +898,11 @@ void ipoib_mcast_restart_task(struct wor
 	struct ipoib_dev_priv *priv =
 		container_of(work, struct ipoib_dev_priv, restart_task);
 	struct net_device *dev = priv->dev;
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	struct netdev_hw_addr *ha;
+#else
+	struct dev_mc_list *mclist;
+#endif
 	struct ipoib_mcast *mcast, *tmcast;
 	LIST_HEAD(remove_list);
 	unsigned long flags;
@@ -921,6 +925,7 @@ void ipoib_mcast_restart_task(struct wor
 		clear_bit(IPOIB_MCAST_FLAG_FOUND, &mcast->flags);
 
 	/* Mark all of the entries that are found or don't exist */
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, dev) {
 		union ib_gid mgid;
 
@@ -928,6 +933,16 @@ void ipoib_mcast_restart_task(struct wor
 			continue;
 
 		memcpy(mgid.raw, ha->addr + 4, sizeof mgid);
+#else
+	for (mclist = dev->mc_list; mclist; mclist = mclist->next) {
+		union ib_gid mgid;
+
+		if (!ipoib_mcast_addr_is_valid(mclist->dmi_addr,
+						dev->broadcast))
+			continue;
+
+		memcpy(mgid.raw, mclist->dmi_addr + 4, sizeof mgid);
+#endif
 
 		mcast = __ipoib_mcast_find(dev, &mgid, &priv->multicast_tree);
 		if (!mcast || test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags)) {
--- a/drivers/infiniband/ulp/ipoib/ipoib_netlink.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_netlink.c
@@ -91,8 +91,11 @@ static int ipoib_changelink(struct net_d
 out_err:
 	return ret;
 }
-
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33)
 static int ipoib_new_child_link(struct net *src_net, struct net_device *dev,
+#else
+static int ipoib_new_child_link(struct net_device *dev,
+#endif
 			       struct nlattr *tb[], struct nlattr *data[])
 {
 	struct net_device *pdev;
@@ -102,8 +105,11 @@ static int ipoib_new_child_link(struct n
 
 	if (!tb[IFLA_LINK])
 		return -EINVAL;
-
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33)
 	pdev = __dev_get_by_index(src_net, nla_get_u32(tb[IFLA_LINK]));
+#else
+	pdev = __dev_get_by_index(dev_net(dev), nla_get_u32(tb[IFLA_LINK]));
+#endif
 	if (!pdev || pdev->type != ARPHRD_INFINIBAND)
 		return -ENODEV;
 
@@ -127,7 +133,11 @@ static int ipoib_new_child_link(struct n
 	return err;
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33)
 static void ipoib_unregister_child_dev(struct net_device *dev, struct list_head *head)
+#else
+static void ipoib_unregister_child_dev(struct net_device *dev)
+#endif
 {
 	struct ipoib_dev_priv *priv, *ppriv;
 
@@ -135,7 +145,11 @@ static void ipoib_unregister_child_dev(s
 	ppriv = netdev_priv(priv->parent);
 
 	down_write(&ppriv->vlan_rwsem);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33)
 	unregister_netdevice_queue(dev, head);
+#else
+	unregister_netdevice(dev);
+#endif
 	list_del(&priv->list);
 	up_write(&ppriv->vlan_rwsem);
 }
--- a/drivers/infiniband/ulp/ipoib/ipoib_vlan.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_vlan.c
@@ -107,7 +107,9 @@ int __ipoib_vlan_add(struct ipoib_dev_pr
 	}
 
 	priv->child_type  = type;
+#ifndef HAVE_NDO_GET_IFLINK
 	priv->dev->iflink = ppriv->dev->ifindex;
+#endif
 
 	list_add_tail(&priv->list, &ppriv->child_intfs);
 
