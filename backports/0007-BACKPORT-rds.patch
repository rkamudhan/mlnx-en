From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: rds

Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 net/rds/af_rds.c     |  4 +++
 net/rds/bind.c       |  7 ++++
 net/rds/connection.c | 34 +++++++++++++++++++
 net/rds/ib.h         |  5 +++
 net/rds/ib_rdma.c    |  4 +++
 net/rds/ib_recv.c    | 80 +++++++++++++++++++++++++++++++++++++++++++
 net/rds/ib_sysctl.c  | 22 ++++++++++++
 net/rds/info.c       | 12 +++++++
 net/rds/iw.h         |  5 +++
 net/rds/iw_recv.c    | 45 +++++++++++++++++++++++-
 net/rds/iw_sysctl.c  | 22 ++++++++++++
 net/rds/message.c    | 96 +++++++++++++++++++++++++++++++++++++++++++++++++++-
 net/rds/rds.h        | 25 ++++++++++++++
 net/rds/recv.c       | 14 ++++++++
 net/rds/send.c       |  8 +++++
 net/rds/sysctl.c     | 21 ++++++++++++
 net/rds/tcp.h        | 13 +++++++
 net/rds/tcp_listen.c | 19 +++++++++++
 net/rds/tcp_recv.c   | 59 +++++++++++++++++++++++++++++++-
 19 files changed, 492 insertions(+), 3 deletions(-)

--- a/net/rds/af_rds.c
+++ b/net/rds/af_rds.c
@@ -440,7 +440,11 @@ static int rds_create(struct net *net, s
 	if (sock->type != SOCK_SEQPACKET || protocol)
 		return -ESOCKTNOSUPPORT;
 
+#ifdef HAVE_SK_ALLOC_5_PARAMS
+	sk = sk_alloc(net, AF_RDS, GFP_ATOMIC, &rds_proto, kern);
+#else
 	sk = sk_alloc(net, AF_RDS, GFP_ATOMIC, &rds_proto);
+#endif
 	if (!sk)
 		return -ENOMEM;
 
--- a/net/rds/bind.c
+++ b/net/rds/bind.c
@@ -52,12 +52,19 @@ static struct rds_sock *rds_bind_lookup(
 					struct rds_sock *insert)
 {
 	struct rds_sock *rs;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *node;
+#endif
 	struct hlist_head *head = hash_to_bucket(addr, port);
 	u64 cmp;
 	u64 needle = ((u64)be32_to_cpu(addr) << 32) | be16_to_cpu(port);
 
 	rcu_read_lock();
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	hlist_for_each_entry(rs, node, head, rs_bound_node) {
+#else
 	hlist_for_each_entry_rcu(rs, head, rs_bound_node) {
+#endif
 		cmp = ((u64)be32_to_cpu(rs->rs_bound_addr) << 32) |
 		      be16_to_cpu(rs->rs_bound_port);
 
--- a/net/rds/connection.c
+++ b/net/rds/connection.c
@@ -51,16 +51,31 @@ static struct kmem_cache *rds_conn_slab;
 
 static struct hlist_head *rds_conn_bucket(__be32 laddr, __be32 faddr)
 {
+#ifdef HAVE_INET_EHASHFN
 	static u32 rds_hash_secret __read_mostly;
+#endif
 
 	unsigned long hash;
 
+#ifdef HAVE_INET_EHASHFN
+#ifdef HAVE_NET_GET_RANDOM_ONCE
 	net_get_random_once(&rds_hash_secret, sizeof(rds_hash_secret));
+#else
+	rds_hash_secret = inet_ehash_secret;
+#endif
+#endif
 
 	/* Pass NULL, don't need struct net for hash */
+#ifdef HAVE_INET_EHASHFN
 	hash = __inet_ehashfn(be32_to_cpu(laddr), 0,
 			      be32_to_cpu(faddr), 0,
 			      rds_hash_secret);
+#else
+	hash = inet_ehashfn(NULL,
+			be32_to_cpu(laddr), 0,
+			be32_to_cpu(faddr), 0);
+#endif
+
 	return &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];
 }
 
@@ -75,8 +90,13 @@ static struct rds_connection *rds_conn_l
 					      struct rds_transport *trans)
 {
 	struct rds_connection *conn, *ret = NULL;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *pos;
 
+	hlist_for_each_entry_rcu(conn, pos, head, c_hash_node) {
+#else
 	hlist_for_each_entry_rcu(conn, head, c_hash_node) {
+#endif
 		if (conn->c_faddr == faddr && conn->c_laddr == laddr &&
 				conn->c_trans == trans) {
 			ret = conn;
@@ -381,6 +401,9 @@ static void rds_conn_message_info(struct
 				  int want_send)
 {
 	struct hlist_head *head;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *pos;
+#endif
 	struct list_head *list;
 	struct rds_connection *conn;
 	struct rds_message *rm;
@@ -394,7 +417,11 @@ static void rds_conn_message_info(struct
 
 	for (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);
 	     i++, head++) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+		hlist_for_each_entry_rcu(conn, pos, head, c_hash_node) {
+#else
 		hlist_for_each_entry_rcu(conn, head, c_hash_node) {
+#endif
 			if (want_send)
 				list = &conn->c_send_queue;
 			else
@@ -443,6 +470,9 @@ void rds_for_each_conn_info(struct socke
 {
 	uint64_t buffer[(item_len + 7) / 8];
 	struct hlist_head *head;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *pos;
+#endif
 	struct rds_connection *conn;
 	size_t i;
 
@@ -453,7 +483,11 @@ void rds_for_each_conn_info(struct socke
 
 	for (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);
 	     i++, head++) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+		hlist_for_each_entry_rcu(conn, pos, head, c_hash_node) {
+#else
 		hlist_for_each_entry_rcu(conn, head, c_hash_node) {
+#endif
 
 			/* XXX no c_lock usage.. */
 			if (!visitor(conn, buffer))
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -316,7 +316,12 @@ int rds_ib_recv_alloc_caches(struct rds_
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_refill(struct rds_connection *conn, int prefill);
 void rds_ib_inc_free(struct rds_incoming *inc);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
+			    size_t size);
+#else
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
+#endif
 void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);
 void rds_ib_recv_tasklet_fn(unsigned long data);
 void rds_ib_recv_init_ring(struct rds_ib_connection *ic);
--- a/net/rds/ib_rdma.c
+++ b/net/rds/ib_rdma.c
@@ -267,7 +267,11 @@ static inline struct rds_ib_mr *rds_ib_r
 	unsigned long *flag;
 
 	preempt_disable();
+#ifdef CONFIG_COMPAT_IS_THIS_CPU_PTR
 	flag = this_cpu_ptr(&clean_list_grace);
+#else
+	flag = &__get_cpu_var(clean_list_grace);
+#endif
 	set_bit(CLEAN_LIST_BUSY_BIT, flag);
 	ret = llist_del_first(&pool->clean_list);
 	if (ret)
--- a/net/rds/ib_recv.c
+++ b/net/rds/ib_recv.c
@@ -421,20 +421,38 @@ static void rds_ib_recv_cache_put(struct
 				 struct rds_ib_refill_cache *cache)
 {
 	unsigned long flags;
+#ifdef HAVE_THIS_CPU_READ
 	struct list_head *old, *chpfirst;
+#else
+	struct rds_ib_cache_head *chp;
+	struct list_head *old;
+#endif
 
 	local_irq_save(flags);
 
+#ifdef HAVE_THIS_CPU_READ
 	chpfirst = __this_cpu_read(cache->percpu->first);
 	if (!chpfirst)
+#else
+	chp = per_cpu_ptr(cache->percpu, smp_processor_id());
+	if (!chp->first)
+#endif
 		INIT_LIST_HEAD(new_item);
 	else /* put on front */
+#ifdef HAVE_THIS_CPU_READ
 		list_add_tail(new_item, chpfirst);
 
 	__this_cpu_write(cache->percpu->first, new_item);
 	__this_cpu_inc(cache->percpu->count);
 
 	if (__this_cpu_read(cache->percpu->count) < RDS_IB_RECYCLE_BATCH_COUNT)
+#else
+		list_add_tail(new_item, chp->first);
+	chp->first = new_item;
+	chp->count++;
+
+	if (chp->count < RDS_IB_RECYCLE_BATCH_COUNT)
+#endif
 		goto end;
 
 	/*
@@ -446,13 +464,23 @@ static void rds_ib_recv_cache_put(struct
 	do {
 		old = xchg(&cache->xfer, NULL);
 		if (old)
+#ifdef HAVE_THIS_CPU_READ
 			list_splice_entire_tail(old, chpfirst);
 		old = cmpxchg(&cache->xfer, NULL, chpfirst);
+#else
+			list_splice_entire_tail(old, chp->first);
+		list_splice_entire_tail(old, chp->first);
+#endif
 	} while (old);
 
 
+#ifdef HAVE_THIS_CPU_READ
 	__this_cpu_write(cache->percpu->first, NULL);
 	__this_cpu_write(cache->percpu->count, 0);
+#else
+	chp->first = NULL;
+	chp->count = 0;
+#endif
 end:
 	local_irq_restore(flags);
 }
@@ -472,7 +500,12 @@ static struct list_head *rds_ib_recv_cac
 	return head;
 }
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *first_iov,
+			    size_t size)
+#else
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to)
+#endif
 {
 	struct rds_ib_incoming *ibinc;
 	struct rds_page_frag *frag;
@@ -481,22 +514,52 @@ int rds_ib_inc_copy_to_user(struct rds_i
 	int copied = 0;
 	int ret;
 	u32 len;
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	struct iovec *iov = first_iov;
+	unsigned long iov_off = 0;
+#endif
 
 	ibinc = container_of(inc, struct rds_ib_incoming, ii_inc);
 	frag = list_entry(ibinc->ii_frags.next, struct rds_page_frag, f_item);
 	len = be32_to_cpu(inc->i_hdr.h_len);
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	while (copied < size && copied < len) {
+#else
 	while (iov_iter_count(to) && copied < len) {
+#endif
 		if (frag_off == RDS_FRAG_SIZE) {
 			frag = list_entry(frag->f_item.next,
 					  struct rds_page_frag, f_item);
 			frag_off = 0;
 		}
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		while (iov_off == iov->iov_len) {
+			iov_off = 0;
+			iov++;
+		}
+
+		to_copy = min(iov->iov_len - iov_off, RDS_FRAG_SIZE - frag_off);
+		to_copy = min_t(size_t, to_copy, size - copied);
+#else
 		to_copy = min_t(unsigned long, iov_iter_count(to),
 				RDS_FRAG_SIZE - frag_off);
+#endif
 		to_copy = min_t(unsigned long, to_copy, len - copied);
 
 		/* XXX needs + offset for multiple recvs per page */
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		ret = rds_page_copy_to_user(sg_page(&frag->f_sg),
+					    frag->f_sg.offset + frag_off,
+					    iov->iov_base + iov_off,
+					    to_copy);
+		if (ret) {
+			copied = ret;
+			break;
+		}
+
+		iov_off += to_copy;
+#else
 		rds_stats_add(s_copy_to_user, to_copy);
 		ret = copy_page_to_iter(sg_page(&frag->f_sg),
 					frag->f_sg.offset + frag_off,
@@ -504,6 +567,7 @@ int rds_ib_inc_copy_to_user(struct rds_i
 					to);
 		if (ret != to_copy)
 			return -EFAULT;
+#endif
 
 		frag_off += to_copy;
 		copied += to_copy;
@@ -583,7 +647,11 @@ static void rds_ib_set_ack(struct rds_ib
 {
 	atomic64_set(&ic->i_ack_next, seq);
 	if (ack_required) {
+#ifndef CONFIG_COMPAT_IS_SMP_MB__AFTER_ATOMIC
+		smp_mb__before_clear_bit();
+#else
 		smp_mb__before_atomic();
+#endif
 		set_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
 	}
 }
@@ -591,7 +659,11 @@ static void rds_ib_set_ack(struct rds_ib
 static u64 rds_ib_get_ack(struct rds_ib_connection *ic)
 {
 	clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);
+#ifndef CONFIG_COMPAT_IS_SMP_MB__AFTER_ATOMIC
+	smp_mb__after_clear_bit();
+#else
 	smp_mb__after_atomic();
+#endif
 
 	return atomic64_read(&ic->i_ack_next);
 }
@@ -752,7 +824,11 @@ static void rds_ib_cong_recv(struct rds_
 		to_copy = min(RDS_FRAG_SIZE - frag_off, PAGE_SIZE - map_off);
 		BUG_ON(to_copy & 7); /* Must be 64bit aligned. */
 
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 		addr = kmap_atomic(sg_page(&frag->f_sg));
+#else
+		addr = kmap_atomic(sg_page(&frag->f_sg), KM_USER0);
+#endif
 
 		src = addr + frag_off;
 		dst = (void *)map->m_page_addrs[map_page] + map_off;
@@ -762,7 +838,11 @@ static void rds_ib_cong_recv(struct rds_
 			uncongested |= ~(*src) & *dst;
 			*dst++ = *src++;
 		}
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 		kunmap_atomic(addr);
+#else
+		kunmap_atomic(addr, KM_USER0);
+#endif
 
 		copied += to_copy;
 
--- a/net/rds/ib_sysctl.c
+++ b/net/rds/ib_sysctl.c
@@ -61,6 +61,7 @@ static unsigned long rds_ib_sysctl_max_u
  */
 unsigned int rds_ib_sysctl_flow_control = 0;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table rds_ib_sysctl_table[] = {
 	{
 		.procname       = "max_send_wr",
@@ -105,17 +106,38 @@ static struct ctl_table rds_ib_sysctl_ta
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path rds_ib_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rds" },
+	{ .procname = "ib" },
+	{ }
+};
+#endif
+#endif
 
 void rds_ib_sysctl_exit(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 	if (rds_ib_sysctl_hdr)
+#ifdef HAVE_REGISTER_NET_SYSCTL
 		unregister_net_sysctl_table(rds_ib_sysctl_hdr);
+#else
+		unregister_sysctl_table(rds_ib_sysctl_hdr);
+#endif
+#endif
 }
 
 int rds_ib_sysctl_init(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	rds_ib_sysctl_hdr = register_net_sysctl(&init_net, "net/rds/ib", rds_ib_sysctl_table);
+#else
+	rds_ib_sysctl_hdr = register_sysctl_paths(rds_ib_ctl_path, rds_ib_sysctl_table);
+#endif
 	if (!rds_ib_sysctl_hdr)
 		return -ENOMEM;
+#endif
 	return 0;
 }
--- a/net/rds/info.c
+++ b/net/rds/info.c
@@ -104,7 +104,11 @@ EXPORT_SYMBOL_GPL(rds_info_deregister_fu
 void rds_info_iter_unmap(struct rds_info_iterator *iter)
 {
 	if (iter->addr) {
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 		kunmap_atomic(iter->addr);
+#else
+		kunmap_atomic(iter->addr, KM_USER0);
+#endif
 		iter->addr = NULL;
 	}
 }
@@ -119,7 +123,11 @@ void rds_info_copy(struct rds_info_itera
 
 	while (bytes) {
 		if (!iter->addr)
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 			iter->addr = kmap_atomic(*iter->pages);
+#else
+			iter->addr = kmap_atomic(*iter->pages,KM_USER0);
+#endif
 
 		this = min(bytes, PAGE_SIZE - iter->offset);
 
@@ -134,7 +142,11 @@ void rds_info_copy(struct rds_info_itera
 		iter->offset += this;
 
 		if (iter->offset == PAGE_SIZE) {
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 			kunmap_atomic(iter->addr);
+#else
+			kunmap_atomic(iter->addr, KM_USER0);
+#endif
 			iter->addr = NULL;
 			iter->offset = 0;
 			iter->pages++;
--- a/net/rds/iw.h
+++ b/net/rds/iw.h
@@ -325,7 +325,12 @@ int rds_iw_recv(struct rds_connection *c
 int rds_iw_recv_refill(struct rds_connection *conn, gfp_t kptr_gfp,
 		       gfp_t page_gfp, int prefill);
 void rds_iw_inc_free(struct rds_incoming *inc);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_iw_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
+			    size_t size);
+#else
 int rds_iw_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
+#endif
 void rds_iw_recv_cq_comp_handler(struct ib_cq *cq, void *context);
 void rds_iw_recv_tasklet_fn(unsigned long data);
 void rds_iw_recv_init_ring(struct rds_iw_connection *ic);
--- a/net/rds/iw_recv.c
+++ b/net/rds/iw_recv.c
@@ -303,7 +303,12 @@ void rds_iw_inc_free(struct rds_incoming
 	BUG_ON(atomic_read(&rds_iw_allocation) < 0);
 }
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_iw_inc_copy_to_user(struct rds_incoming *inc, struct iovec *first_iov,
+			    size_t size)
+#else
 int rds_iw_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to)
+#endif
 {
 	struct rds_iw_incoming *iwinc;
 	struct rds_page_frag *frag;
@@ -312,22 +317,52 @@ int rds_iw_inc_copy_to_user(struct rds_i
 	int copied = 0;
 	int ret;
 	u32 len;
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	struct iovec *iov = first_iov;
+	unsigned long iov_off = 0;
+#endif
 
 	iwinc = container_of(inc, struct rds_iw_incoming, ii_inc);
 	frag = list_entry(iwinc->ii_frags.next, struct rds_page_frag, f_item);
 	len = be32_to_cpu(inc->i_hdr.h_len);
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	while (copied < size && copied < len) {
+#else
 	while (iov_iter_count(to) && copied < len) {
+#endif
 		if (frag_off == RDS_FRAG_SIZE) {
 			frag = list_entry(frag->f_item.next,
 					  struct rds_page_frag, f_item);
 			frag_off = 0;
 		}
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		while (iov_off == iov->iov_len) {
+			iov_off = 0;
+			iov++;
+		}
+
+		to_copy = min(iov->iov_len - iov_off, RDS_FRAG_SIZE - frag_off);
+		to_copy = min_t(size_t, to_copy, size - copied);
+#else
 		to_copy = min_t(unsigned long, iov_iter_count(to),
 				RDS_FRAG_SIZE - frag_off);
+#endif
 		to_copy = min_t(unsigned long, to_copy, len - copied);
 
 		/* XXX needs + offset for multiple recvs per page */
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		ret = rds_page_copy_to_user(frag->f_page,
+					    frag->f_offset + frag_off,
+					    iov->iov_base + iov_off,
+					    to_copy);
+		if (ret) {
+			copied = ret;
+			break;
+		}
+
+		iov_off += to_copy;
+#else
 		rds_stats_add(s_copy_to_user, to_copy);
 		ret = copy_page_to_iter(frag->f_page,
 					frag->f_offset + frag_off,
@@ -335,6 +370,7 @@ int rds_iw_inc_copy_to_user(struct rds_i
 					to);
 		if (ret != to_copy)
 			return -EFAULT;
+#endif
 
 		frag_off += to_copy;
 		copied += to_copy;
@@ -582,8 +618,11 @@ static void rds_iw_cong_recv(struct rds_
 
 		to_copy = min(RDS_FRAG_SIZE - frag_off, PAGE_SIZE - map_off);
 		BUG_ON(to_copy & 7); /* Must be 64bit aligned. */
-
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 		addr = kmap_atomic(frag->f_page);
+#else
+		addr = kmap_atomic(frag->f_page, KM_USER0);
+#endif
 
 		src = addr + frag_off;
 		dst = (void *)map->m_page_addrs[map_page] + map_off;
@@ -593,7 +632,11 @@ static void rds_iw_cong_recv(struct rds_
 			uncongested |= ~(*src) & *dst;
 			*dst++ = *src++;
 		}
+#ifdef HAVE_KMAP_ATOMIC_1_PARAM
 		kunmap_atomic(addr);
+#else
+		kunmap_atomic(addr, KM_USER0);
+#endif
 
 		copied += to_copy;
 
--- a/net/rds/iw_sysctl.c
+++ b/net/rds/iw_sysctl.c
@@ -55,6 +55,7 @@ static unsigned long rds_iw_sysctl_max_u
 
 unsigned int rds_iw_sysctl_flow_control = 1;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table rds_iw_sysctl_table[] = {
 	{
 		.procname       = "max_send_wr",
@@ -108,16 +109,37 @@ static struct ctl_table rds_iw_sysctl_ta
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path rds_iw_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rds" },
+	{ .procname = "iw" },
+	{ }
+};
+#endif
+#endif
 
 void rds_iw_sysctl_exit(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(rds_iw_sysctl_hdr);
+#else
+	unregister_sysctl_table(rds_iw_sysctl_hdr);
+#endif
+#endif
 }
 
 int rds_iw_sysctl_init(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	rds_iw_sysctl_hdr = register_net_sysctl(&init_net, "net/rds/iw", rds_iw_sysctl_table);
+#else
+	rds_iw_sysctl_hdr = register_sysctl_paths(rds_iw_ctl_path, rds_iw_sysctl_table);
+#endif
 	if (!rds_iw_sysctl_hdr)
 		return -ENOMEM;
+#endif
 	return 0;
 }
--- a/net/rds/message.c
+++ b/net/rds/message.c
@@ -264,31 +264,84 @@ struct rds_message *rds_message_map_page
 	return rm;
 }
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_message_copy_from_user(struct rds_message *rm, struct iovec *first_iov,
+			       size_t total_len, gfp_t gfp)
+#else
 int rds_message_copy_from_user(struct rds_message *rm, struct iov_iter *from)
+#endif
 {
-	unsigned long to_copy, nbytes;
+	unsigned long to_copy;
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	unsigned long iov_off;
+	struct iovec *iov;
+#else
+	unsigned long nbytes;
+#endif
 	unsigned long sg_off;
 	struct scatterlist *sg;
 	int ret = 0;
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	rm->m_inc.i_hdr.h_len = cpu_to_be32(total_len);
+#else
 	rm->m_inc.i_hdr.h_len = cpu_to_be32(iov_iter_count(from));
+#endif
 
 	/*
 	 * now allocate and copy in the data payload.
 	 */
 	sg = rm->data.op_sg;
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	iov = first_iov;
+	iov_off = 0;
+#endif
 	sg_off = 0; /* Dear gcc, sg->page will be null from kzalloc. */
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	while (total_len) {
+#else
 	while (iov_iter_count(from)) {
+#endif
 		if (!sg_page(sg)) {
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+			ret = rds_page_remainder_alloc(sg, total_len,
+							GFP_ATOMIC == gfp ?
+								      gfp :
+								      GFP_HIGHUSER);
+#else
 			ret = rds_page_remainder_alloc(sg, iov_iter_count(from),
 						       GFP_HIGHUSER);
+#endif
 			if (ret)
 				return ret;
 			rm->data.op_nents++;
 			sg_off = 0;
 		}
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		while (iov_off == iov->iov_len) {
+			iov_off = 0;
+			iov++;
+		}
+
+		to_copy = min(iov->iov_len - iov_off, sg->length - sg_off);
+		to_copy = min_t(size_t, to_copy, total_len);
+
+		rdsdebug("copying %lu bytes from user iov [%p, %zu] + %lu to "
+			 "sg [%p, %u, %u] + %lu\n",
+			 to_copy, iov->iov_base, iov->iov_len, iov_off,
+			 (void *)sg_page(sg), sg->offset, sg->length, sg_off);
+
+		ret = rds_page_copy_from_user(sg_page(sg), sg->offset + sg_off,
+					      iov->iov_base + iov_off,
+					      to_copy);
+		if (ret)
+			return ret;
+
+		iov_off += to_copy;
+		total_len -= to_copy;
+#else
 		to_copy = min_t(unsigned long, iov_iter_count(from),
 				sg->length - sg_off);
 
@@ -297,6 +350,7 @@ int rds_message_copy_from_user(struct rd
 					     to_copy, from);
 		if (nbytes != to_copy)
 			return -EFAULT;
+#endif
 
 		sg_off += to_copy;
 
@@ -307,9 +361,18 @@ int rds_message_copy_from_user(struct rd
 	return ret;
 }
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_message_inc_copy_to_user(struct rds_incoming *inc,
+				 struct iovec *first_iov, size_t size)
+#else
 int rds_message_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to)
+#endif
 {
 	struct rds_message *rm;
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	struct iovec *iov;
+	unsigned long iov_off;
+#endif
 	struct scatterlist *sg;
 	unsigned long to_copy;
 	unsigned long vec_off;
@@ -320,10 +383,40 @@ int rds_message_inc_copy_to_user(struct
 	rm = container_of(inc, struct rds_message, m_inc);
 	len = be32_to_cpu(rm->m_inc.i_hdr.h_len);
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	iov = first_iov;
+	iov_off = 0;
+#endif
 	sg = rm->data.op_sg;
 	vec_off = 0;
 	copied = 0;
 
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	while (copied < size && copied < len) {
+		while (iov_off == iov->iov_len) {
+			iov_off = 0;
+			iov++;
+		}
+
+		to_copy = min(iov->iov_len - iov_off, sg->length - vec_off);
+		to_copy = min_t(size_t, to_copy, size - copied);
+		to_copy = min_t(unsigned long, to_copy, len - copied);
+
+		rdsdebug("copying %lu bytes to user iov [%p, %zu] + %lu to "
+			 "sg [%p, %u, %u] + %lu\n",
+			 to_copy, iov->iov_base, iov->iov_len, iov_off,
+			 sg_page(sg), sg->offset, sg->length, vec_off);
+
+		ret = rds_page_copy_to_user(sg_page(sg), sg->offset + vec_off,
+					    iov->iov_base + iov_off,
+					    to_copy);
+		if (ret) {
+			copied = ret;
+			break;
+		}
+
+		iov_off += to_copy;
+#else
 	while (iov_iter_count(to) && copied < len) {
 		to_copy = min_t(unsigned long, iov_iter_count(to),
 				sg->length - vec_off);
@@ -334,6 +427,7 @@ int rds_message_inc_copy_to_user(struct
 					to_copy, to);
 		if (ret != to_copy)
 			return -EFAULT;
+#endif
 
 		vec_off += to_copy;
 		copied += to_copy;
--- a/net/rds/rds.h
+++ b/net/rds/rds.h
@@ -7,6 +7,7 @@
 #include <rdma/rdma_cm.h>
 #include <linux/mutex.h>
 #include <linux/rds.h>
+#include <linux/printk.h>
 
 #include "info.h"
 
@@ -431,7 +432,12 @@ struct rds_transport {
 	int (*xmit_rdma)(struct rds_connection *conn, struct rm_rdma_op *op);
 	int (*xmit_atomic)(struct rds_connection *conn, struct rm_atomic_op *op);
 	int (*recv)(struct rds_connection *conn);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	int (*inc_copy_to_user)(struct rds_incoming *inc, struct iovec *iov,
+				size_t size);
+#else
 	int (*inc_copy_to_user)(struct rds_incoming *inc, struct iov_iter *to);
+#endif
 	void (*inc_free)(struct rds_incoming *inc);
 
 	int (*cm_handle_connect)(struct rdma_cm_id *cm_id,
@@ -656,7 +662,12 @@ rds_conn_connecting(struct rds_connectio
 /* message.c */
 struct rds_message *rds_message_alloc(unsigned int nents, gfp_t gfp);
 struct scatterlist *rds_message_alloc_sgs(struct rds_message *rm, int nents);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_message_copy_from_user(struct rds_message *rm, struct iovec *first_iov,
+			       size_t total_len, gfp_t gfp);
+#else
 int rds_message_copy_from_user(struct rds_message *rm, struct iov_iter *from);
+#endif
 struct rds_message *rds_message_map_pages(unsigned long *page_addrs, unsigned int total_len);
 void rds_message_populate_header(struct rds_header *hdr, __be16 sport,
 				 __be16 dport, u64 seq);
@@ -665,7 +676,12 @@ int rds_message_add_extension(struct rds
 int rds_message_next_extension(struct rds_header *hdr,
 			       unsigned int *pos, void *buf, unsigned int *buflen);
 int rds_message_add_rdma_dest_extension(struct rds_header *hdr, u32 r_key, u32 offset);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_message_inc_copy_to_user(struct rds_incoming *inc,
+				 struct iovec *first_iov, size_t size);
+#else
 int rds_message_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
+#endif
 void rds_message_inc_free(struct rds_incoming *inc);
 void rds_message_addref(struct rds_message *rm);
 void rds_message_put(struct rds_message *rm);
@@ -702,8 +718,13 @@ void rds_inc_init(struct rds_incoming *i
 void rds_inc_put(struct rds_incoming *inc);
 void rds_recv_incoming(struct rds_connection *conn, __be32 saddr, __be32 daddr,
 		       struct rds_incoming *inc, gfp_t gfp);
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 int rds_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 		size_t size, int msg_flags);
+#else
+int rds_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
+		int msg_flags);
+#endif
 void rds_clear_recv_queue(struct rds_sock *rs);
 int rds_notify_queue_get(struct rds_sock *rs, struct msghdr *msg);
 void rds_inc_info_copy(struct rds_incoming *inc,
@@ -711,8 +732,12 @@ void rds_inc_info_copy(struct rds_incomi
 		       __be32 saddr, __be32 daddr, int flip);
 
 /* send.c */
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 int rds_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 		size_t payload_len);
+#else
+int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len);
+#endif
 void rds_send_reset(struct rds_connection *conn);
 int rds_send_xmit(struct rds_connection *conn);
 struct sockaddr_in;
--- a/net/rds/recv.c
+++ b/net/rds/recv.c
@@ -395,8 +395,13 @@ static int rds_cmsg_recv(struct rds_inco
 	return 0;
 }
 
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 int rds_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 		size_t size, int msg_flags)
+#else
+int rds_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
+		int msg_flags)
+#endif
 {
 	struct sock *sk = sock->sk;
 	struct rds_sock *rs = rds_sk_to_rs(sk);
@@ -414,7 +419,9 @@ int rds_recvmsg(struct kiocb *iocb, stru
 		goto out;
 
 	while (1) {
+#ifndef CONFIG_COMPAT_IS_MSG_IOV
 		struct iov_iter save;
+#endif
 		/* If there are pending notifications, do those - and nothing else */
 		if (!list_empty(&rs->rs_notify_queue)) {
 			ret = rds_notify_queue_get(rs, msg);
@@ -450,8 +457,13 @@ int rds_recvmsg(struct kiocb *iocb, stru
 		rdsdebug("copying inc %p from %pI4:%u to user\n", inc,
 			 &inc->i_conn->c_faddr,
 			 ntohs(inc->i_hdr.h_sport));
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		ret = inc->i_conn->c_trans->inc_copy_to_user(inc, msg->msg_iov,
+							     size);
+#else
 		save = msg->msg_iter;
 		ret = inc->i_conn->c_trans->inc_copy_to_user(inc, &msg->msg_iter);
+#endif
 		if (ret < 0)
 			break;
 
@@ -464,7 +476,9 @@ int rds_recvmsg(struct kiocb *iocb, stru
 			rds_inc_put(inc);
 			inc = NULL;
 			rds_stats_inc(s_recv_deliver_raced);
+#ifndef CONFIG_COMPAT_IS_MSG_IOV
 			msg->msg_iter = save;
+#endif
 			continue;
 		}
 
--- a/net/rds/send.c
+++ b/net/rds/send.c
@@ -920,8 +920,12 @@ static int rds_cmsg_send(struct rds_sock
 	return ret;
 }
 
+#ifdef HAVE_NET_SENDMSG_4_PARAMS
 int rds_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 		size_t payload_len)
+#else
+int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
+#endif
 {
 	struct sock *sk = sock->sk;
 	struct rds_sock *rs = rds_sk_to_rs(sk);
@@ -982,7 +986,11 @@ int rds_sendmsg(struct kiocb *iocb, stru
 			ret = -ENOMEM;
 			goto out;
 		}
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		ret = rds_message_copy_from_user(rm, msg->msg_iov, payload_len, GFP_KERNEL);
+#else
 		ret = rds_message_copy_from_user(rm, &msg->msg_iter);
+#endif
 		if (ret)
 			goto out;
 	}
--- a/net/rds/sysctl.c
+++ b/net/rds/sysctl.c
@@ -49,6 +49,7 @@ unsigned int  rds_sysctl_max_unacked_byt
 
 unsigned int rds_sysctl_ping_enable = 1;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table rds_sysctl_rds_table[] = {
 	{
 		.procname       = "reconnect_min_delay_ms",
@@ -91,10 +92,24 @@ static struct ctl_table rds_sysctl_rds_t
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path rds_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rds" },
+	{ }
+};
+#endif
+#endif
 
 void rds_sysctl_exit(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(rds_sysctl_reg_table);
+#else
+	unregister_sysctl_table(rds_sysctl_reg_table);
+#endif
+#endif
 }
 
 int rds_sysctl_init(void)
@@ -102,8 +117,14 @@ int rds_sysctl_init(void)
 	rds_sysctl_reconnect_min = msecs_to_jiffies(1);
 	rds_sysctl_reconnect_min_jiffies = rds_sysctl_reconnect_min;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	rds_sysctl_reg_table = register_net_sysctl(&init_net,"net/rds", rds_sysctl_rds_table);
+#else
+	rds_sysctl_reg_table = register_sysctl_paths(rds_ctl_path, rds_sysctl_rds_table);
+#endif
 	if (!rds_sysctl_reg_table)
 		return -ENOMEM;
+#endif
 	return 0;
 }
--- a/net/rds/tcp.h
+++ b/net/rds/tcp.h
@@ -61,15 +61,28 @@ void rds_tcp_state_change(struct sock *s
 /* tcp_listen.c */
 int rds_tcp_listen_init(void);
 void rds_tcp_listen_stop(void);
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+void rds_tcp_listen_data_ready(struct sock *sk, int bytes);
+#else
 void rds_tcp_listen_data_ready(struct sock *sk);
+#endif
 
 /* tcp_recv.c */
 int rds_tcp_recv_init(void);
 void rds_tcp_recv_exit(void);
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+void rds_tcp_data_ready(struct sock *sk, int bytes);
+#else
 void rds_tcp_data_ready(struct sock *sk);
+#endif
 int rds_tcp_recv(struct rds_connection *conn);
 void rds_tcp_inc_free(struct rds_incoming *inc);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_tcp_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
+			     size_t size);
+#else
 int rds_tcp_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
+#endif
 
 /* tcp_send.c */
 void rds_tcp_xmit_prepare(struct rds_connection *conn);
--- a/net/rds/tcp_listen.c
+++ b/net/rds/tcp_listen.c
@@ -67,12 +67,21 @@ static int rds_tcp_accept_one(struct soc
 
 	inet = inet_sk(new_sock->sk);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33))
 	rdsdebug("accepted tcp %pI4:%u -> %pI4:%u\n",
 		 &inet->inet_saddr, ntohs(inet->inet_sport),
 		 &inet->inet_daddr, ntohs(inet->inet_dport));
 
 	conn = rds_conn_create(inet->inet_saddr, inet->inet_daddr,
 			       &rds_tcp_transport, GFP_KERNEL);
+#else
+	rdsdebug("accepted tcp %pI4:%u -> %pI4:%u\n",
+		 &inet->saddr, ntohs(inet->sport),
+		 &inet->daddr, ntohs(inet->dport));
+
+	conn = rds_conn_create(inet->saddr, inet->daddr, &rds_tcp_transport,
+			       GFP_KERNEL);
+#endif
 	if (IS_ERR(conn)) {
 		ret = PTR_ERR(conn);
 		goto out;
@@ -108,9 +117,15 @@ static void rds_tcp_accept_worker(struct
 		cond_resched();
 }
 
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+void rds_tcp_listen_data_ready(struct sock *sk, int bytes)
+{
+	void (*ready)(struct sock *sk, int bytes);
+#else
 void rds_tcp_listen_data_ready(struct sock *sk)
 {
 	void (*ready)(struct sock *sk);
+#endif
 
 	rdsdebug("listen data ready sk %p\n", sk);
 
@@ -132,7 +147,11 @@ void rds_tcp_listen_data_ready(struct so
 
 out:
 	read_unlock(&sk->sk_callback_lock);
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+	ready(sk, bytes);
+#else
 	ready(sk);
+#endif
 }
 
 int rds_tcp_listen_init(void)
--- a/net/rds/tcp_recv.c
+++ b/net/rds/tcp_recv.c
@@ -59,18 +59,64 @@ void rds_tcp_inc_free(struct rds_incomin
 /*
  * this is pretty lame, but, whatever.
  */
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+int rds_tcp_inc_copy_to_user(struct rds_incoming *inc, struct iovec *first_iov,
+			     size_t size)
+#else
 int rds_tcp_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to)
+#endif
 {
 	struct rds_tcp_incoming *tinc;
 	struct sk_buff *skb;
 	int ret = 0;
-
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	struct iovec *iov, tmp;
+	unsigned long to_copy, skb_off;
+#endif
+
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	if (size == 0)
+#else
 	if (!iov_iter_count(to))
+#endif
 		goto out;
 
 	tinc = container_of(inc, struct rds_tcp_incoming, ti_inc);
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+	iov = first_iov;
+	tmp = *iov;
+#endif
 
 	skb_queue_walk(&tinc->ti_skb_list, skb) {
+#ifdef CONFIG_COMPAT_IS_MSG_IOV
+		skb_off = 0;
+		while (skb_off < skb->len) {
+			while (tmp.iov_len == 0) {
+				iov++;
+				tmp = *iov;
+			}
+
+			to_copy = min(tmp.iov_len, size);
+			to_copy = min(to_copy, skb->len - skb_off);
+
+			rdsdebug("ret %d size %zu skb %p skb_off %lu "
+				 "skblen %d iov_base %p iov_len %zu cpy %lu\n",
+				 ret, size, skb, skb_off, skb->len,
+				 tmp.iov_base, tmp.iov_len, to_copy);
+
+			/* modifies tmp as it copies */
+			if (skb_copy_datagram_iovec(skb, skb_off, &tmp,
+						    to_copy)) {
+				ret = -EFAULT;
+				goto out;
+			}
+
+			rds_stats_add(s_copy_to_user, to_copy);
+			size -= to_copy;
+			ret += to_copy;
+			skb_off += to_copy;
+			if (size == 0)
+#else
 		unsigned long to_copy, skb_off;
 		for (skb_off = 0; skb_off < skb->len; skb_off += to_copy) {
 			to_copy = iov_iter_count(to);
@@ -83,6 +129,7 @@ int rds_tcp_inc_copy_to_user(struct rds_
 			ret += to_copy;
 
 			if (!iov_iter_count(to))
+#endif
 				goto out;
 		}
 	}
@@ -294,9 +341,15 @@ int rds_tcp_recv(struct rds_connection *
 	return ret;
 }
 
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+void rds_tcp_data_ready(struct sock *sk, int bytes)
+{
+	void (*ready)(struct sock *sk, int bytes);
+#else
 void rds_tcp_data_ready(struct sock *sk)
 {
 	void (*ready)(struct sock *sk);
+#endif
 	struct rds_connection *conn;
 	struct rds_tcp_connection *tc;
 
@@ -317,7 +370,11 @@ void rds_tcp_data_ready(struct sock *sk)
 		queue_delayed_work(rds_wq, &conn->c_recv_w, 0);
 out:
 	read_unlock(&sk->sk_callback_lock);
+#ifdef HAVE_SK_DATA_READY_2_PARAMS
+	ready(sk, bytes);
+#else
 	ready(sk);
+#endif
 }
 
 int rds_tcp_recv_init(void)
